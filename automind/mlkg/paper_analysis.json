{
    "TAG/2302.11050v1.json": {
        "title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on  Textual-Edge Networks",
        "abs": "Edges in many real-world social/information networks are associated with rich\ntext information (e.g., user-user communications or user-product reviews).\nHowever, mainstream network representation learning models focus on propagating\nand aggregating node attributes, lacking specific designs to utilize text\nsemantics on edges. While there exist edge-aware graph neural networks, they\ndirectly initialize edge attributes as a feature vector, which cannot fully\ncapture the contextualized text semantics of edges. In this paper, we propose\nEdgeformers, a framework built upon graph-enhanced Transformers, to perform\nedge and node representation learning by modeling texts on edges in a\ncontextualized way. Specifically, in edge representation learning, we inject\nnetwork information into each Transformer layer when encoding edge texts; in\nnode representation learning, we aggregate edge representations through an\nattention mechanism within each node's ego-graph. On five public datasets from\nthree different domains, Edgeformers consistently outperform state-of-the-art\nbaselines in edge classification and link prediction, demonstrating the\nefficacy in learning edge and node representations, respectively.",
        "Task": "They address two primary tasks on textual-edge networks: (1) edge-level classification by predicting categories (e.g., star-ratings) of edges, and (2) node-level link prediction by deciding whether an edge should exist between two nodes. Both tasks are evaluated on multiple real-world datasets (e.g., Amazon, Goodreads, StackOverflow).",
        "Data_type": "They focus on homogeneous graph-structured data where each edge contains free-text documents (e.g., user–item reviews). Node features are not used, and the framework solely models and encodes textual information on edges for downstream tasks.",
        "Data_domain": "They use three real-world datasets spanning e-commerce (Amazon), reader–book networks (Goodreads), and Q&A communities (StackOverflow). Specifically, Amazon reviews, Goodreads comments, and StackOverflow answers serve as the textual-edge data sources.",
        "ML_phase": "They present a multi-layer Transformer-based architecture (Edgeformer-E and Edgeformer-N) that injects node or structural information at each encoding layer to jointly learn edge and node representations. Training proceeds via supervised edge classification or unsupervised link prediction, with attention-based aggregation for integrating local network structure."
    },
    "TAG/llm_2209.07972v1.json": {
        "title": "A Multi-turn Machine Reading Comprehension Framework with Rethink  Mechanism for Emotion-Cause Pair Extraction",
        "abs": "Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause\nanalysis, which extracts potential emotion-cause pairs from an emotional\ndocument. Most recent studies use end-to-end methods to tackle the ECPE task.\nHowever, these methods either suffer from a label sparsity problem or fail to\nmodel complicated relations between emotions and causes. Furthermore, they all\ndo not consider explicit semantic information of clauses. To this end, we\ntransform the ECPE task into a document-level machine reading comprehension\n(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism\n(MM-R). Our framework can model complicated relations between emotions and\ncauses while avoiding generating the pairing matrix (the leading cause of the\nlabel sparsity problem). Besides, the multi-turn structure can fuse explicit\nsemantic information flow between emotions and causes. Extensive experiments on\nthe benchmark emotion cause corpus demonstrate the effectiveness of our\nproposed framework, which outperforms existing state-of-the-art methods.",
        "Task": "They address the emotion-cause pair extraction (ECPE) task, framed as a multi-label classification problem that identifies all emotion clauses and their corresponding cause clauses in a document. They evaluate on a Chinese benchmark dataset of 1,945 SINA NEWS articles using precision, recall, and F1 scores.",
        "Data_type": "They use a Chinese textual dataset from SINA NEWS, consisting of 1,945 documents split into 28,727 clauses. Each document is represented purely by textual data at the clause level for emotion-cause pair extraction.",
        "Data_domain": "The dataset is derived from Chinese news articles gathered from the SINA NEWS website. It encompasses real-world news documents, situating the data firmly in the news domain.",
        "ML_phase": "They convert emotion-cause pair extraction into a multi-turn MRC pipeline that uses BERT at the token level, then aggregates clause representations via attention and GAT, and finally classifies query–clause pairs across three sequential turns (emotion extraction, cause extraction, and “rethinking”). Training employs a joint cross-entropy loss spanning these turns, with an additional probability adjustment in the final step to validate candidate emotion-cause pairs."
    },
    "TAG/2101.06323v3.json": {
        "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored  Search",
        "abs": "Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.",
        "Task": "They address a binary classification task to predict the relevance of ads given query-keyword pairs. The performance is evaluated using large-scale human-labeled data with ROC-AUC as the main metric.",
        "Data_type": "They use textual node attributes (queries and advertiser keywords) in a homogeneous graph, where edges represent user click feedback. The graph is further augmented with approximate neighbors found via semantic embeddings for low-frequency or sparse coverage scenarios.",
        "Data_domain": "The data stems from a large-scale sponsored search platform encompassing query-keyword interactions and user click behavior logs. It includes both native click graphs and augmented neighbor relationships derived from historical user searches and approximate nearest neighbor searches.",
        "ML_phase": "They construct a graph-based neighbor set from user click logs and integrate it with twin-tower text encoders plus a GNN aggregator for representation, then combine query- and keyword-side outputs. For training, the authors apply knowledge distillation from a larger RoBERTa teacher model to a smaller student network, ensuring feasible real-time performance."
    },
    "TAG/llm_2106.05234v5.json": {
        "title": "Do Transformers Really Perform Bad for Graph Representation?",
        "abs": "The Transformer architecture has become a dominant choice in many domains,\nsuch as natural language processing and computer vision. Yet, it has not\nachieved competitive performance on popular leaderboards of graph-level\nprediction compared to mainstream GNN variants. Therefore, it remains a mystery\nhow Transformers could perform well for graph representation learning. In this\npaper, we solve this mystery by presenting Graphormer, which is built upon the\nstandard Transformer architecture, and could attain excellent results on a\nbroad range of graph representation learning tasks, especially on the recent\nOGB Large-Scale Challenge. Our key insight to utilizing Transformer in the\ngraph is the necessity of effectively encoding the structural information of a\ngraph into the model. To this end, we propose several simple yet effective\nstructural encoding methods to help Graphormer better model graph-structured\ndata. Besides, we mathematically characterize the expressive power of\nGraphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the\nspecial cases of Graphormer.",
        "Task": "They address graph-level property prediction tasks, encompassing both regression (e.g., quantum chemistry on PCQM4M-LSC and ZINC) and classification (e.g., OGBG-MolHIV, OGBG-MolPCBA). These datasets specifically evaluate performance on predicting numerical or categorical graph-level properties.",
        "Data_type": "They handle graph-structured data with node- and edge-level features (e.g., molecular bonds in chemistry datasets) by explicitly incorporating structural information. The paper focuses on large-scale molecular graphs rather than text-based or multi-modal inputs.",
        "Data_domain": "All experimental data are molecular graphs derived from quantum chemistry and related molecular property prediction benchmarks. In particular, the paper focuses on large-scale datasets (e.g., PCQM4M-LSC) and established platforms such as OGB and ZINC.",
        "ML_phase": "Graphormer extends a standard Transformer by injecting structural encodings (centrality, distance, edge features) into the self-attention mechanism, forming an end-to-end architecture for graph-level learning. The training framework uses Adam-based optimization with linear decay, often starting from large-scale pre-training and then fine-tuning for downstream tasks."
    },
    "TAG/2106.05234v5.json": {
        "title": "Do Transformers Really Perform Bad for Graph Representation?",
        "abs": "The Transformer architecture has become a dominant choice in many domains,\nsuch as natural language processing and computer vision. Yet, it has not\nachieved competitive performance on popular leaderboards of graph-level\nprediction compared to mainstream GNN variants. Therefore, it remains a mystery\nhow Transformers could perform well for graph representation learning. In this\npaper, we solve this mystery by presenting Graphormer, which is built upon the\nstandard Transformer architecture, and could attain excellent results on a\nbroad range of graph representation learning tasks, especially on the recent\nOGB Large-Scale Challenge. Our key insight to utilizing Transformer in the\ngraph is the necessity of effectively encoding the structural information of a\ngraph into the model. To this end, we propose several simple yet effective\nstructural encoding methods to help Graphormer better model graph-structured\ndata. Besides, we mathematically characterize the expressive power of\nGraphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the\nspecial cases of Graphormer.",
        "Task": "They conduct graph-level property prediction tasks spanning both regression (e.g., PCQM4M-LSC, ZINC) and classification (e.g., MolHIV, MolPCBA). These tasks are evaluated on multiple benchmarks of varying scales, notably including OGB-LSC and other popular datasets.",
        "Data_type": "They handle graph-structured molecular data with node- and edge-level attributes, such as atomic features and bond types. Their approach also accommodates large-scale SMILES-based datasets, focusing on molecular property prediction tasks.",
        "Data_domain": "The datasets primarily consist of molecular graphs drawn from quantum chemistry (e.g., PCQM4M-LSC) and molecular property prediction benchmarks (e.g., MolHIV, MolPCBA, ZINC). They center on large-scale and smaller-scale chemistry-related graph tasks.",
        "ML_phase": "Graphormer integrates graph-specific encodings (node centrality, shortest-path distances, edge features) directly into the Transformer architecture, producing a global self-attention mechanism that respects graph topology. It is trained end-to-end with AdamW, employing techniques such as linear warm-up, attention/embedding dropout, and data augmentation."
    },
    "TAG/2402.16240v2.json": {
        "title": "High-Frequency-aware Hierarchical Contrastive Selective Coding for  Representation Learning on Text-attributed Graphs",
        "abs": "We investigate node representation learning on text-attributed graphs (TAGs),\nwhere nodes are associated with text information. Although recent studies on\ngraph neural networks (GNNs) and pretrained language models (PLMs) have\nexhibited their power in encoding network and text signals, respectively, less\nattention has been paid to delicately coupling these two types of models on\nTAGs. Specifically, existing GNNs rarely model text in each node in a\ncontextualized way; existing PLMs can hardly be applied to characterize graph\nstructures due to their sequence architecture. To address these challenges, we\npropose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive\nSelective Coding method that integrates GNNs and PLMs into a unified model.\nDifferent from previous \"cascaded architectures\" that directly add GNN layers\nupon a PLM, our HASH-CODE relies on five self-supervised optimization\nobjectives to facilitate thorough mutual enhancement between network and text\nsignals in diverse granularities. Moreover, we show that existing contrastive\nobjective learns the low-frequency component of the augmentation graph and\npropose a high-frequency component (HFC)-aware contrastive learning objective\nthat makes the learned embeddings more distinctive. Extensive experiments on\nsix real-world benchmarks substantiate the efficacy of our proposed approach.\nIn addition, theoretical analysis and item embedding visualization provide\ninsights into our model interoperability.",
        "Task": "They focus on node-level classification and link prediction tasks over text-attributed graphs. Their experiments utilize multiple benchmark datasets (e.g., DBLP, Wikidata5M, Amazon reviews) to evaluate performance in these tasks.",
        "Data_type": "They focus on text-attributed graphs, where nodes contain textual attributes and edges capture relational structure. The data is homogeneous graphs with text-enriched nodes, suitable for downstream tasks such as link prediction or node classification.",
        "Data_domain": "The paper’s datasets come from text-attributed graphs spanning academic (e.g., DBLP citation networks), e-commerce (e.g., Amazon product data), and knowledge-based corpora (e.g., Wikidata) domains. They encompass large-scale node-link structures enriched with textual attributes across these diverse real-world sources.",
        "ML_phase": "They combine text-attributed graph data (token-, node-, and subgraph-level inputs) into a GNN-nested Transformer framework, where graph sampling and hierarchical encoding produce the training inputs. Next, they jointly optimize five self-supervised contrastive objectives (including a high-frequency–aware spectral term) to learn node embeddings in an end-to-end fashion."
    },
    "TAG/2310.00149v3.json": {
        "title": "One for All: Towards Training One Graph Model for All Classification  Tasks",
        "abs": "Designing a single model to address multiple tasks has been a long-standing\nobjective in artificial intelligence. Recently, large language models have\ndemonstrated exceptional capability in solving different tasks within the\nlanguage domain. However, a unified model for various graph tasks remains\nunderexplored, primarily due to the challenges unique to the graph learning\ndomain. First, graph data from different areas carry distinct attributes and\nfollow different distributions. Such discrepancy makes it hard to represent\ngraphs in a single representation space. Second, tasks on graphs diversify into\nnode, link, and graph tasks, requiring distinct embedding strategies. Finally,\nan appropriate graph prompting paradigm for in-context learning is unclear. We\npropose \\textbf{One for All (OFA)}, the first general framework that can use a\nsingle graph model to address the above challenges. Specifically, OFA proposes\ntext-attributed graphs to unify different graph data by describing nodes and\nedges with natural language and uses language models to encode the diverse and\npossibly cross-domain text attributes to feature vectors in the same embedding\nspace. Furthermore, OFA introduces the concept of nodes-of-interest to\nstandardize different tasks with a single task representation. For in-context\nlearning on graphs, OFA introduces a novel graph prompting paradigm that\nappends prompting substructures to the input graph, which enables it to address\nvaried tasks without fine-tuning. We train the OFA model using graph data from\nmultiple domains (including citation networks, molecular graphs, knowledge\ngraphs, etc.) simultaneously and evaluate its ability in supervised, few-shot,\nand zero-shot learning scenarios. OFA performs well across different tasks,\nmaking it the first general-purpose across-domains classification model on\ngraphs.",
        "Task": "They address classification tasks at node-, link-, and graph-level. They further evaluate performance under supervised, few-shot, and zero-shot classification settings.",
        "Data_type": "They handle graph-structured data across citation networks, knowledge graphs, and molecular graphs by converting all node/edge information (including SMILES) into unified text descriptions (TAGs). This enables node-, link-, and graph-level classification for heterogeneous domains via a single GNN foundation.",
        "Data_domain": "The paper leverages graph datasets spanning multiple domains, including academic citation networks, molecular graphs, and knowledge graphs. These text-attributed graphs cover tasks from node-level classification in citation networks to link-level reasoning in knowledge bases and graph-level prediction in molecular data.",
        "ML_phase": "They unify cross-domain graph datasets by converting them into text-attributed graphs, where each node/edge is described in standardized text form and then embedded by an LLM. A single GNN-based training pipeline then processes these embeddings using a tailored graph prompting paradigm, enabling both supervised and in-context (few-/zero-shot) training."
    },
    "TAG/2205.12454v4.json": {
        "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
        "abs": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph\nTransformer with linear complexity and state-of-the-art results on a diverse\nset of benchmarks. Graph Transformers (GTs) have gained popularity in the field\nof graph representation learning with a variety of recent publications but they\nlack a common foundation about what constitutes a good positional or structural\nencoding, and what differentiates them. In this paper, we summarize the\ndifferent types of encodings with a clearer definition and categorize them as\nbeing $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. The prior GTs\nare constrained to small graphs with a few hundred nodes, here we propose the\nfirst architecture with a complexity linear in the number of nodes and edges\n$O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected\nTransformer. We argue that this decoupling does not negatively affect the\nexpressivity, with our architecture being a universal function approximator on\ngraphs. Our GPS recipe consists of choosing 3 main ingredients: (i)\npositional/structural encoding, (ii) local message-passing mechanism, and (iii)\nglobal attention mechanism. We provide a modular framework $\\textit{GraphGPS}$\nthat supports multiple types of encodings and that provides efficiency and\nscalability both in small and large graphs. We test our architecture on 16\nbenchmarks and show highly competitive results in all of them, show-casing the\nempirical benefits gained by the modularity and the combination of different\nstrategies.",
        "Task": "They address a broad set of supervised tasks at node-, link-, and graph-level (classification and regression). They evaluate on diverse benchmarks (e.g., ZINC, MNIST, CIFAR10, PATTERN, CLUSTER, OGB, LRGB, MalNet-Tiny) to measure performance across multiple domains.",
        "Data_type": "They operate on generic graph-structured data with both node- and edge-level features, including molecular graphs (atom/bond information), superpixel image graphs, protein-protein association networks, and ASTs in source code. These features can be real-valued or structural (e.g., distances, random-walk encodings), enabling broad applicability across molecular, textual, or visual domains of graph data.",
        "Data_domain": "They evaluate models on a diverse set of graph datasets from molecular chemistry, protein-protein networks, superpixel images (MNIST/CIFAR10), and function-call graphs (MalNet-Tiny), as well as synthetic graph benchmarks. Additional tasks include source-code abstract syntax trees (ogbg-code2) and large-scale molecular data (OGB-LSC PCQM4Mv2).",
        "ML_phase": "They introduce a unified framework where, prior to training, node and edge features are augmented with local/global positional and structural encodings, then processed by a layer combining MPNN-based local aggregation and Transformer-like global attention. The training pipeline uses standard optimizers (e.g., AdamW) with warm-up and cosine learning-rate scheduling, and the architecture is designed to flexibly incorporate new encoding or attention modules."
    },
    "TAG/llm_2402.16240v2.json": {
        "title": "High-Frequency-aware Hierarchical Contrastive Selective Coding for  Representation Learning on Text-attributed Graphs",
        "abs": "We investigate node representation learning on text-attributed graphs (TAGs),\nwhere nodes are associated with text information. Although recent studies on\ngraph neural networks (GNNs) and pretrained language models (PLMs) have\nexhibited their power in encoding network and text signals, respectively, less\nattention has been paid to delicately coupling these two types of models on\nTAGs. Specifically, existing GNNs rarely model text in each node in a\ncontextualized way; existing PLMs can hardly be applied to characterize graph\nstructures due to their sequence architecture. To address these challenges, we\npropose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive\nSelective Coding method that integrates GNNs and PLMs into a unified model.\nDifferent from previous \"cascaded architectures\" that directly add GNN layers\nupon a PLM, our HASH-CODE relies on five self-supervised optimization\nobjectives to facilitate thorough mutual enhancement between network and text\nsignals in diverse granularities. Moreover, we show that existing contrastive\nobjective learns the low-frequency component of the augmentation graph and\npropose a high-frequency component (HFC)-aware contrastive learning objective\nthat makes the learned embeddings more distinctive. Extensive experiments on\nsix real-world benchmarks substantiate the efficacy of our proposed approach.\nIn addition, theoretical analysis and item embedding visualization provide\ninsights into our model interoperability.",
        "Task": "They focus on link prediction and node classification tasks on text-attributed graphs. These tasks are evaluated on multiple large-scale datasets, including DBLP, Wikidata5M, and several Amazon product corpora.",
        "Data_type": "They operate on text-attributed graphs where nodes are enriched with textual attributes, forming a homogeneous graph structure. The method integrates both the graph’s topology (edges and neighbor relationships) and each node’s textual content for representation learning.",
        "Data_domain": "The data originates from academic citation graphs, social media networks, and e-commerce product graphs. These text-attributed graphs incorporate content such as paper titles and abstracts, tweets, and product descriptions, forming multi-domain real-world datasets.",
        "ML_phase": "They preprocess text by tokenizing node attributes, sampling neighbor subgraphs, and constructing coherent text-attributed graph inputs. Then, they employ a GNN-nested Transformer design with five self-supervised contrastive objectives, all trained end-to-end to fuse textual and structural signals under a unified co-training framework."
    },
    "TAG/llm_2308.07134v5.json": {
        "title": "Language is All a Graph Needs",
        "abs": "The emergence of large-scale pre-trained language models has revolutionized\nvarious AI research domains. Transformers-based Large Language Models (LLMs)\nhave gradually replaced CNNs and RNNs to unify fields of computer vision and\nnatural language processing. Compared with independent data samples such as\nimages, videos or texts, graphs usually contain rich structural and relational\ninformation. Meanwhile, language, especially natural language, being one of the\nmost expressive mediums, excels in describing complex structures. However,\nexisting work on incorporating graph problems into the generative language\nmodeling framework remains very limited. Considering the rising prominence of\nLLMs, it becomes essential to explore whether LLMs can also replace GNNs as the\nfoundation model for graphs. In this paper, we propose InstructGLM\n(Instruction-finetuned Graph Language Model) with highly scalable prompts based\non natural language instructions. We use natural language to describe\nmulti-scale geometric structure of the graph and then instruction finetune an\nLLM to perform graph tasks, which enables Generative Graph Learning. Our method\nsurpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets,\nunderscoring its effectiveness and sheds light on generative LLMs as new\nfoundation model for graph machine learning. Our code is open-sourced at\nhttps://github.com/agiresearch/InstructGLM.",
        "Task": "They focus on node-level classification and link-level prediction tasks in graphs. The experiments are conducted on standard benchmark datasets, including ogbn-arxiv, Cora, and PubMed.",
        "Data_type": "They handle graph-structured data with textual node attributes, supporting both node- and edge-level information through natural language prompts. Furthermore, the approach generalizes to diverse data modalities (e.g., images, audio) by tokenizing non-text features into LLM-compatible tokens.",
        "Data_domain": "The datasets originate from academic citation networks, such as ogbn-arxiv, Cora, and PubMed, where each node represents a scientific paper with associated text features. Experimental validation focuses on these academic graphs, leveraging paper connections and metadata for tasks like node classification.",
        "ML_phase": "They convert graph structure and features into natural language descriptions via multi-hop sampling and mini-batch processing, producing prompts that serve as the model’s input. Then, they apply a multi-task instruction-tuning framework on a generative LLM backbone, unifying node classification and link prediction within a single end-to-end pipeline."
    },
    "TAG/2402.07630v3.json": {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering",
        "abs": "Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop a\nGraph Question Answering (GraphQA) benchmark with data collected from different\ntasks. Then, we propose our G-Retriever method, introducing the first\nretrieval-augmented generation (RAG) approach for general textual graphs, which\ncan be fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and mitigates\nhallucination.~\\footnote{Our codes and datasets are available at:\n\\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "Task": "They tackle a graph question answering task on textual graphs, testing whether models can accurately answer complex and multi-hop queries. Performance is measured by the correctness of answers to diverse questions spanning commonsense reasoning, scene understanding, and knowledge graph reasoning.",
        "Data_type": "They handle graph-structured data with textual node and edge attributes, including real-world knowledge graphs, scene graphs, and commonsense graphs. Their benchmark integrates tasks such as commonsense reasoning, scene understanding, and knowledge graph QA, all of which involve textualized nodes and edges.",
        "Data_domain": "They compile a benchmark of real-world textual graphs drawn from three sources: common-sense knowledge (ExplaGraphs), scene descriptions (SceneGraphs), and multi-hop knowledge graphs (WebQSP). This covers diverse domains including commonsense reasoning, visual scene understanding, and knowledge-based question answering.",
        "ML_phase": "They implement a multi-stage pipeline: indexing textual graphs with a pre-trained LM, retrieving subgraphs via a PCST-based approach, encoding them through a GNN, and finally feeding both graph token embeddings plus textualized subgraphs into a frozen LLM for answer generation. Training occurs with prompt tuning or LoRA while freezing the LLM’s parameters, thus focusing on efficient fine-tuning and hallucination mitigation via retrieval-augmented generation."
    },
    "TAG/llm_2205.12454v4.json": {
        "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
        "abs": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph\nTransformer with linear complexity and state-of-the-art results on a diverse\nset of benchmarks. Graph Transformers (GTs) have gained popularity in the field\nof graph representation learning with a variety of recent publications but they\nlack a common foundation about what constitutes a good positional or structural\nencoding, and what differentiates them. In this paper, we summarize the\ndifferent types of encodings with a clearer definition and categorize them as\nbeing $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. The prior GTs\nare constrained to small graphs with a few hundred nodes, here we propose the\nfirst architecture with a complexity linear in the number of nodes and edges\n$O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected\nTransformer. We argue that this decoupling does not negatively affect the\nexpressivity, with our architecture being a universal function approximator on\ngraphs. Our GPS recipe consists of choosing 3 main ingredients: (i)\npositional/structural encoding, (ii) local message-passing mechanism, and (iii)\nglobal attention mechanism. We provide a modular framework $\\textit{GraphGPS}$\nthat supports multiple types of encodings and that provides efficiency and\nscalability both in small and large graphs. We test our architecture on 16\nbenchmarks and show highly competitive results in all of them, show-casing the\nempirical benefits gained by the modularity and the combination of different\nstrategies.",
        "Task": "They tackle node-level classification, link-level prediction, and graph-level classification or regression tasks (including multi-label and multi-task settings). Empirical evaluations are conducted across diverse benchmarks (e.g., ZINC, MNIST, CIFAR10, PATTERN, CLUSTER, various OGB datasets, PCQM4Mv2, MalNet-Tiny, and LRGB).",
        "Data_type": "It supports graph-structured data with node- and edge-level features, including textual, chemical (e.g., SMILES/fingerprints), or image-based (superpixel) attributes. The method is applicable to large, heterogeneous graphs from domains such as molecular chemistry, source code ASTs, and function call networks.",
        "Data_domain": "They evaluate graph-structured data from diverse sources, including molecular (ZINC, ogbg-mol, PCQM4Mv2), code-based (ogbg-code2), image superpixels (MNIST, CIFAR10), function call graphs (MalNet), and synthetic SBM-based benchmarks (PATTERN, CLUSTER). These datasets span chemistry, software analysis, computer vision, and more, forming a comprehensive collection of real and synthetic graph domains.",
        "ML_phase": "They propose a pipeline that embeds graph inputs with modular positional and structural encodings, then passes them through a hybrid architecture of local MPNN and global attention layers. The training framework incorporates residual connections, batch normalization, and either linear or standard attention mechanisms to scale efficiently to large graphs."
    },
    "TAG/llm_2307.03393v4.json": {
        "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on  Graphs",
        "abs": "Learning on Graphs has attracted immense attention due to its wide real-world\napplications. The most popular pipeline for learning on graphs with textual\nnode attributes primarily relies on Graph Neural Networks (GNNs), and utilizes\nshallow text embedding as initial node representations, which has limitations\nin general knowledge and profound semantic understanding. In recent years,\nLarge Language Models (LLMs) have been proven to possess extensive common\nknowledge and powerful semantic comprehension abilities that have\nrevolutionized existing workflows to handle text data. In this paper, we aim to\nexplore the potential of LLMs in graph machine learning, especially the node\nclassification task, and investigate two possible pipelines: LLMs-as-Enhancers\nand LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text\nattributes with their massive knowledge and then generate predictions through\nGNNs. The latter attempts to directly employ LLMs as standalone predictors. We\nconduct comprehensive and systematical studies on these two pipelines under\nvarious settings. From comprehensive empirical results, we make original\nobservations and find new insights that open new possibilities and suggest\npromising directions to leverage LLMs for learning on graphs. Our codes and\ndatasets are available at https://github.com/CurryTang/Graph-LLM.",
        "Task": "They address node classification on text‐attributed graphs, where each node contains textual data and labels to predict. Benchmarks include Cora, Citeseer, PubMed, Arxiv, and Products for evaluating node‐level classification performance.",
        "Data_type": "They focus on text‐attributed graphs, where each node is associated with textual content (e.g., abstracts, product descriptions) in homogeneous graph structures. The study primarily addresses node‐level tasks on these textual‐attribute graphs, such as citation networks and product networks.",
        "Data_domain": "The datasets originate primarily from academic citation networks (e.g., Cora, Citeseer, Pubmed, Arxiv) and product networks, with some discussion of social network applications. Consequently, their data domain spans scholarly publications and commercial product information.",
        "ML_phase": "They introduce two pipelines that embed LLMs into a conventional node classification workflow on text-attributed graphs, either as an attribute enhancer (feature/text-level augmentation) or as a direct predictor. Within these pipelines, the authors detail how data are pre-processed into textual prompts or embeddings, propose iterative versus cascading training frameworks, and integrate GNN-based message-passing with LLM-generated features or predictions."
    },
    "TAG/llm_2402.07630v3.json": {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering",
        "abs": "Given a graph with textual attributes, we enable users to `chat with their\ngraph': that is, to ask questions about the graph using a conversational\ninterface. In response to a user's questions, our method provides textual\nreplies and highlights the relevant parts of the graph. While existing works\nintegrate large language models (LLMs) and graph neural networks (GNNs) in\nvarious ways, they mostly focus on either conventional graph tasks (such as\nnode, edge, and graph classification), or on answering simple graph queries on\nsmall or synthetic graphs. In contrast, we develop a flexible\nquestion-answering framework targeting real-world textual graphs, applicable to\nmultiple applications including scene graph understanding, common sense\nreasoning, and knowledge graph reasoning. Toward this goal, we first develop a\nGraph Question Answering (GraphQA) benchmark with data collected from different\ntasks. Then, we propose our G-Retriever method, introducing the first\nretrieval-augmented generation (RAG) approach for general textual graphs, which\ncan be fine-tuned to enhance graph understanding via soft prompting. To resist\nhallucination and to allow for textual graphs that greatly exceed the LLM's\ncontext window size, G-Retriever performs RAG over a graph by formulating this\ntask as a Prize-Collecting Steiner Tree optimization problem. Empirical\nevaluations show that our method outperforms baselines on textual graph tasks\nfrom multiple domains, scales well with larger graph sizes, and mitigates\nhallucination.~\\footnote{Our codes and datasets are available at:\n\\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "Task": "They define a question-answering task on textual graphs (GraphQA), requiring multi-hop reasoning over real-world structured data. They evaluate performance on a newly introduced benchmark spanning common sense reasoning, scene understanding, and knowledge graph QA, using metrics such as accuracy and Hit@1.",
        "Data_type": "They focus on graph-structured data where both nodes and edges carry textual attributes, forming “textual graphs.” These include diverse real-world domains such as commonsense graphs, scene graphs, and knowledge graphs, each represented in a unified textual format for multi-hop queries over node/edge text.",
        "Data_domain": "The data comes from diverse real-world textual graphs encompassing commonsense arguments, visual scenes, and Freebase knowledge graph facts. Specifically, the authors use ExplaGraphs (common sense arguments), SceneGraphs (scene representations), and WebQSP (knowledge graph queries) to span multiple application domains.",
        "ML_phase": "They construct a retrieval-augmented QA pipeline by first converting graphs into text, indexing node/edge embeddings, and then using a k-nearest neighbor plus PCST-based retrieval step to form a subgraph input to a GNN. The GNN output is projected as a soft prompt for a frozen LLM, with training governed by parameter-efficient fine-tuning (e.g., prompt tuning or LoRA) alongside end-to-end GNN optimization."
    },
    "TAG/2209.07972v1.json": {
        "title": "A Multi-turn Machine Reading Comprehension Framework with Rethink  Mechanism for Emotion-Cause Pair Extraction",
        "abs": "Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause\nanalysis, which extracts potential emotion-cause pairs from an emotional\ndocument. Most recent studies use end-to-end methods to tackle the ECPE task.\nHowever, these methods either suffer from a label sparsity problem or fail to\nmodel complicated relations between emotions and causes. Furthermore, they all\ndo not consider explicit semantic information of clauses. To this end, we\ntransform the ECPE task into a document-level machine reading comprehension\n(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism\n(MM-R). Our framework can model complicated relations between emotions and\ncauses while avoiding generating the pairing matrix (the leading cause of the\nlabel sparsity problem). Besides, the multi-turn structure can fuse explicit\nsemantic information flow between emotions and causes. Extensive experiments on\nthe benchmark emotion cause corpus demonstrate the effectiveness of our\nproposed framework, which outperforms existing state-of-the-art methods.",
        "Task": "They tackle a document-level classification task that identifies emotion clauses and their corresponding cause clauses from Chinese news articles. Evaluation is conducted on a publicly available ECPE benchmark dataset (Xia et al. 2019) using standard precision, recall, and F1 metrics.",
        "Data_type": "They process purely textual data from a Chinese news corpus, where each document is segmented into multiple clauses for emotion-cause pair extraction. No multi-modal or graph-structured inputs (e.g., images, SMILES, or fingerprints) are involved.",
        "Data_domain": "The dataset is derived from Chinese news articles published on the SINA NEWS website, reflecting a news media domain.",
        "ML_phase": "They convert the task into a multi-turn MRC pipeline that first identifies emotion clauses, then extracts cause clauses through dynamic queries, and finally verifies each candidate pair with a “rethink” mechanism. A BERT-based encoder (token-level), GAT-based clause encoder, and a single classifier are jointly trained via cross-entropy loss across all turns to complete the end-to-end framework."
    },
    "TAG/2307.03393v4.json": {
        "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on  Graphs",
        "abs": "Learning on Graphs has attracted immense attention due to its wide real-world\napplications. The most popular pipeline for learning on graphs with textual\nnode attributes primarily relies on Graph Neural Networks (GNNs), and utilizes\nshallow text embedding as initial node representations, which has limitations\nin general knowledge and profound semantic understanding. In recent years,\nLarge Language Models (LLMs) have been proven to possess extensive common\nknowledge and powerful semantic comprehension abilities that have\nrevolutionized existing workflows to handle text data. In this paper, we aim to\nexplore the potential of LLMs in graph machine learning, especially the node\nclassification task, and investigate two possible pipelines: LLMs-as-Enhancers\nand LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text\nattributes with their massive knowledge and then generate predictions through\nGNNs. The latter attempts to directly employ LLMs as standalone predictors. We\nconduct comprehensive and systematical studies on these two pipelines under\nvarious settings. From comprehensive empirical results, we make original\nobservations and find new insights that open new possibilities and suggest\npromising directions to leverage LLMs for learning on graphs. Our codes and\ndatasets are available at https://github.com/CurryTang/Graph-LLM.",
        "Task": "They address a node-level classification task on text-attributed graphs, treating it as a multi-class classification problem. To evaluate performance, they employ popular graph datasets such as Cora, Citeseer, PubMed, Arxiv, and Products.",
        "Data_type": "They handle text-attributed, homogeneous graph data where each node is associated with textual attributes and edges represent relationships (e.g., citations or product links). No multi-modal images, SMILES/fingerprint, or other data types are considered.",
        "Data_domain": "The paper’s data comes from academic citation graphs (e.g., Cora, Citeseer, PubMed, ArXiv) and product networks, where nodes contain text attributes like paper abstracts or product descriptions. Edges represent relationships such as citations or product connections in these domains.",
        "ML_phase": "They propose two main pipelines for incorporating LLMs into text-attributed graph node classification: (1) “LLMs-as-Enhancers,” where text attributes are either encoded directly as initial node features or augmented at the text level before passing them to GNNs, and (2) “LLMs-as-Predictors,” where structural and attribute information is reformulated into textual prompts for direct inference. The training frameworks include cascading and iterative co-training structures for feature-level enhancement, as well as text-level augmentation or instruction-based prompting for LLM-centered prediction."
    },
    "TAG/llm_2202.08455v1.json": {
        "title": "Transformer for Graphs: An Overview from Architecture Perspective",
        "abs": "Recently, Transformer model, which has achieved great success in many\nartificial intelligence fields, has demonstrated its great potential in\nmodeling graph-structured data. Till now, a great variety of Transformers has\nbeen proposed to adapt to the graph-structured data. However, a comprehensive\nliterature review and systematical evaluation of these Transformer variants for\ngraphs are still unavailable. It's imperative to sort out the existing\nTransformer models for graphs and systematically investigate their\neffectiveness on various graph tasks. In this survey, we provide a\ncomprehensive review of various Graph Transformer models from the architectural\ndesign perspective. We first disassemble the existing models and conclude three\ntypical ways to incorporate the graph information into the vanilla Transformer:\n1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and\n3) Improved Attention Matrix from Graphs. Furthermore, we implement the\nrepresentative components in three groups and conduct a comprehensive\ncomparison on various kinds of famous graph data benchmarks to investigate the\nreal performance gain of each component. Our experiments confirm the benefits\nof current graph-specific modules on Transformer and reveal their advantages on\ndifferent kinds of graph tasks.",
        "Task": "They tackle both node-level classification (on datasets like Flickr, ogbn-product, ogbn-arxiv) and graph-level classification or regression (on datasets such as ZINC, ogbg-molhiv, ogbg-molpcba). Performance is measured via accuracy for classification and mean absolute error for regression.",
        "Data_type": "They principally deal with graph-structured data, where nodes and edges can have numeric or textual attributes (e.g., molecular graphs with SMILES annotations, large-scale social networks, citation networks). The datasets span both many small graphs (e.g., molecular) and single large graphs (e.g., ogbn-arxiv), generally focusing on homogeneous, attributed graphs.",
        "Data_domain": "This paper concentrates on graph-structured data drawn from diverse sources, including molecular (chemistry), biological, social media, and academic citation networks. Specifically, benchmarks such as ZINC (molecular), ogbg-molhiv/ogbg-molpcba (chemical), Flickr (social network), ogbn-arxiv, and ogbn-product (academic/large-scale networks) are employed.",
        "ML_phase": "They position their approach within the model architecture design phase by injecting graph priors into Transformer blocks via three strategies: GNN modules, graph-based positional embeddings, and attention matrix modifications. They train these variants with consistent hyperparameters and, for large graphs, employ a subgraph sampling strategy to manage computational costs."
    },
    "TAG/2105.02605v3.json": {
        "title": "GraphFormers: GNN-nested Transformers for Representation Learning on  Textual Graph",
        "abs": "The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.",
        "Task": "They focus on link prediction in textual graphs, aiming to determine whether two nodes should be connected based on learned embeddings. The approach is evaluated on large-scale datasets (DBLP, Wiki, Product) by measuring link prediction accuracy.",
        "Data_type": "They handle textual node attributes on large-scale graphs, including both homogeneous and heterogeneous structures. The approach focuses solely on the text assigned to nodes, without incorporating edge attributes or other data modalities.",
        "Data_domain": "They use three large-scale textual graphs spanning an academic citation network (DBLP), a knowledge graph from Wikipedia (Wikidata5M), and an e-commerce product network. Thus, the data domain includes academic, knowledge-based, and commercial environments.",
        "ML_phase": "Data processing comprises tokenizing each node’s text, sampling neighbor nodes, and preparing them as inputs to a GNN-nested Transformers architecture for iterative feature encoding. Training proceeds via a two-stage progressive scheme, initially masking tokens to enforce neighborhood reliance and then fine-tuning on unmasked data for final adaptation."
    },
    "TAG/2106.03893v3.json": {
        "title": "Rethinking Graph Transformers with Spectral Attention",
        "abs": "In recent years, the Transformer architecture has proven to be very\nsuccessful in sequence processing, but its application to other data\nstructures, such as graphs, has remained limited due to the difficulty of\nproperly defining positions. Here, we present the $\\textit{Spectral Attention\nNetwork}$ (SAN), which uses a learned positional encoding (LPE) that can take\nadvantage of the full Laplacian spectrum to learn the position of each node in\na given graph. This LPE is then added to the node features of the graph and\npassed to a fully-connected Transformer. By leveraging the full spectrum of the\nLaplacian, our model is theoretically powerful in distinguishing graphs, and\ncan better detect similar sub-structures from their resonance. Further, by\nfully connecting the graph, the Transformer does not suffer from\nover-squashing, an information bottleneck of most GNNs, and enables better\nmodeling of physical phenomenons such as heat transfer and electric\ninteraction. When tested empirically on a set of 4 standard datasets, our model\nperforms on par or better than state-of-the-art GNNs, and outperforms any\nattention-based model by a wide margin, becoming the first fully-connected\narchitecture to perform well on graph benchmarks.",
        "Task": "They address graph-level regression on the ZINC dataset, node-level classification on the PATTERN and CLUSTER datasets, and graph-level classification on MolHIV and MolPCBA. Performance is evaluated through established benchmarks assessing regression accuracy and classification metrics at both node- and graph-level scales.",
        "Data_type": "They use graph-structured data with both node and edge features, focusing on molecular graphs (e.g., MolHIV, MolPCBA) and synthetic graph benchmarks (ZINC, PATTERN, CLUSTER). Each graph has node features of dimension d (e.g., atom-level or synthetic node attributes), optional edge features, and an N×N adjacency structure.",
        "Data_domain": "The paper uses datasets spanning synthetic social graphs (via Stochastic Block Models such as PATTERN and CLUSTER) and molecular graphs (e.g., ZINC, MolHIV, and MolPCBA), covering both social network and chemical domains.",
        "ML_phase": "They construct a Transformer-based GNN pipeline that first computes Laplacian eigenfunctions as node-wise (or edge-wise) positional encodings, then feeds these encodings into a multi-layer attention framework combining local edges with full connectivity. The training protocol employs Adam optimization with learning rate scheduling on standard graph benchmarks, using ablation studies and hyperparameter tuning for performance improvements."
    },
    "TAG/2205.10282v2.json": {
        "title": "Heterformer: Transformer-based Deep Node Representation Learning on  Heterogeneous Text-Rich Networks",
        "abs": "Representation learning on networks aims to derive a meaningful vector\nrepresentation for each node, thereby facilitating downstream tasks such as\nlink prediction, node classification, and node clustering. In heterogeneous\ntext-rich networks, this task is more challenging due to (1) presence or\nabsence of text: Some nodes are associated with rich textual information, while\nothers are not; (2) diversity of types: Nodes and edges of multiple types form\na heterogeneous network structure. As pretrained language models (PLMs) have\ndemonstrated their effectiveness in obtaining widely generalizable text\nrepresentations, a substantial amount of effort has been made to incorporate\nPLMs into representation learning on text-rich networks. However, few of them\ncan jointly consider heterogeneous structure (network) information as well as\nrich textual semantic information of each node effectively. In this paper, we\npropose Heterformer, a Heterogeneous Network-Empowered Transformer that\nperforms contextualized text encoding and heterogeneous structure encoding in a\nunified model. Specifically, we inject heterogeneous structure information into\neach Transformer layer when encoding node texts. Meanwhile, Heterformer is\ncapable of characterizing node/edge type heterogeneity and encoding nodes with\nor without texts. We conduct comprehensive experiments on three tasks (i.e.,\nlink prediction, node classification, and node clustering) on three large-scale\ndatasets from different domains, where Heterformer outperforms competitive\nbaselines significantly and consistently.",
        "Task": "They aim to learn node representations on heterogeneous text-rich networks for link prediction, node classification, and node clustering. Experiments are conducted on DBLP, Twitter, and Goodreads to evaluate performance on these tasks.",
        "Data_type": "They work with heterogeneous text-rich graph data containing multi-typed nodes and edges, where certain node types have associated textual documents while others do not. Across academic, social media, and product networks, each node can either hold semantically rich text or be textless, capturing both structural and textual attributes for downstream tasks.",
        "Data_domain": "They use real-world text-rich heterogeneous networks spanning three main domains: academic (DBLP), social media (Twitter), and e-commerce (Goodreads). These datasets contain multi-typed nodes (e.g., papers, tweets, books) and edges, with some nodes accompanied by rich textual information.",
        "ML_phase": "They introduce a Transformer-based architecture that fuses text (from a pretrained language model) and heterogeneous network structure via “virtual neighbor tokens,” alongside a warm-up stage for textless node embeddings. The system is trained end-to-end on an unsupervised link prediction objective with negative sampling, enabling node representation learning across both text-rich and textless nodes."
    },
    "TAG/llm_2112.11070v1.json": {
        "title": "An Inference Approach To Question Answering Over Knowledge Graphs",
        "abs": "Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.",
        "Task": "They formulate multi-hop question answering over knowledge graphs as a classification task (entailment vs. contradiction), converting each query into premise-hypothesis pairs. They evaluate performance on the MetaQA and PathQuestions datasets.",
        "Data_type": "They handle graph-structured data with entities (as nodes) and relations (as edges), where the node attributes and queries are in natural language text. Their experiments focus on textual node attributes in homogeneous knowledge graphs (e.g., movie-based or Freebase-derived) without involving other modalities such as images or chemical fingerprints.",
        "Data_domain": "They use datasets anchored in movie-centric knowledge graphs (e.g., the WikiMovies graph and a Freebase subset), covering entities like films, directors, and actors.",
        "ML_phase": "They construct premise-hypothesis examples by extracting paths from the knowledge graph, linking entities, and converting questions into multiple-choice pairs with labeled entailment or contradiction. Subsequently, they train and evaluate ESIM and HRPE architectures within a standard cross-entropy framework, leveraging LSTM-based encoders and attention mechanisms to perform QA inference."
    },
    "TAG/2202.08455v1.json": {
        "title": "Transformer for Graphs: An Overview from Architecture Perspective",
        "abs": "Recently, Transformer model, which has achieved great success in many\nartificial intelligence fields, has demonstrated its great potential in\nmodeling graph-structured data. Till now, a great variety of Transformers has\nbeen proposed to adapt to the graph-structured data. However, a comprehensive\nliterature review and systematical evaluation of these Transformer variants for\ngraphs are still unavailable. It's imperative to sort out the existing\nTransformer models for graphs and systematically investigate their\neffectiveness on various graph tasks. In this survey, we provide a\ncomprehensive review of various Graph Transformer models from the architectural\ndesign perspective. We first disassemble the existing models and conclude three\ntypical ways to incorporate the graph information into the vanilla Transformer:\n1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and\n3) Improved Attention Matrix from Graphs. Furthermore, we implement the\nrepresentative components in three groups and conduct a comprehensive\ncomparison on various kinds of famous graph data benchmarks to investigate the\nreal performance gain of each component. Our experiments confirm the benefits\nof current graph-specific modules on Transformer and reveal their advantages on\ndifferent kinds of graph tasks.",
        "Task": "They address both graph-level and node-level supervised tasks, specifically regression (ZINC) and classification (ogbg-molhiv, ogbg-molpcba) at the graph level, and node classification (Flickr, ogbn-arxiv, ogbn-product) at the node level. These datasets serve as benchmarks to systematically evaluate the performance of the proposed Transformer-based methods on graph-structured data.",
        "Data_type": "They exclusively address graph-structured data, incorporating both node- and edge-level information (e.g., adjacency matrices, node features, and optional edge features). The paper’s scope spans heterogeneous tasks on small molecular graphs, large single-network benchmarks, and other standard graph domains, but does not involve text data, images, SMILES, or multi-modal inputs.",
        "Data_domain": "They employ diverse graph datasets drawn from molecular (chemical) domains, social media interactions, citation networks, and e-commerce product co-purchasing. In particular, the experiments involve datasets such as ZINC (chemical), ogbg-molhiv/ogbg-molpcba (chemical), Flickr (social network), ogbn-arxiv (academic citation), and ogbn-product (e-commerce).",
        "ML_phase": "They systematically incorporate graph priors into Transformers at the model architecture level (via GNN modules, positional embeddings, or attention constraints) and provide a unified training setup to evaluate these variants. They also detail subgraph sampling for large-scale graphs, fix key hyperparameters (e.g., batch size, dropout, learning rate), and conduct fair ablation studies under consistent evaluation protocols."
    },
    "TAG/llm_2106.03893v3.json": {
        "title": "Rethinking Graph Transformers with Spectral Attention",
        "abs": "In recent years, the Transformer architecture has proven to be very\nsuccessful in sequence processing, but its application to other data\nstructures, such as graphs, has remained limited due to the difficulty of\nproperly defining positions. Here, we present the $\\textit{Spectral Attention\nNetwork}$ (SAN), which uses a learned positional encoding (LPE) that can take\nadvantage of the full Laplacian spectrum to learn the position of each node in\na given graph. This LPE is then added to the node features of the graph and\npassed to a fully-connected Transformer. By leveraging the full spectrum of the\nLaplacian, our model is theoretically powerful in distinguishing graphs, and\ncan better detect similar sub-structures from their resonance. Further, by\nfully connecting the graph, the Transformer does not suffer from\nover-squashing, an information bottleneck of most GNNs, and enables better\nmodeling of physical phenomenons such as heat transfer and electric\ninteraction. When tested empirically on a set of 4 standard datasets, our model\nperforms on par or better than state-of-the-art GNNs, and outperforms any\nattention-based model by a wide margin, becoming the first fully-connected\narchitecture to perform well on graph benchmarks.",
        "Task": "They address node-level classification tasks on the PATTERN and CLUSTER datasets and graph-level classification or regression tasks on ZINC, MolHIV, and MolPCBA. These benchmarks are used to evaluate performance on both synthetic (PATTERN, CLUSTER, ZINC) and real-world molecular (MolHIV, MolPCBA) tasks.",
        "Data_type": "They process homogeneous graph-structured data with node- and edge-level features, leveraging Laplacian-based encodings for tasks like molecular property prediction and node classification. No multi-modal or textual attributes are handled; the approach focuses solely on standard adjacency representations within graphs.",
        "Data_domain": "They evaluate the proposed methods on synthetic graph benchmarks derived from Stochastic Block Models (simulating social-network-like data) and on real-world molecular datasets (e.g., ZINC, MolHIV, MolPCBA). Thus, the study covers both synthetic social-style networks and chemistry-focused molecular graphs.",
        "ML_phase": "The paper describes a pipeline where graph Laplacian eigenfunctions are computed to form learned positional encodings, which are then concatenated or transformed into node or edge features. These features feed into a full-graph or sparse attention Transformer architecture, trained end-to-end with hyperparameter tuning on standard benchmarks."
    },
    "TAG/2210.14709v2.json": {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "abs": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by training large language models and GNNs together. In this paper, we\npropose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows training the two modules separately while simultaneously\nallowing the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
        "Task": "They address node classification on text-attributed graphs. Experiments are conducted on large-scale benchmarks (ogbn-arxiv, ogbn-products, ogbn-papers100M) to assess classification performance.",
        "Data_type": "They handle text-attributed homogeneous graphs, where each node has textual attributes (e.g., titles, abstracts) and edges encode adjacency information. The text dimension is node-level, and the graph dimension captures structural connections among nodes.",
        "Data_domain": "These datasets are drawn from large-scale text-attributed graphs spanning academic citation networks (ogbn-arxiv, ogbn-papers100M) and an e-commerce co-purchase network (ogbn-products). The textual content includes paper titles and abstracts in the academic domain, as well as product descriptions in the e-commerce domain.",
        "ML_phase": "They propose an iterative EM-style training pipeline where a language model (LM) and a graph neural network (GNN) are alternately optimized. The LM is updated using gold plus GNN-generated pseudo-labels, and the GNN is trained on LM-generated embeddings and pseudo-labels, enabling large-scale node representation learning in text-attributed graphs."
    },
    "TAG/llm_2310.00149v3.json": {
        "title": "One for All: Towards Training One Graph Model for All Classification  Tasks",
        "abs": "Designing a single model to address multiple tasks has been a long-standing\nobjective in artificial intelligence. Recently, large language models have\ndemonstrated exceptional capability in solving different tasks within the\nlanguage domain. However, a unified model for various graph tasks remains\nunderexplored, primarily due to the challenges unique to the graph learning\ndomain. First, graph data from different areas carry distinct attributes and\nfollow different distributions. Such discrepancy makes it hard to represent\ngraphs in a single representation space. Second, tasks on graphs diversify into\nnode, link, and graph tasks, requiring distinct embedding strategies. Finally,\nan appropriate graph prompting paradigm for in-context learning is unclear. We\npropose \\textbf{One for All (OFA)}, the first general framework that can use a\nsingle graph model to address the above challenges. Specifically, OFA proposes\ntext-attributed graphs to unify different graph data by describing nodes and\nedges with natural language and uses language models to encode the diverse and\npossibly cross-domain text attributes to feature vectors in the same embedding\nspace. Furthermore, OFA introduces the concept of nodes-of-interest to\nstandardize different tasks with a single task representation. For in-context\nlearning on graphs, OFA introduces a novel graph prompting paradigm that\nappends prompting substructures to the input graph, which enables it to address\nvaried tasks without fine-tuning. We train the OFA model using graph data from\nmultiple domains (including citation networks, molecular graphs, knowledge\ngraphs, etc.) simultaneously and evaluate its ability in supervised, few-shot,\nand zero-shot learning scenarios. OFA performs well across different tasks,\nmaking it the first general-purpose across-domains classification model on\ngraphs.",
        "Task": "They address classification tasks at node-, link-, and graph-level across a diverse set of graph datasets. The paper evaluates performance under supervised, few-shot, and zero-shot settings on citation networks, knowledge graphs, and molecular graphs.",
        "Data_type": "They handle node and edge textual descriptions from multiple graph domains (citation networks, knowledge graphs, molecules via SMILES), all unified as text-attributed graphs. Each node and edge is represented by a fixed-length LLM-based text embedding (e.g., dimension 768), enabling cross-domain integration in a single embedding space.",
        "Data_domain": "They incorporate graph datasets from diverse domains, primarily academic citation networks, knowledge graphs, and molecular structures. These datasets reflect cross-domain sources such as computer science literature, semantic entity relations, and biochemical compounds.",
        "ML_phase": "They convert diverse graph datasets into text-attributed inputs and generate embeddings via an LLM, forming a unified feature space. A subgraph-based GNN with prompt nodes is then trained end-to-end for node-, link-, and graph-level classification under supervised, few-shot, and zero-shot modes."
    },
    "TAG/2012.09699v2.json": {
        "title": "A Generalization of Transformer Networks to Graphs",
        "abs": "We propose a generalization of transformer neural network architecture for\narbitrary graphs. The original transformer was designed for Natural Language\nProcessing (NLP), which operates on fully connected graphs representing all\nconnections between the words in a sequence. Such architecture does not\nleverage the graph connectivity inductive bias, and can perform poorly when the\ngraph topology is important and has not been encoded into the node features. We\nintroduce a graph transformer with four new properties compared to the standard\nmodel. First, the attention mechanism is a function of the neighborhood\nconnectivity for each node in the graph. Second, the positional encoding is\nrepresented by the Laplacian eigenvectors, which naturally generalize the\nsinusoidal positional encodings often used in NLP. Third, the layer\nnormalization is replaced by a batch normalization layer, which provides faster\ntraining and better generalization performance. Finally, the architecture is\nextended to edge feature representation, which can be critical to tasks s.a.\nchemistry (bond type) or link prediction (entity relationship in knowledge\ngraphs). Numerical experiments on a graph benchmark demonstrate the performance\nof the proposed graph transformer architecture. This work closes the gap\nbetween the original transformer, which was designed for the limited case of\nline graphs, and graph neural networks, that can work with arbitrary graphs. As\nour architecture is simple and generic, we believe it can be used as a black\nbox for future applications that wish to consider transformer and graphs.",
        "Task": "They evaluate performance on a graph-level regression task using the ZINC dataset (predicting molecular properties) and on node-level classification tasks using the PATTERN and CLUSTER datasets (classifying each node into communities).",
        "Data_type": "They handle homogeneous graph-structured data with node and (optionally) edge features, including molecular graphs with bond attributes. Other modalities such as pure text or image-text multi-modal data are not addressed.",
        "Data_domain": "They primarily employ molecular graphs (ZINC) and synthetic SBM-based graphs (PATTERN, CLUSTER) in their experiments. The paper also mentions broader applicability in domains such as knowledge graphs, social sciences, and physics.",
        "ML_phase": "They first preprocess graphs by computing Laplacian eigenvectors as node positional encodings, linearly projecting node/edge features, and forming sparse adjacency for attention. Then they train a multi-layer Graph Transformer with local attention heads, residual/normalization/FFN blocks, and task-specific MLP heads for graph or node predictions."
    },
    "TAG/2305.12268v1.json": {
        "title": "Patton: Language Model Pretraining on Text-Rich Networks",
        "abs": "A real-world text corpus sometimes comprises not only text documents but also\nsemantic links between them (e.g., academic papers in a bibliographic network\nare linked by citations and co-authorships). Text documents and semantic\nconnections form a text-rich network, which empowers a wide range of downstream\ntasks such as classification and retrieval. However, pretraining methods for\nsuch structures are still lacking, making it difficult to build one generic\nmodel that can be adapted to various tasks on text-rich networks. Current\npretraining objectives, such as masked language modeling, purely model texts\nand do not take inter-document structure information into consideration. To\nthis end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.\nPatton includes two pretraining strategies: network-contextualized masked\nlanguage modeling and masked node prediction, to capture the inherent\ndependency between textual attributes and network structure. We conduct\nexperiments on four downstream tasks in five datasets from both academic and\ne-commerce domains, where Patton outperforms baselines significantly and\nconsistently.",
        "Task": "They tackle node-level classification, retrieval, reranking, and link prediction tasks on text-rich networks. They evaluate performance on multiple academic (MAG) and e-commerce (Amazon) datasets, each providing labels and links to support those four tasks.",
        "Data_type": "They operate on text-rich networks represented as homogeneous graphs, where each node is associated with textual attributes and edges encode semantic relationships between documents. Their approach leverages these node-level textual embeddings alongside the graph's edge structures to learn context-aware representations.",
        "Data_domain": "The paper’s data comes from two domains: academic networks (Microsoft Academic Graph) and e-commerce networks (Amazon).",
        "ML_phase": "They utilize a GNN-nested Transformer (GraphFormers) architecture and jointly train it with two pretraining objectives (network-contextualized MLM and masked node prediction) on text-rich networks. The model is then fine-tuned for classification, retrieval, reranking, and link prediction within the same framework, where each downstream task uses the final CLS representation, optionally combined with neighbor information if available."
    },
    "TAG/2308.07134v5.json": {
        "title": "Language is All a Graph Needs",
        "abs": "The emergence of large-scale pre-trained language models has revolutionized\nvarious AI research domains. Transformers-based Large Language Models (LLMs)\nhave gradually replaced CNNs and RNNs to unify fields of computer vision and\nnatural language processing. Compared with independent data samples such as\nimages, videos or texts, graphs usually contain rich structural and relational\ninformation. Meanwhile, language, especially natural language, being one of the\nmost expressive mediums, excels in describing complex structures. However,\nexisting work on incorporating graph problems into the generative language\nmodeling framework remains very limited. Considering the rising prominence of\nLLMs, it becomes essential to explore whether LLMs can also replace GNNs as the\nfoundation model for graphs. In this paper, we propose InstructGLM\n(Instruction-finetuned Graph Language Model) with highly scalable prompts based\non natural language instructions. We use natural language to describe\nmulti-scale geometric structure of the graph and then instruction finetune an\nLLM to perform graph tasks, which enables Generative Graph Learning. Our method\nsurpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets,\nunderscoring its effectiveness and sheds light on generative LLMs as new\nfoundation model for graph machine learning. Our code is open-sourced at\nhttps://github.com/agiresearch/InstructGLM.",
        "Task": "They focus on node-level classification and link prediction using citation networks, specifically ogbn-arxiv, Cora, and PubMed. Both tasks are fundamental to graph learning, involving categorizing individual nodes and predicting connections between nodes.",
        "Data_type": "They handle graph-structured data with textual node attributes (e.g., paper abstracts), incorporating node-and-edge features in a homogeneous setting for tasks like node classification and link prediction. This method also supports multimodal inputs (e.g., images or user profiles) by converting various node/edge features into LLM-compatible tokens.",
        "Data_domain": "They use academic citation networks (ogbn-arxiv, Cora, PubMed). Each dataset contains scholarly papers (e.g., titles and abstracts) as nodes with citation links as edges.",
        "ML_phase": "They transform graph structures (nodes, edges, and multi-hop connectivity) into natural language prompts, integrate these prompts with a large language model via token embeddings, and then apply a multi-prompt instruction-tuning framework for node classification and link prediction. The training scheme uses a generative approach that fuses data processing (neighbor sampling and prompt creation) and model architecture (LLM backbone) into a unified pipeline."
    },
    "TAG/llm_2305.19523v5.json": {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced  Text-Attributed Graph Representation Learning",
        "abs": "Representation learning on text-attributed graphs (TAGs) has become a\ncritical research problem in recent years. A typical example of a TAG is a\npaper citation graph, where the text of each paper serves as node attributes.\nInitial graph neural network (GNN) pipelines handled these text attributes by\ntransforming them into shallow or hand-crafted features, such as skip-gram or\nbag-of-words features. Recent efforts have focused on enhancing these pipelines\nwith language models (LMs), which typically demand intricate designs and\nsubstantial computational resources. With the advent of powerful large language\nmodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and\nto utilize general knowledge, there is a growing need for techniques which\ncombine the textual modelling abilities of LLMs with the structural learning\ncapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to\ncapture textual information as features, which can be used to boost GNN\nperformance on downstream tasks. A key innovation is our use of explanations as\nfeatures: we prompt an LLM to perform zero-shot classification, request textual\nexplanations for its decision-making process, and design an LLM-to-LM\ninterpreter to translate these explanations into informative features for\ndownstream GNNs. Our experiments demonstrate that our method achieves\nstate-of-the-art results on well-established TAG datasets, including Cora,\nPubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23.\nFurthermore, our method significantly speeds up training, achieving a 2.88\ntimes improvement over the closest baseline on ogbn-arxiv. Lastly, we believe\nthe versatility of the proposed method extends beyond TAGs and holds the\npotential to enhance other tasks involving graph-text data. Our codes and\ndatasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "Task": "They pursue a node-level multi-class classification task on text-attributed graphs (TAGs). Their method is evaluated on five TAG datasets—Cora, PubMed, ogbn-arxiv, ogbn-products, and tape-arxiv23—to predict node labels.",
        "Data_type": "They focus on text-attributed graphs, where nodes (e.g., papers) have textual attributes (such as titles and abstracts) and edges represent relationships (e.g., citations). The data is homogeneous graph-structured, with each node richly described by text.",
        "Data_domain": "The datasets primarily consist of academic papers (Cora, PubMed, arXiv) forming citation networks—i.e., text-attributed graphs where each node is a scientific publication. Additionally, an e-commerce domain is included (ogbn-products) featuring product co-purchasing relationships.",
        "ML_phase": "They propose a pipeline that first queries an LLM to produce textual explanations and predictions, then fine-tunes a smaller LM to transform these outputs into node features, and finally uses these enriched features for downstream GNN training. Implementation involves a dedicated prompting stage for explanation generation, an LM-based interpreter for feature extraction, and a separate GNN training step, each occupying distinct phases in the ML workflow."
    },
    "TAG/2112.11070v1.json": {
        "title": "An Inference Approach To Question Answering Over Knowledge Graphs",
        "abs": "Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.",
        "Task": "They frame multi-hop question answering over knowledge graphs as a natural language inference classification task, where each potential answer is labeled as “entailment” or “contradiction.” They evaluate performance on the MetaQA and PathQuestions datasets.",
        "Data_type": "They focus on graph-structured data where each node and edge corresponds to textual entities and relations within knowledge graphs. The node attributes (e.g., entity names) and edge labels (e.g., relation types) are all textual.",
        "Data_domain": "The data primarily comes from large-scale knowledge graphs in the movie domain (MetaQA) and from a subset of Freebase (PathQuestions). These knowledge graphs cover entities such as movies, actors, directors, and related relationships.",
        "ML_phase": "They transform a KG-based QA task into an inference problem by extracting query entities, generating potential answers and corresponding premise paths, and converting each question into labeled premise-hypothesis pairs. They then train either an ESIM or the proposed Hierarchical Recurrent Path Encoder using Adam’s optimizer with batch normalization and dropout to predict the correct answers."
    },
    "TAG/2305.19523v5.json": {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced  Text-Attributed Graph Representation Learning",
        "abs": "Representation learning on text-attributed graphs (TAGs) has become a\ncritical research problem in recent years. A typical example of a TAG is a\npaper citation graph, where the text of each paper serves as node attributes.\nInitial graph neural network (GNN) pipelines handled these text attributes by\ntransforming them into shallow or hand-crafted features, such as skip-gram or\nbag-of-words features. Recent efforts have focused on enhancing these pipelines\nwith language models (LMs), which typically demand intricate designs and\nsubstantial computational resources. With the advent of powerful large language\nmodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and\nto utilize general knowledge, there is a growing need for techniques which\ncombine the textual modelling abilities of LLMs with the structural learning\ncapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to\ncapture textual information as features, which can be used to boost GNN\nperformance on downstream tasks. A key innovation is our use of explanations as\nfeatures: we prompt an LLM to perform zero-shot classification, request textual\nexplanations for its decision-making process, and design an LLM-to-LM\ninterpreter to translate these explanations into informative features for\ndownstream GNNs. Our experiments demonstrate that our method achieves\nstate-of-the-art results on well-established TAG datasets, including Cora,\nPubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23.\nFurthermore, our method significantly speeds up training, achieving a 2.88\ntimes improvement over the closest baseline on ogbn-arxiv. Lastly, we believe\nthe versatility of the proposed method extends beyond TAGs and holds the\npotential to enhance other tasks involving graph-text data. Our codes and\ndatasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "Task": "They conduct a node-level classification task on text-attributed graphs. Their evaluations use datasets such as Cora, PubMed, ogbn-arxiv, ogbn-products, and a newly introduced tape-arxiv23 for benchmarking performance.",
        "Data_type": "They handle text-attributed, homogeneous graph data, with nodes often representing documents or sentences and edges capturing their relationships. Each node is enriched with textual attributes (e.g., titles, abstracts), enabling representation learning on citation networks and similar TAGs.",
        "Data_domain": "The paper’s datasets primarily derive from academic publications (e.g., Cora, PubMed, arXiv) and an e-commerce product co-purchasing network. Each dataset includes text attributes such as paper abstracts and titles or product descriptions, forming text-attributed graphs.",
        "ML_phase": "They first query an LLM for ranked predictions and textual explanations of each node’s text attributes, then treat these explanations (and original text) as input to a smaller LM for feature extraction. These enriched embeddings are passed to a GNN for final training, keeping the LLM stage decoupled from the GNN to reduce complexity."
    },
    "TAG/llm_2101.06323v3.json": {
        "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored  Search",
        "abs": "Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.",
        "Task": "They tackle a binary classification task that predicts Ad relevance in a sponsored search setting using human-annotated labels (e.g., relevant vs. non-relevant). Model performance is primarily evaluated with ROC-AUC on offline labeled data and further validated via online metrics (e.g., Revenue Per Mille).",
        "Data_type": "They use textual queries and advertiser keywords (short texts) as node attributes, and edges derived from user click logs to form a query-keyword behavior graph. The resulting graph is homogeneous with each node containing textual information, and edges encoding click-based relationships.",
        "Data_domain": "The data is drawn from a large-scale sponsored search platform’s logs, containing user queries, advertiser keywords, and associated click behaviors. User behavior graphs constructed from click logs and approximate nearest neighbor searches further augment this textual data.",
        "ML_phase": "User logs are processed to build a click graph (and use ANN if necessary) to augment query-keyword pairs with additional neighbor information, while a large teacher model provides soft labels for training via knowledge distillation. The final model adopts a twin-tower text encoder architecture combined with a GNN aggregator, producing representations usable under strict latency constraints in sponsored search."
    },
    "TAG/llm_2012.09699v2.json": {
        "title": "A Generalization of Transformer Networks to Graphs",
        "abs": "We propose a generalization of transformer neural network architecture for\narbitrary graphs. The original transformer was designed for Natural Language\nProcessing (NLP), which operates on fully connected graphs representing all\nconnections between the words in a sequence. Such architecture does not\nleverage the graph connectivity inductive bias, and can perform poorly when the\ngraph topology is important and has not been encoded into the node features. We\nintroduce a graph transformer with four new properties compared to the standard\nmodel. First, the attention mechanism is a function of the neighborhood\nconnectivity for each node in the graph. Second, the positional encoding is\nrepresented by the Laplacian eigenvectors, which naturally generalize the\nsinusoidal positional encodings often used in NLP. Third, the layer\nnormalization is replaced by a batch normalization layer, which provides faster\ntraining and better generalization performance. Finally, the architecture is\nextended to edge feature representation, which can be critical to tasks s.a.\nchemistry (bond type) or link prediction (entity relationship in knowledge\ngraphs). Numerical experiments on a graph benchmark demonstrate the performance\nof the proposed graph transformer architecture. This work closes the gap\nbetween the original transformer, which was designed for the limited case of\nline graphs, and graph neural networks, that can work with arbitrary graphs. As\nour architecture is simple and generic, we believe it can be used as a black\nbox for future applications that wish to consider transformer and graphs.",
        "Task": "They address graph-level regression on ZINC for molecular property prediction and node-level classification on PATTERN and CLUSTER for community-label assignments. These tasks respectively involve predicting a continuous solubility score for each molecule and assigning discrete labels to each node in synthetic graphs.",
        "Data_type": "They handle homogeneous graph-structured data with node features and optional edge attributes (e.g., molecular graphs with bond information). There is no coverage of multi-modal or purely textual data, as their focus remains on node- and edge-level encodings within arbitrary graph structures.",
        "Data_domain": "They use molecular data from the ZINC dataset and synthetic graph data generated with the Stochastic Block Models (PATTERN and CLUSTER). These datasets span chemical (ZINC) and synthetic domains for node and graph-level tasks.",
        "ML_phase": "They precompute Laplacian eigenvectors for node positions, linearly embed node/edge features (if available), and feed these embeddings into stacked Graph Transformer layers with attention-based message passing. The final representations then go through an MLP for either node- or graph-level prediction, trained end-to-end via backpropagation with batch/layer normalization."
    },
    "Additional/2302.11050v1.json": {
        "title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on  Textual-Edge Networks",
        "abs": "Edges in many real-world social/information networks are associated with rich\ntext information (e.g., user-user communications or user-product reviews).\nHowever, mainstream network representation learning models focus on propagating\nand aggregating node attributes, lacking specific designs to utilize text\nsemantics on edges. While there exist edge-aware graph neural networks, they\ndirectly initialize edge attributes as a feature vector, which cannot fully\ncapture the contextualized text semantics of edges. In this paper, we propose\nEdgeformers, a framework built upon graph-enhanced Transformers, to perform\nedge and node representation learning by modeling texts on edges in a\ncontextualized way. Specifically, in edge representation learning, we inject\nnetwork information into each Transformer layer when encoding edge texts; in\nnode representation learning, we aggregate edge representations through an\nattention mechanism within each node's ego-graph. On five public datasets from\nthree different domains, Edgeformers consistently outperform state-of-the-art\nbaselines in edge classification and link prediction, demonstrating the\nefficacy in learning edge and node representations, respectively.",
        "Task": "They address two primary tasks on textual-edge networks: (1) edge-level classification by predicting categories (e.g., star-ratings) of edges, and (2) node-level link prediction by deciding whether an edge should exist between two nodes. Both tasks are evaluated on multiple real-world datasets (e.g., Amazon, Goodreads, StackOverflow).",
        "Data_type": "They focus on homogeneous graph-structured data where each edge contains free-text documents (e.g., user–item reviews). Node features are not used, and the framework solely models and encodes textual information on edges for downstream tasks.",
        "Data_domain": "They use three real-world datasets spanning e-commerce (Amazon), reader–book networks (Goodreads), and Q&A communities (StackOverflow). Specifically, Amazon reviews, Goodreads comments, and StackOverflow answers serve as the textual-edge data sources.",
        "ML_phase": "They present a multi-layer Transformer-based architecture (Edgeformer-E and Edgeformer-N) that injects node or structural information at each encoding layer to jointly learn edge and node representations. Training proceeds via supervised edge classification or unsupervised link prediction, with attention-based aggregation for integrating local network structure."
    },
    "Additional/2105.02605v3.json": {
        "title": "GraphFormers: GNN-nested Transformers for Representation Learning on  Textual Graph",
        "abs": "The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.",
        "Task": "They focus on link prediction in textual graphs, aiming to determine whether two nodes should be connected based on learned embeddings. The approach is evaluated on large-scale datasets (DBLP, Wiki, Product) by measuring link prediction accuracy.",
        "Data_type": "They handle textual node attributes on large-scale graphs, including both homogeneous and heterogeneous structures. The approach focuses solely on the text assigned to nodes, without incorporating edge attributes or other data modalities.",
        "Data_domain": "They use three large-scale textual graphs spanning an academic citation network (DBLP), a knowledge graph from Wikipedia (Wikidata5M), and an e-commerce product network. Thus, the data domain includes academic, knowledge-based, and commercial environments.",
        "ML_phase": "Data processing comprises tokenizing each node’s text, sampling neighbor nodes, and preparing them as inputs to a GNN-nested Transformers architecture for iterative feature encoding. Training proceeds via a two-stage progressive scheme, initially masking tokens to enforce neighborhood reliance and then fine-tuning on unmasked data for final adaptation."
    },
    "Additional/2205.10282v2.json": {
        "title": "Heterformer: Transformer-based Deep Node Representation Learning on  Heterogeneous Text-Rich Networks",
        "abs": "Representation learning on networks aims to derive a meaningful vector\nrepresentation for each node, thereby facilitating downstream tasks such as\nlink prediction, node classification, and node clustering. In heterogeneous\ntext-rich networks, this task is more challenging due to (1) presence or\nabsence of text: Some nodes are associated with rich textual information, while\nothers are not; (2) diversity of types: Nodes and edges of multiple types form\na heterogeneous network structure. As pretrained language models (PLMs) have\ndemonstrated their effectiveness in obtaining widely generalizable text\nrepresentations, a substantial amount of effort has been made to incorporate\nPLMs into representation learning on text-rich networks. However, few of them\ncan jointly consider heterogeneous structure (network) information as well as\nrich textual semantic information of each node effectively. In this paper, we\npropose Heterformer, a Heterogeneous Network-Empowered Transformer that\nperforms contextualized text encoding and heterogeneous structure encoding in a\nunified model. Specifically, we inject heterogeneous structure information into\neach Transformer layer when encoding node texts. Meanwhile, Heterformer is\ncapable of characterizing node/edge type heterogeneity and encoding nodes with\nor without texts. We conduct comprehensive experiments on three tasks (i.e.,\nlink prediction, node classification, and node clustering) on three large-scale\ndatasets from different domains, where Heterformer outperforms competitive\nbaselines significantly and consistently.",
        "Task": "They aim to learn node representations on heterogeneous text-rich networks for link prediction, node classification, and node clustering. Experiments are conducted on DBLP, Twitter, and Goodreads to evaluate performance on these tasks.",
        "Data_type": "They work with heterogeneous text-rich graph data containing multi-typed nodes and edges, where certain node types have associated textual documents while others do not. Across academic, social media, and product networks, each node can either hold semantically rich text or be textless, capturing both structural and textual attributes for downstream tasks.",
        "Data_domain": "They use real-world text-rich heterogeneous networks spanning three main domains: academic (DBLP), social media (Twitter), and e-commerce (Goodreads). These datasets contain multi-typed nodes (e.g., papers, tweets, books) and edges, with some nodes accompanied by rich textual information.",
        "ML_phase": "They introduce a Transformer-based architecture that fuses text (from a pretrained language model) and heterogeneous network structure via “virtual neighbor tokens,” alongside a warm-up stage for textless node embeddings. The system is trained end-to-end on an unsupervised link prediction objective with negative sampling, enabling node representation learning across both text-rich and textless nodes."
    },
    "Additional/2210.14709v2.json": {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "abs": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by training large language models and GNNs together. In this paper, we\npropose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows training the two modules separately while simultaneously\nallowing the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
        "Task": "They address node classification on text-attributed graphs. Experiments are conducted on large-scale benchmarks (ogbn-arxiv, ogbn-products, ogbn-papers100M) to assess classification performance.",
        "Data_type": "They handle text-attributed homogeneous graphs, where each node has textual attributes (e.g., titles, abstracts) and edges encode adjacency information. The text dimension is node-level, and the graph dimension captures structural connections among nodes.",
        "Data_domain": "These datasets are drawn from large-scale text-attributed graphs spanning academic citation networks (ogbn-arxiv, ogbn-papers100M) and an e-commerce co-purchase network (ogbn-products). The textual content includes paper titles and abstracts in the academic domain, as well as product descriptions in the e-commerce domain.",
        "ML_phase": "They propose an iterative EM-style training pipeline where a language model (LM) and a graph neural network (GNN) are alternately optimized. The LM is updated using gold plus GNN-generated pseudo-labels, and the GNN is trained on LM-generated embeddings and pseudo-labels, enabling large-scale node representation learning in text-attributed graphs."
    },
    "Additional/2305.12268v1.json": {
        "title": "Patton: Language Model Pretraining on Text-Rich Networks",
        "abs": "A real-world text corpus sometimes comprises not only text documents but also\nsemantic links between them (e.g., academic papers in a bibliographic network\nare linked by citations and co-authorships). Text documents and semantic\nconnections form a text-rich network, which empowers a wide range of downstream\ntasks such as classification and retrieval. However, pretraining methods for\nsuch structures are still lacking, making it difficult to build one generic\nmodel that can be adapted to various tasks on text-rich networks. Current\npretraining objectives, such as masked language modeling, purely model texts\nand do not take inter-document structure information into consideration. To\nthis end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.\nPatton includes two pretraining strategies: network-contextualized masked\nlanguage modeling and masked node prediction, to capture the inherent\ndependency between textual attributes and network structure. We conduct\nexperiments on four downstream tasks in five datasets from both academic and\ne-commerce domains, where Patton outperforms baselines significantly and\nconsistently.",
        "Task": "They tackle node-level classification, retrieval, reranking, and link prediction tasks on text-rich networks. They evaluate performance on multiple academic (MAG) and e-commerce (Amazon) datasets, each providing labels and links to support those four tasks.",
        "Data_type": "They operate on text-rich networks represented as homogeneous graphs, where each node is associated with textual attributes and edges encode semantic relationships between documents. Their approach leverages these node-level textual embeddings alongside the graph's edge structures to learn context-aware representations.",
        "Data_domain": "The paper’s data comes from two domains: academic networks (Microsoft Academic Graph) and e-commerce networks (Amazon).",
        "ML_phase": "They utilize a GNN-nested Transformer (GraphFormers) architecture and jointly train it with two pretraining objectives (network-contextualized MLM and masked node prediction) on text-rich networks. The model is then fine-tuned for classification, retrieval, reranking, and link prediction within the same framework, where each downstream task uses the final CLS representation, optionally combined with neighbor information if available."
    },
    "TimeSeries/2202.01575v3.json": {
        "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend  Representations for Time Series Forecasting",
        "abs": "Deep learning has been actively studied for time series forecasting, and the\nmainstream paradigm is based on the end-to-end training of neural network\narchitectures, ranging from classical LSTM/RNNs to more recent TCNs and\nTransformers. Motivated by the recent success of representation learning in\ncomputer vision and natural language processing, we argue that a more promising\nparadigm for time series forecasting, is to first learn disentangled feature\nrepresentations, followed by a simple regression fine-tuning step -- we justify\nsuch a paradigm from a causal perspective. Following this principle, we propose\na new time series representation learning framework for time series forecasting\nnamed CoST, which applies contrastive learning methods to learn disentangled\nseasonal-trend representations. CoST comprises both time domain and frequency\ndomain contrastive losses to learn discriminative trend and seasonal\nrepresentations, respectively. Extensive experiments on real-world datasets\nshow that CoST consistently outperforms the state-of-the-art methods by a\nconsiderable margin, achieving a 21.3% improvement in MSE on multivariate\nbenchmarks. It is also robust to various choices of backbone encoders, as well\nas downstream regressors. Code is available at\nhttps://github.com/salesforce/CoST.",
        "Task": "This paper tackles the time-series forecasting task (a regression problem), where the goal is to predict future values of temporal data. They evaluate performance on multiple real-world public benchmark datasets (ETT, Electricity, Weather, M5) using metrics such as MSE and MAE in both univariate and multivariate settings.",
        "Data_type": "They handle univariate and multivariate numeric time series data, dimensioned T×m, where T is the time length and m is the number of features. No graph-structured, multi-modal, or textual data is involved in their experiments.",
        "Data_domain": "These experiments use time series data drawn from multiple real-world domains, primarily focusing on electrical (e.g., transformer temperature and consumption) and meteorological (e.g., weather) measurements, with additional retail (M5) data. This scope emphasizes diverse industrial and climate-related forecasting tasks.",
        "ML_phase": "They perform self-supervised representation learning separate from the forecasting regressor, using data augmentations that simulate interventions on the noise variable. Time- and frequency-domain contrastive losses disentangle seasonal and trend features, which are then passed to a downstream forecaster."
    },
    "TimeSeries/2402.10635v1.json": {
        "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series  Modeling",
        "abs": "Modeling continuous-time dynamics on irregular time series is critical to\naccount for data evolution and correlations that occur continuously.\nTraditional methods including recurrent neural networks or Transformer models\nleverage inductive bias via powerful neural architectures to capture complex\npatterns. However, due to their discrete characteristic, they have limitations\nin generalizing to continuous-time data paradigms. Though neural ordinary\ndifferential equations (Neural ODEs) and their variants have shown promising\nresults in dealing with irregular time series, they often fail to capture the\nintricate correlations within these sequences. It is challenging yet demanding\nto concurrently model the relationship between input data points and capture\nthe dynamic changes of the continuous-time system. To tackle this problem, we\npropose ContiFormer that extends the relation modeling of vanilla Transformer\nto the continuous-time domain, which explicitly incorporates the modeling\nabilities of continuous dynamics of Neural ODEs with the attention mechanism of\nTransformers. We mathematically characterize the expressive power of\nContiFormer and illustrate that, by curated designs of function hypothesis,\nmany Transformer variants specialized in irregular time series modeling can be\ncovered as a special case of ContiFormer. A wide range of experiments on both\nsynthetic and real-world datasets have illustrated the superior modeling\ncapacities and prediction performance of ContiFormer on irregular time series\ndata. The project link is https://seqml.github.io/contiformer/.",
        "Task": "They tackle multiple irregular time series problems, including time-series interpolation and forecasting (regression), time-series classification, and event prediction. Corresponding datasets cover tasks such as UEA archive classification, MTPP-based event prediction, and standard forecasting benchmarks for comprehensive evaluation.",
        "Data_type": "They address continuous and irregular time series data of varying dimensions (e.g., univariate, multivariate) with non-uniformly sampled or missing observations. Additionally, they demonstrate applicability to sequences of 2D images (pendulum frames) and discrete event-type sequences (e.g., transaction logs), all treated as time-evolving data.",
        "Data_domain": "These datasets span multiple real-world domains, including medical (e.g., MIMIC, Neonate), financial transactions (e.g., BookOrder), sensor-based monitoring (e.g., Traffic), community question answering (e.g., StackOverflow), and various academic benchmarks (e.g., UEA Archive). In particular, they encompass diverse settings such as clinical, economic, and user interaction data for irregular time series analysis.",
        "ML_phase": "They propose a continuous-time Transformer architecture that integrates ODE-based attention, enabling the model to handle irregular time intervals. The training framework reparameterizes the ODE solver for parallel computation within a standard Transformer pipeline, preserving efficiency in backpropagation."
    },
    "TimeSeries/2310.06625v4.json": {
        "title": "iTransformer: Inverted Transformers Are Effective for Time Series  Forecasting",
        "abs": "The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformers are challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the embedding for each temporal token fuses multiple\nvariates that represent potential delayed events and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any modification to the basic components. We propose iTransformer that\nsimply applies the attention and feed-forward network on the inverted\ndimensions. Specifically, the time points of individual series are embedded\ninto variate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves state-of-the-art on challenging real-world datasets, which further\nempowers the Transformer family with promoted performance, generalization\nability across different variates, and better utilization of arbitrary lookback\nwindows, making it a nice alternative as the fundamental backbone of time\nseries forecasting. Code is available at this repository:\nhttps://github.com/thuml/iTransformer.",
        "Task": "They tackle regression-based multivariate time series forecasting, evaluating predictive accuracy on extensive real-world benchmarks (ECL, ETT, Exchange, Traffic, Weather, Solar-Energy, PEMS, Market, etc.). The primary goal is to forecast future values given historical observations.",
        "Data_type": "They handle multivariate numeric time-series data, where each time step includes multiple continuous variables (e.g., electricity consumption, weather indicators). No explicit extension to graph structures, text, or multimodal data (like images or SMILES) is discussed.",
        "Data_domain": "The paper’s datasets span multiple real-world domains, including electricity consumption records, currency exchange rates, meteorological indicators, traffic sensor measurements, solar power production, and online transaction server loads. Collectively, these represent economic, energy, weather, transportation, and enterprise application data sources.",
        "ML_phase": "They reorganize input channels at the embedding stage and route them through an inverted Transformer backbone applying standard encoder blocks. They also adopt a partial-channel sampling strategy to reduce memory usage, train with self-attention across channels, and use a shared feed-forward network on each channel’s features."
    },
    "TimeSeries/2106.06947v1.json": {
        "title": "Graph Neural Network-Based Anomaly Detection in Multivariate Time Series",
        "abs": "Given high-dimensional time series data (e.g., sensor data), how can we\ndetect anomalous events, such as system faults and attacks? More challengingly,\nhow can we do this in a way that captures complex inter-sensor relationships,\nand detects and explains anomalies which deviate from these relationships?\nRecently, deep learning approaches have enabled improvements in anomaly\ndetection in high-dimensional datasets; however, existing methods do not\nexplicitly learn the structure of existing relationships between variables, or\nuse them to predict the expected behavior of time series. Our approach combines\na structure learning approach with graph neural networks, additionally using\nattention weights to provide explainability for the detected anomalies.\nExperiments on two real-world sensor datasets with ground truth anomalies show\nthat our method detects anomalies more accurately than baseline approaches,\naccurately captures correlations between sensors, and allows users to deduce\nthe root cause of a detected anomaly.",
        "Task": "They present an unsupervised anomaly detection approach, which can be framed as a classification task (normal vs. anomalous) on multivariate time series data. Multiple real-world sensor datasets are used to evaluate performance.",
        "Data_type": "It processes graph-structured data with a focus on node attributes in homogeneous graphs. There is no indication of handling multi-modal data such as images or SMILES/fingerprints.",
        "Data_domain": "This work’s datasets span real-world multivariate time series collected from industrial and sensor-based sources, such as server machines, satellite systems, and water treatment testbeds. They primarily consist of operational signals representing various machine or process conditions in cyber-physical environments.",
        "ML_phase": "They construct graph-based features from the original data to capture correlations, then feed these representations into a specialized network for representation learning. A subsequent deviation-based training framework monitors anomalies by comparing learned embeddings against expected normal patterns."
    },
    "TimeSeries/2203.03991v1.json": {
        "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate  Time-series",
        "abs": "We propose an end-to-end architecture for multivariate time-series prediction\nthat integrates a spatial-temporal graph neural network with a matrix filtering\nmodule. This module generates filtered (inverse) correlation graphs from\nmultivariate time series before inputting them into a GNN. In contrast with\nexisting sparsification methods adopted in graph neural network, our model\nexplicitly leverage time-series filtering to overcome the low signal-to-noise\nratio typical of complex systems data. We present a set of experiments, where\nwe predict future sales from a synthetic time-series sales dataset. The\nproposed spatial-temporal graph neural network displays superior performances\nwith respect to baseline approaches, with no graphical information, and with\nfully connected, disconnected graphs and unfiltered graphs.",
        "Task": "They address multivariate time-series forecasting (a regression task) to predict daily sales across multiple products and stores. The dataset comes from a Kaggle competition containing 5-year sales records for 50 products in 10 stores.",
        "Data_type": "They utilize multivariate time-series data composed of daily sales for 50 products across 10 stores, where each store’s series serves as a node. Edges are determined by (inverse) correlation values between these time-series nodes, forming a graph-structured dataset.",
        "Data_domain": "The data is from the retail domain, specifically a Kaggle store item demand forecasting competition. It comprises five-year daily sales time-series for 50 products across 10 different stores.",
        "ML_phase": "They construct correlation or inverse-correlation graphs from the multivariate time-series, filter them with methods such as shrinkage, Graphical LASSO, or MFCF, generate node features (e.g., statistical moments), and then feed these together with LSTM-derived temporal embeddings into GCN or GAT, followed by an MLP read-out for final predictions. Training uses an 80–20 split, repeated with multiple seeds, and evaluation relies on standard error metrics (RMSE, MAE, MAPE)."
    },
    "TimeSeries/2106.09305v3.json": {
        "title": "SCINet: Time Series Modeling and Forecasting with Sample Convolution and  Interaction",
        "abs": "One unique property of time series is that the temporal relations are largely\npreserved after downsampling into two sub-sequences. By taking advantage of\nthis property, we propose a novel neural network architecture that conducts\nsample convolution and interaction for temporal modeling and forecasting, named\nSCINet. Specifically, SCINet is a recursive downsample-convolve-interact\narchitecture. In each layer, we use multiple convolutional filters to extract\ndistinct yet valuable temporal features from the downsampled sub-sequences or\nfeatures. By combining these rich features aggregated from multiple\nresolutions, SCINet effectively models time series with complex temporal\ndynamics. Experimental results show that SCINet achieves significant\nforecasting accuracy improvements over both existing convolutional models and\nTransformer-based solutions across various real-world time series forecasting\ndatasets. Our codes and data are available at\nhttps://github.com/cure-lab/SCINet.",
        "Task": "They address multi-step time series forecasting, treating it as a supervised regression task. The authors evaluate performance on diverse real-world benchmarks (e.g., ETTh(1,2), ETTm1, Traffic, Solar-Energy, Electricity, Exchange-Rate, and PeMS) to demonstrate forecasting accuracy.",
        "Data_type": "They focus on multivariate, numerical time-series data recorded at regular intervals. Each sample is a sequence of real-valued observations providing temporal context for forecasting tasks.",
        "Data_domain": "Data are multivariate time series drawn from diverse real-world domains: electric power systems (transformer temperature), solar energy production, highway traffic sensors, electricity consumption, and foreign exchange rates. These datasets capture both regular and complex spatio-temporal patterns in sectors such as energy, transportation, and finance.",
        "ML_phase": "SCINet first normalizes and partitions the input time series, then processes it with a hierarchical downsample–convolve–interact architecture to learn multi-resolution features. The model is trained end-to-end using an L1-based loss, optionally stacked for deeper representation, and uses a final fully-connected decoder for forecasting."
    },
    "TimeSeries/2405.14616v1.json": {
        "title": "TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting",
        "abs": "Time series forecasting is widely used in extensive applications, such as\ntraffic planning and weather forecasting. However, real-world time series\nusually present intricate temporal variations, making forecasting extremely\nchallenging. Going beyond the mainstream paradigms of plain decomposition and\nmultiperiodicity analysis, we analyze temporal variations in a novel view of\nmultiscale-mixing, which is based on an intuitive but important observation\nthat time series present distinct patterns in different sampling scales. The\nmicroscopic and the macroscopic information are reflected in fine and coarse\nscales respectively, and thereby complex variations can be inherently\ndisentangled. Based on this observation, we propose TimeMixer as a fully\nMLP-based architecture with Past-Decomposable-Mixing (PDM) and\nFuture-Multipredictor-Mixing (FMM) blocks to take full advantage of\ndisentangled multiscale series in both past extraction and future prediction\nphases. Concretely, PDM applies the decomposition to multiscale series and\nfurther mixes the decomposed seasonal and trend components in fine-to-coarse\nand coarse-to-fine directions separately, which successively aggregates the\nmicroscopic seasonal and macroscopic trend information. FMM further ensembles\nmultiple predictors to utilize complementary forecasting capabilities in\nmultiscale observations. Consequently, TimeMixer is able to achieve consistent\nstate-of-the-art performances in both long-term and short-term forecasting\ntasks with favorable run-time efficiency.",
        "Task": "They address univariate and multivariate time series forecasting, benchmarking on diverse real-world datasets (e.g., ETT, Weather, M4, PeMS). Both long-term and short-term forecasts are measured with standard metrics to evaluate prediction accuracy.",
        "Data_type": "They handle and forecast numeric time series data (both univariate and multivariate) over temporal sequences. No graph-structured, text, or multi-modal inputs are addressed.",
        "Data_domain": "The datasets span diverse real-world domains, encompassing meteorology (weather), energy consumption (electricity, solar), transportation (traffic, PEMS), and finance or economic contexts (M4). They represent multi-frequency time series capturing macro-level, industrial, and demographic signals.",
        "ML_phase": "TimeMixer operates in the model design and training phase by first downsampling multiscale series, then applying decomposition-based blocks (for separate seasonal/trend mixing) and finally aggregating multiscale predictors for forecasting. The model is trained end-to-end with MSE loss in PyTorch, using Adam optimization."
    },
    "TimeSeries/2311.06190v1.json": {
        "title": "FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure  Graph Perspective",
        "abs": "Multivariate time series (MTS) forecasting has shown great importance in\nnumerous industries. Current state-of-the-art graph neural network (GNN)-based\nforecasting methods usually require both graph networks (e.g., GCN) and\ntemporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and\nintra-series (temporal) dependencies, respectively. However, the uncertain\ncompatibility of the two networks puts an extra burden on handcrafted model\ndesigns. Moreover, the separate spatial and temporal modeling naturally\nviolates the unified spatiotemporal inter-dependencies in real world, which\nlargely hinders the forecasting performance. To overcome these problems, we\nexplore an interesting direction of directly applying graph networks and\nrethink MTS forecasting from a pure graph perspective. We first define a novel\ndata structure, hypervariate graph, which regards each series value (regardless\nof variates or timestamps) as a graph node, and represents sliding windows as\nspace-time fully-connected graphs. This perspective considers spatiotemporal\ndynamics unitedly and reformulates classic MTS forecasting into the predictions\non hypervariate graphs. Then, we propose a novel architecture Fourier Graph\nNeural Network (FourierGNN) by stacking our proposed Fourier Graph Operator\n(FGO) to perform matrix multiplications in Fourier space. FourierGNN\naccommodates adequate expressiveness and achieves much lower complexity, which\ncan effectively and efficiently accomplish the forecasting. Besides, our\ntheoretical analysis reveals FGO's equivalence to graph convolutions in the\ntime domain, which further verifies the validity of FourierGNN. Extensive\nexperiments on seven datasets have demonstrated our superior performance with\nhigher efficiency and fewer parameters compared with state-of-the-art methods.",
        "Task": "They address a multivariate time series forecasting task, formulated as a regression problem for predicting future values of multiple time series. They evaluate performance on seven real-world datasets using standard forecasting metrics (e.g., MAE, RMSE, MAPE).",
        "Data_type": "They handle multivariate time series by constructing a hypervariate graph of size N×T, where each element (a value of one variate at one timestamp) becomes a node. All nodes are fully connected, yielding an adjacency matrix of dimension (N×T)×(N×T).",
        "Data_domain": "They evaluate their approach on seven real-world multivariate time series datasets spanning transportation (e.g., traffic flow), energy (e.g., electricity usage), web traffic (Wiki), electrocardiogram signals, and COVID-19 hospitalization data. These datasets capture diverse spatiotemporal patterns from domains such as public health, infrastructure, and online platforms.",
        "ML_phase": "They convert each multivariate time-series window into a fully connected “hypervariate graph,” embed the nodes, and then apply a stacked Fourier Graph Neural Network (FourierGNN) that operates in the frequency domain. Training uses RMSProp with mean squared error loss, where final feed-forward layers project learned spatiotemporal representations onto multi-step forecasts."
    },
    "TimeSeries/2106.10466v4.json": {
        "title": "TS2Vec: Towards Universal Representation of Time Series",
        "abs": "This paper presents TS2Vec, a universal framework for learning\nrepresentations of time series in an arbitrary semantic level. Unlike existing\nmethods, TS2Vec performs contrastive learning in a hierarchical way over\naugmented context views, which enables a robust contextual representation for\neach timestamp. Furthermore, to obtain the representation of an arbitrary\nsub-sequence in the time series, we can apply a simple aggregation over the\nrepresentations of corresponding timestamps. We conduct extensive experiments\non time series classification tasks to evaluate the quality of time series\nrepresentations. As a result, TS2Vec achieves significant improvement over\nexisting SOTAs of unsupervised time series representation on 125 UCR datasets\nand 29 UEA datasets. The learned timestamp-level representations also achieve\nsuperior results in time series forecasting and anomaly detection tasks. A\nlinear regression trained on top of the learned representations outperforms\nprevious SOTAs of time series forecasting. Furthermore, we present a simple way\nto apply the learned representations for unsupervised anomaly detection, which\nestablishes SOTA results in the literature. The source code is publicly\navailable at https://github.com/yuezhihan/ts2vec.",
        "Task": "They address three time-series tasks: classification (using UCR and UEA archives), forecasting (ETT and Electricity datasets), and anomaly detection (Yahoo and KPI datasets). They evaluate their learned representations on each task to demonstrate effectiveness.",
        "Data_type": "They handle univariate and multivariate time series data, accommodating varied lengths and missing observations. The method is applicable across tasks requiring universal time series representations (e.g., classification, forecasting, anomaly detection).",
        "Data_domain": "Time‐series datasets originate from diverse real‐world sources, including electricity consumption records, industrial sensor measurements, and Internet KPI metrics (e.g., Yahoo and KPI benchmarks). They also encompass broad collections from standard archives (UCR/UEA), reflecting a wide spectrum of application domains.",
        "ML_phase": "They employ a dilated CNN encoder with an input projection layer, alongside random timestamp masking and random cropping in the data preprocessing steps to form augmented views. Training proceeds via hierarchical contrastive learning (at both temporal and instance levels) across multiple pooled scales, yielding universal sub-series representations."
    },
    "TimeSeries/2201.12886v6.json": {
        "title": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
        "abs": "Recent progress in neural forecasting accelerated improvements in the\nperformance of large-scale forecasting systems. Yet, long-horizon forecasting\nremains a very difficult task. Two common challenges afflicting the task are\nthe volatility of the predictions and their computational complexity. We\nintroduce N-HiTS, a model which addresses both challenges by incorporating\nnovel hierarchical interpolation and multi-rate data sampling techniques. These\ntechniques enable the proposed method to assemble its predictions sequentially,\nemphasizing components with different frequencies and scales while decomposing\nthe input signal and synthesizing the forecast. We prove that the hierarchical\ninterpolation technique can efficiently approximate arbitrarily long horizons\nin the presence of smoothness. Additionally, we conduct extensive large-scale\ndataset experiments from the long-horizon forecasting literature, demonstrating\nthe advantages of our method over the state-of-the-art methods, where N-HiTS\nprovides an average accuracy improvement of almost 20% over the latest\nTransformer architectures while reducing the computation time by an order of\nmagnitude (50 times). Our code is available at bit.ly/3VA5DoT",
        "Task": "They address the long-horizon time-series forecasting task. They evaluate performance on multiple real-world datasets (e.g., electricity consumption, traffic, weather, exchange rate, and influenza-like illness).",
        "Data_type": "They handle numeric time-series data in univariate and multivariate settings, where each sequence has length T (time dimension) and F (variable dimension). No graph-structured, image-text, or SMILES/fingerprint data are addressed.",
        "Data_domain": "Data is drawn from multiple real-world domains, including energy consumption (electricity), finance (exchange rates), public transportation (highway traffic), meteorological measurements (weather), and healthcare (influenza-like illness). These time-series datasets originate from diverse sources such as power grid sensors, economic indicators, transit systems, weather stations, and public health records.",
        "ML_phase": "They normalize time-series data, then apply multi-rate sampling and hierarchical interpolation within a multi-block MLP-based architecture that produces both backcast and forecast outputs. Next, they train the network end-to-end (using Adam and Bayesian hyperparameter optimization) to generate multi-step forecasts directly, controlling model complexity through pooling kernels and expressiveness ratios."
    },
    "Pretrain/2006.15437.json": {
        "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
        "abs": "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
        "Task": "They tackle multiple node-level classification tasks (e.g., predicting paper fields, paper venues, and review ratings) and a link-level task (author name disambiguation) across large-scale graphs. Their experiments primarily involve heterogeneous data from OAG and Amazon, where they evaluate performance on these classification and link prediction benchmarks.",
        "Data_type": "The paper deals with large-scale homogeneous and heterogeneous graph-structured data, where nodes hold textual attributes (e.g., paper titles, product reviews) and edges capture relational information (e.g., author-paper, user-review links). It focuses on learning from these textual node features and graph connectivity for downstream tasks in academic networks and recommendation systems.",
        "Data_domain": "They use academic data from the Open Academic Graph, covering publications, authors, fields, venues, and institutes, as well as e-commerce data from Amazon reviews and social posts from Reddit. These diverse sources reflect real-world domains such as academic networks, online product reviews, and social communities.",
        "ML_phase": "They introduce a generative pre-training stage within the GNN training pipeline, where node attributes and edges are masked and then reconstructed in one forward pass. The framework separates nodes to prevent attribute leakage, employs an adaptive queue for negative sampling, and finally fine-tunes the pre-trained model on downstream tasks."
    },
    "Pretrain/2107.03936v2.json": {
        "title": "Graph Neural Pre-training for Enhancing Recommendations using Side  Information",
        "abs": "Leveraging the side information associated with entities (i.e. users and\nitems) to enhance the performance of recommendation systems has been widely\nrecognized as an important modelling dimension. While many existing approaches\nfocus on the integration scheme to incorporate entity side information -- by\ncombining the recommendation loss function with an extra side information-aware\nloss -- in this paper, we propose instead a novel pre-training scheme for\nleveraging the side information. In particular, we first pre-train a\nrepresentation model using the side information of the entities, and then\nfine-tune it using an existing general representation-based recommendation\nmodel. Specifically, we propose two pre-training models, named GCN-P and COM-P,\nby considering the entities and their relations constructed from side\ninformation as two different types of graphs respectively, to pre-train entity\nembeddings. For the GCN-P model, two single-relational graphs are constructed\nfrom all the users' and items' side information respectively, to pre-train\nentity representations by using the Graph Convolutional Networks. For the COM-P\nmodel, two multi-relational graphs are constructed to pre-train the entity\nrepresentations by using the Composition-based Graph Convolutional Networks. An\nextensive evaluation of our pre-training models fine-tuned under four general\nrepresentation-based recommender models, i.e. MF, NCF, NGCF and LightGCN, shows\nthat effectively pre-training embeddings with both the user's and item's side\ninformation can significantly improve these original models in terms of both\neffectiveness and stability.",
        "Task": "They address a recommendation task focusing on top-k item ranking to predict user-item preferences. The performance is evaluated on Foursquare, Movielens-1M, and Epinions datasets.",
        "Data_type": "They leverage textual side information (e.g., user bag-of-words features, item attributes) to construct single- or multi-relational graphs, where nodes represent entities (users/items) and edges encode similarity or shared feature categories. These graphs contain node-level textual features and serve as the input data for recommendation pre-training.",
        "Data_domain": "The paper’s datasets originate from a location-based social network (Foursquare), a movie-rating platform (Movielens), and a consumer review website (Epinions). These domains encompass social media check-ins, user-item ratings, and user-generated product reviews.",
        "ML_phase": "They first construct single-relational or multi-relational graphs from user/item features and learn initial embeddings via GNN-based architectures, then fine-tune these embeddings using standard recommenders on interaction data. Hence, the pipeline includes data preparation (feature extraction and graph construction), a pre-training phase with graph neural models, and a final fine-tuning stage within existing recommendation frameworks."
    },
    "Pretrain/2312.16855v1.json": {
        "title": "Molecular Property Prediction Based on Graph Structure Learning",
        "abs": "Molecular property prediction (MPP) is a fundamental but challenging task in\nthe computer-aided drug discovery process. More and more recent works employ\ndifferent graph-based models for MPP, which have made considerable progress in\nimproving prediction performance. However, current models often ignore\nrelationships between molecules, which could be also helpful for MPP. For this\nsake, in this paper we propose a graph structure learning (GSL) based MPP\napproach, called GSL-MPP. Specifically, we first apply graph neural network\n(GNN) over molecular graphs to extract molecular representations. Then, with\nmolecular fingerprints, we construct a molecular similarity graph (MSG).\nFollowing that, we conduct graph structure learning on the MSG (i.e.,\nmolecule-level graph structure learning) to get the final molecular embeddings,\nwhich are the results of fusing both GNN encoded molecular representations and\nthe relationships among molecules, i.e., combining both intra-molecule and\ninter-molecule information. Finally, we use these molecular embeddings to\nperform MPP. Extensive experiments on seven various benchmark datasets show\nthat our method could achieve state-of-the-art performance in most cases,\nespecially on classification tasks. Further visualization studies also\ndemonstrate the good molecular representations of our method.",
        "Task": "They address molecular property prediction on benchmark datasets, tackling both classification (e.g., BACE, BBBP, SIDER, ClinTox) and regression (e.g., ESOL, Lipophilicity, FreeSolv) tasks. Specifically, each molecule is treated as a graph-level instance, and performance is measured in terms of classification AUC-ROC and regression RMSE.",
        "Data_type": "They handle molecular graph-structured data in which atoms serve as nodes and bonds as edges, and they also employ Extended Connectivity Fingerprints (ECFP) for measuring inter-molecule similarity. Fingerprint-based adjacency further refines relationships among molecules for property prediction.",
        "Data_domain": "These datasets come from the chemical/drug discovery domain, encompassing multiple real-world molecular benchmarks (e.g., BACE, BBBP, SIDER) provided by MoleculeNet. They involve molecules represented as graphs with nodes (atoms) and edges (bonds), reflecting properties pertinent to pharmacological tasks.",
        "ML_phase": "They first construct atom-level molecular graphs and use a GNN (GIN) to obtain initial embeddings, then form an inter-molecule similarity graph based on ECFP-based fingerprint scores. This similarity graph and the embeddings are iteratively refined via metric-based GSL and subsequently passed to a final prediction layer, supervised by both a task-specific loss and a GSL-specific loss."
    },
    "Pretrain/2311.15317v5.json": {
        "title": "Generalized Graph Prompt: Toward a Unification of Pre-Training and  Downstream Tasks on Graphs",
        "abs": "Graph neural networks have emerged as a powerful tool for graph\nrepresentation learning, but their performance heavily relies on abundant\ntask-specific supervision. To reduce labeling requirement, the \"pre-train,\nprompt\" paradigms have become increasingly common. However, existing study of\nprompting on graphs is limited, lacking a universal treatment to appeal to\ndifferent downstream tasks. In this paper, we propose GraphPrompt, a novel\npre-training and prompting framework on graphs. GraphPrompt not only unifies\npre-training and downstream tasks into a common task template but also employs\na learnable prompt to assist a downstream task in locating the most relevant\nknowledge from the pre-trained model in a task-specific manner. To further\nenhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with\ntwo major enhancements. First, we generalize several popular graph pre-training\ntasks beyond simple link prediction to broaden the compatibility with our task\ntemplate. Second, we propose a more generalized prompt design that incorporates\na series of prompt vectors within every layer of the pre-trained graph encoder,\nin order to capitalize on the hierarchical information across different layers\nbeyond just the readout layer. Finally, we conduct extensive experiments on\nfive public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.",
        "Task": "They tackle node classification and graph classification on five benchmark datasets (Flickr, PROTEINS, ENZYMES, COX2, and BZR) in few-shot scenarios. Additionally, they employ link prediction on unlabeled graphs as a self-supervised pre-training task.",
        "Data_type": "They focus on homogeneous graph-structured data, where each node can have numeric or textual attributes (e.g., social networks, molecular graphs). The approach supports both node- and graph-level tasks, including small-molecule datasets (COX2, BZR) and protein graphs (PROTEINS, ENZYMES).",
        "Data_domain": "The paper utilizes both social network data (Flickr) and biochemical/molecular datasets (PROTEINS, COX2, ENZYMES, BZR). Consequently, the data domain spans large-scale social networks and smaller biological graphs.",
        "ML_phase": "They propose a two-phase pipeline where a GNN is first pre-trained on unlabeled graphs using a self-supervised contrastive or link prediction task to learn transferable node embeddings. In the downstream phase, these embeddings are combined with learnable prompts during the readout step to unify node- or graph-level classification under a subgraph similarity framework."
    },
    "Pretrain/2205.12454v4.json": {
        "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
        "abs": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph\nTransformer with linear complexity and state-of-the-art results on a diverse\nset of benchmarks. Graph Transformers (GTs) have gained popularity in the field\nof graph representation learning with a variety of recent publications but they\nlack a common foundation about what constitutes a good positional or structural\nencoding, and what differentiates them. In this paper, we summarize the\ndifferent types of encodings with a clearer definition and categorize them as\nbeing $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. The prior GTs\nare constrained to small graphs with a few hundred nodes, here we propose the\nfirst architecture with a complexity linear in the number of nodes and edges\n$O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected\nTransformer. We argue that this decoupling does not negatively affect the\nexpressivity, with our architecture being a universal function approximator on\ngraphs. Our GPS recipe consists of choosing 3 main ingredients: (i)\npositional/structural encoding, (ii) local message-passing mechanism, and (iii)\nglobal attention mechanism. We provide a modular framework $\\textit{GraphGPS}$\nthat supports multiple types of encodings and that provides efficiency and\nscalability both in small and large graphs. We test our architecture on 16\nbenchmarks and show highly competitive results in all of them, show-casing the\nempirical benefits gained by the modularity and the combination of different\nstrategies.",
        "Task": "They address a broad set of supervised tasks at node-, link-, and graph-level (classification and regression). They evaluate on diverse benchmarks (e.g., ZINC, MNIST, CIFAR10, PATTERN, CLUSTER, OGB, LRGB, MalNet-Tiny) to measure performance across multiple domains.",
        "Data_type": "They operate on generic graph-structured data with both node- and edge-level features, including molecular graphs (atom/bond information), superpixel image graphs, protein-protein association networks, and ASTs in source code. These features can be real-valued or structural (e.g., distances, random-walk encodings), enabling broad applicability across molecular, textual, or visual domains of graph data.",
        "Data_domain": "They evaluate models on a diverse set of graph datasets from molecular chemistry, protein-protein networks, superpixel images (MNIST/CIFAR10), and function-call graphs (MalNet-Tiny), as well as synthetic graph benchmarks. Additional tasks include source-code abstract syntax trees (ogbg-code2) and large-scale molecular data (OGB-LSC PCQM4Mv2).",
        "ML_phase": "They introduce a unified framework where, prior to training, node and edge features are augmented with local/global positional and structural encodings, then processed by a layer combining MPNN-based local aggregation and Transformer-like global attention. The training pipeline uses standard optimizers (e.g., AdamW) with warm-up and cosine learning-rate scheduling, and the architecture is designed to flexibly incorporate new encoding or attention modules."
    },
    "Pretrain/2110.13567v2.json": {
        "title": "Pairwise Half-graph Discrimination: A Simple Graph-level Self-supervised  Strategy for Pre-training Graph Neural Networks",
        "abs": "Self-supervised learning has gradually emerged as a powerful technique for\ngraph representation learning. However, transferable, generalizable, and robust\nrepresentation learning on graph data still remains a challenge for\npre-training graph neural networks. In this paper, we propose a simple and\neffective self-supervised pre-training strategy, named Pairwise Half-graph\nDiscrimination (PHD), that explicitly pre-trains a graph neural network at\ngraph-level. PHD is designed as a simple binary classification task to\ndiscriminate whether two half-graphs come from the same source. Experiments\ndemonstrate that the PHD is an effective pre-training strategy that offers\ncomparable or superior performance on 13 graph classification tasks compared\nwith state-of-the-art strategies, and achieves notable improvements when\ncombined with node-level strategies. Moreover, the visualization of learned\nrepresentation revealed that PHD strategy indeed empowers the model to learn\ngraph-level knowledge like the molecular scaffold. These results have\nestablished PHD as a powerful and effective self-supervised learning strategy\nin graph-level representation learning.",
        "Task": "They address graph-level classification tasks on multiple molecular property prediction datasets (e.g., MUV, HIV, BACE) and standard graph benchmarks (e.g., MUTAG, PTC-MR) by pre-training a GNN with a self-supervised scheme. The learned model is then fine-tuned on these datasets to evaluate classification performance.",
        "Data_type": "They handle graph-structured data, focusing on molecular graphs with node and bond (edge) features extracted (e.g., via RDKit) and also small-scale social network datasets. The data includes attributed nodes and edges (e.g., chemical properties) suitable for graph-level tasks.",
        "Data_domain": "The paper primarily uses molecular graph data from chemistry (e.g., ZINC15) and also includes social network benchmark datasets (e.g., IMDB, REDDIT). These datasets span bioinformatics and social domains for evaluating graph-level representation learning.",
        "ML_phase": "They first decompose each graph into two half-graphs to create positive/negative pairs and introduce a virtual “collection node” to aggregate global features for the self-supervised PHD objective. Subsequently, the pre-trained GNN is fine-tuned on downstream tasks, completing the transfer learning pipeline."
    },
    "Pretrain/2205.10803v3.json": {
        "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
        "abs": "Self-supervised learning (SSL) has been extensively explored in recent years.\nParticularly, generative SSL has seen emerging success in natural language\nprocessing and other AI fields, such as the wide adoption of BERT and GPT.\nDespite this, contrastive learning-which heavily relies on structural data\naugmentation and complicated training strategies-has been the dominant approach\nin graph SSL, while the progress of generative SSL on graphs, especially graph\nautoencoders (GAEs), has thus far not reached the potential as promised in\nother fields. In this paper, we identify and examine the issues that negatively\nimpact the development of GAEs, including their reconstruction objective,\ntraining robustness, and error metric. We present a masked graph autoencoder\nGraphMAE that mitigates these issues for generative self-supervised graph\npretraining. Instead of reconstructing graph structures, we propose to focus on\nfeature reconstruction with both a masking strategy and scaled cosine error\nthat benefit the robust training of GraphMAE. We conduct extensive experiments\non 21 public datasets for three different graph learning tasks. The results\nmanifest that GraphMAE-a simple graph autoencoder with careful designs-can\nconsistently generate outperformance over both contrastive and generative\nstate-of-the-art baselines. This study provides an understanding of graph\nautoencoders and demonstrates the potential of generative self-supervised\npre-training on graphs.",
        "Task": "They focus on self-supervised graph representation learning for node-level classification, graph-level classification, and transfer learning (molecular property prediction) tasks. Performance is evaluated on multiple standard benchmarks, including Cora, Citeseer, PubMed for node classification, IMDB-B, MUTAG for graph classification, and eight MoleculeNet datasets for transfer learning.",
        "Data_type": "They handle graph-structured data where each node has either continuous or discrete features (e.g., text, molecular, or degree/label attributes), and edges encode adjacency relationships. The framework applies to both node-level and graph-level tasks in primarily homogeneous graphs with masked node features.",
        "Data_domain": "They employ multiple real-world graph datasets spanning academic citation networks (Cora, Citeseer, PubMed, Ogbn-arxiv), social platforms (Reddit, IMDB), and biological or molecular contexts (PPI, chemistry). This includes both node-level and graph-level benchmarks covering diverse domains such as academic, social, and biochemical data.",
        "ML_phase": "They mask node features and feed the corrupted graph into a GNN encoder, then re-mask the encoder output for a GNN decoder that reconstructs the original features. Training is driven by a scaled cosine error objective, ensuring stable optimization with large mask ratios and no need for negative samples."
    },
    "Pretrain/2202.03104v3.json": {
        "title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data  Augmentation",
        "abs": "Graph contrastive learning (GCL) has emerged as a dominant technique for\ngraph representation learning which maximizes the mutual information between\npaired graph augmentations that share the same semantics. Unfortunately, it is\ndifficult to preserve semantics well during augmentations in view of the\ndiverse nature of graph data. Currently, data augmentations in GCL that are\ndesigned to preserve semantics broadly fall into three unsatisfactory ways.\nFirst, the augmentations can be manually picked per dataset by\ntrial-and-errors. Second, the augmentations can be selected via cumbersome\nsearch. Third, the augmentations can be obtained by introducing expensive\ndomain-specific knowledge as guidance. All of these limit the efficiency and\nmore general applicability of existing GCL methods. To circumvent these crucial\nissues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph\n\\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity,\nwhich does not require data augmentations. Specifically, we take original graph\nas input and GNN model with its perturbed version as two encoders to obtain two\ncorrelated views for contrast. SimGRACE is inspired by the observation that\ngraph data can preserve their semantics well during encoder perturbations while\nnot requiring manual trial-and-errors, cumbersome search or expensive domain\nknowledge for augmentations selection. Also, we explain why SimGRACE can\nsucceed. Furthermore, we devise adversarial training scheme, dubbed\n\\textbf{AT-SimGRACE}, to enhance the robustness of graph contrastive learning\nand theoretically explain the reasons. Albeit simple, we show that SimGRACE can\nyield competitive or better performance compared with state-of-the-art methods\nin terms of generalizability, transferability and robustness, while enjoying\nunprecedented degree of flexibility and efficiency.",
        "Task": "They address graph-level classification tasks through contrastive learning, targeting both unsupervised and semi-supervised settings. The evaluation spans multiple social and biochemical datasets to assess classification performance, transferability, and robustness.",
        "Data_type": "They handle graph-structured data, including node and edge information, primarily on social networks (e.g., COLLAB, RDT-B) and biochemical molecules (e.g., MUTAG, NCI1). The paper does not incorporate multi-modal inputs such as images or text, focusing instead on graphs with node attributes and connectivity.",
        "Data_domain": "They employ graph data from both social networks (e.g., COLLAB, RDT-B, IMDB-B) and biochemical molecules (e.g., NCI1, PROTEINS, ZINC-2M) in TUDataset. Additionally, large-scale protein–protein interaction graphs are used for pre-training and transfer tasks.",
        "ML_phase": "SimGRACE integrates into the training phase of the GNN pipeline by omitting explicit data augmentations and instead introducing random or adversarial weight perturbations to the encoder for contrastive objectives. It follows a standard model-pretraining-then-finetuning workflow, where the encoder plus projection head are learned on unlabeled graphs and subsequently adapted for downstream graph-level tasks."
    },
    "Pretrain/2110.07728v2.json": {
        "title": "Pre-training Molecular Graph Representation with 3D Geometry",
        "abs": "Molecular graph representation learning is a fundamental problem in modern\ndrug and material discovery. Molecular graphs are typically modeled by their 2D\ntopological structures, but it has been recently discovered that 3D geometric\ninformation plays a more vital role in predicting molecular functionalities.\nHowever, the lack of 3D information in real-world scenarios has significantly\nimpeded the learning of geometric graph representation. To cope with this\nchallenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework\nwhere self-supervised learning (SSL) is performed by leveraging the\ncorrespondence and consistency between 2D topological structures and 3D\ngeometric views. GraphMVP effectively learns a 2D molecular graph encoder that\nis enhanced by richer and more discriminative 3D geometry. We further provide\ntheoretical insights to justify the effectiveness of GraphMVP. Finally,\ncomprehensive experiments show that GraphMVP can consistently outperform\nexisting graph SSL methods.",
        "Task": "They conduct graph-level classification and regression tasks on molecular property prediction and drug-target affinity. These tasks are evaluated across multiple benchmark datasets, including classification sets (e.g., BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE) and regression sets (e.g., ESOL, Lipo, Malaria, CEP, Davis, KIBA).",
        "Data_type": "They handle 2D molecular graphs with atom-level node features, bond-level edge attributes, and additionally utilize 3D coordinates to incorporate geometric information as separate views. The framework also supports multiple 3D conformers per molecule, reflecting diverse spatial configurations for richer representation.",
        "Data_domain": "The data come from the chemical domain, specifically small-molecule datasets with both 2D and 3D structural information. Pretraining is performed on a large collection of molecules (GEOM), and fine-tuning targets molecular property prediction tasks.",
        "ML_phase": "During pre-training, the framework processes paired 2D topologies and 3D conformations with subgraph masking and multi-view transformations to jointly train GNN encoders via contrastive and generative objectives. In the fine-tuning phase, the learned 2D encoder is adapted on downstream tasks without 3D inputs, leveraging the pre-trained representations for improved performance."
    },
    "Pretrain/2304.04779v1.json": {
        "title": "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner",
        "abs": "Graph self-supervised learning (SSL), including contrastive and generative\napproaches, offers great potential to address the fundamental challenge of\nlabel scarcity in real-world graph data. Among both sets of graph SSL\ntechniques, the masked graph autoencoders (e.g., GraphMAE)--one type of\ngenerative method--have recently produced promising results. The idea behind\nthis is to reconstruct the node features (or structures)--that are randomly\nmasked from the input--with the autoencoder architecture. However, the\nperformance of masked feature reconstruction naturally relies on the\ndiscriminability of the input features and is usually vulnerable to disturbance\nin the features. In this paper, we present a masked self-supervised learning\nframework GraphMAE2 with the goal of overcoming this issue. The idea is to\nimpose regularization on feature reconstruction for graph SSL. Specifically, we\ndesign the strategies of multi-view random re-mask decoding and latent\nrepresentation prediction to regularize the feature reconstruction. The\nmulti-view random re-mask decoding is to introduce randomness into\nreconstruction in the feature space, while the latent representation prediction\nis to enforce the reconstruction in the embedding space. Extensive experiments\nshow that GraphMAE2 can consistently generate top results on various public\ndatasets, including at least 2.45% improvements over state-of-the-art baselines\non ogbn-Papers100M with 111M nodes and 1.6B edges.",
        "Task": "No specific machine learning task is described in the provided text. There is no mention of datasets or any evaluation of ML performance.",
        "Data_type": "No specific data type is described in the paper. Consequently, it does not address or handle any particular form of input data (e.g., textual, graph-structured, multi-modal).",
        "Data_domain": "The content primarily addresses academic publishing and manuscript preparation. No external data domain beyond general academic writing guidelines is evident.",
        "ML_phase": "This paper does not present any explicit steps for data processing, model architecture design, or training framework. It primarily discusses formatting and multilingual considerations, offering no direct information about ML pipeline phases."
    },
    "EdgeClassification/2502.15699v1.json": {
        "title": "Disentangling Popularity and Quality: An Edge Classification Approach  for Fair Recommendation",
        "abs": "Graph neural networks (GNNs) have proven to be an effective tool for\nenhancing the performance of recommender systems. However, these systems often\nsuffer from popularity bias, leading to an unfair advantage for frequently\ninteracted items, while overlooking high-quality but less popular items. In\nthis paper, we propose a GNN-based recommendation model that disentangles\npopularity and quality to address this issue. Our approach introduces an edge\nclassification technique to differentiate between popularity bias and genuine\nquality disparities among items. Furthermore, it uses cost-sensitive learning\nto adjust the misclassification penalties, ensuring that underrepresented yet\nrelevant items are not unfairly disregarded. Experimental results demonstrate\nimprovements in fairness metrics by approximately 2-74%, while maintaining\ncompetitive accuracy, with only minor variations compared to state-of-the-art\nmethods.",
        "Task": "This work addresses an edge-level classification task on user–item bipartite graphs for recommendation, using real-world Bookcrossing, Amazon CDs, and Amazon Electronics datasets to evaluate performance. It classifies both observed and unobserved user–item interactions to mitigate popularity bias while preserving accuracy.",
        "Data_type": "They use user–item bipartite graphs, where nodes represent users and items, and edges encode interaction histories or ratings. No additional textual, multi-modal, or chemical string representations are involved, focusing strictly on these user–item edges in a homogeneous graph.",
        "Data_domain": "They utilize user-item interaction data from one book-sharing website (Bookcrossing) and two Amazon product categories (CDs and Electronics). Hence, the domain is e-commerce and book-rating platforms, reflecting typical online recommendation scenarios.",
        "ML_phase": "They first preprocess the interaction graph by removing edges corresponding to low-quality items, identified via a combination of baseline estimation and item degree thresholds. Then, they train using a cost-sensitive edge classification framework (replacing the BPR objective with cross-entropy), adjusting misclassification penalties to reduce popularity bias while preserving accuracy."
    },
    "EdgeClassification/2405.01844v3.json": {
        "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,  Solutions, and Challenges",
        "abs": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
        "Task": "They focus on predicting content popularity (regression/forecasting) and user preferences, as well as learning optimal caching policies (reinforcement learning) to reduce service latency and network costs. These tasks center on leveraging historical or real-time request data to guide proactive cache placement and management in edge networks.",
        "Data_type": "They address user-generated request traces, personal identifiers (e.g., textual records), geographic location data, and private content (including multimedia) stored or delivered at the edge. The work also covers content popularity and knowledge data derived from user logs, encompassing both textual and numeric features for caching decisions.",
        "Data_domain": "The paper’s data primarily originates from network usage and user-generated logs in edge environments spanning IoT, vehicular networks, and social media applications. It includes user requests, personal and location information, as well as derived content popularity data used to inform caching decisions.",
        "ML_phase": "Data is gathered from user requests, subjected to privacy-preserving transformations (e.g., obfuscation or differential privacy), and then fed into federated or distributed ML frameworks (often combined with reinforcement learning) for model training. These approaches optimize parameter updates while preventing direct exposure of raw training data, forming an end-to-end pipeline from data ingestion to model deployment."
    },
    "EdgeClassification/llm_llm_2410.10285v2.json": {
        "title": "ABBA-VSM: Time Series Classification using Symbolic Representation on  the Edge",
        "abs": "In recent years, Edge AI has become more prevalent with applications across\nvarious industries, from environmental monitoring to smart city management.\nEdge AI facilitates the processing of Internet of Things (IoT) data and\nprovides privacy-enabled and latency-sensitive services to application users\nusing Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC).\nHowever, existing TSC algorithms require access to full raw data and demand\nsubstantial computing resources to train and use them effectively in runtime.\nThis makes them impractical for deployment in resource-constrained Edge\nenvironments. To address this, in this paper, we propose an Adaptive Brownian\nBridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new\nTSC model designed for classification services on Edge. Here, we first\nadaptively compress the raw time series into symbolic representations, thus\ncapturing the changing trends of data. Subsequently, we train the\nclassification model directly on these symbols. ABBA-VSM reduces communication\ndata between IoT and Edge devices, as well as computation cycles, in the\ndevelopment of resource-efficient TSC services on Edge. We evaluate our\nsolution with extensive experiments using datasets from the UCR time series\nclassification archive. The results demonstrate that the ABBA-VSM achieves up\nto 80% compression ratio and 90-100% accuracy for binary classification.\nWhereas, for non-binary classification, it achieves an average compression\nratio of 60% and accuracy ranging from 60-80%.",
        "Task": "They address a time series classification task to predict class labels of time series data. They evaluate their approach on multiple real-world time series classification datasets (e.g., from the UCR Archive).",
        "Data_type": "They operate on univariate numeric time-series data generated by IoT devices. The focus is on reducing and encoding 1D time-series data for classification.",
        "Data_domain": "The data is time-series measurements drawn from IoT devices in various industries (e.g., environmental monitoring, city management) and from the UCR Time Series Classification Archive (covering domains such as algae, beef, fish, and coffee classification). These datasets encompass diverse real-world contexts, reflecting both industrial and academic benchmarks for time-series analysis.",
        "ML_phase": "They compress raw time series at the IoT device, send segments to the Edge for clustering into symbols, and apply a sliding window to form bag-of-words. Then they train a TF-IDF-based vector space model on these symbolic representations, using cosine similarity for classification."
    },
    "EdgeClassification/2306.11330v2.json": {
        "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on  FPGAs",
        "abs": "In-time particle trajectory reconstruction in the Large Hadron Collider is\nchallenging due to the high collision rate and numerous particle hits. Using\nGNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible\ntrajectory classification. However, existing GNN architectures have inefficient\nresource usage and insufficient parallelism for edge classification. This paper\nintroduces a resource-efficient GNN architecture on FPGAs for low latency\nparticle tracking. The modular architecture facilitates design scalability to\nsupport large graphs. Leveraging the geometric properties of hit detectors\nfurther reduces graph complexity and resource usage. Our results on Xilinx\nUltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU\nand GPU respectively.",
        "Task": "They address an edge-level classification task for particle track reconstruction in graphs derived from LHC collision data. They use the TrackML dataset to evaluate performance in identifying which edges correspond to true trajectories.",
        "Data_type": "They process graph-structured data that uses node features (hits) and edge information (potential connections) for trajectory reconstruction. The data is numeric and homogeneous, focusing exclusively on node-edge relationships without additional textual or multimodal attributes.",
        "Data_domain": "The data in this paper originates from high-energy physics, specifically proton-proton collision events at the Large Hadron Collider (LHC).",
        "ML_phase": "Collision hits are transformed into node-based graphs, where edges denote potential track segments, and an Interaction Network–based GNN (Edgeblock → Aggregate → Nodeblock) is used for edge classification. The model is synthesized to an FPGA implementation through hls4ml, leveraging modular parallelism and geometry-based constraints within the pipeline."
    },
    "EdgeClassification/2410.10285v2.json": {
        "title": "ABBA-VSM: Time Series Classification using Symbolic Representation on  the Edge",
        "abs": "In recent years, Edge AI has become more prevalent with applications across\nvarious industries, from environmental monitoring to smart city management.\nEdge AI facilitates the processing of Internet of Things (IoT) data and\nprovides privacy-enabled and latency-sensitive services to application users\nusing Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC).\nHowever, existing TSC algorithms require access to full raw data and demand\nsubstantial computing resources to train and use them effectively in runtime.\nThis makes them impractical for deployment in resource-constrained Edge\nenvironments. To address this, in this paper, we propose an Adaptive Brownian\nBridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new\nTSC model designed for classification services on Edge. Here, we first\nadaptively compress the raw time series into symbolic representations, thus\ncapturing the changing trends of data. Subsequently, we train the\nclassification model directly on these symbols. ABBA-VSM reduces communication\ndata between IoT and Edge devices, as well as computation cycles, in the\ndevelopment of resource-efficient TSC services on Edge. We evaluate our\nsolution with extensive experiments using datasets from the UCR time series\nclassification archive. The results demonstrate that the ABBA-VSM achieves up\nto 80% compression ratio and 90-100% accuracy for binary classification.\nWhereas, for non-binary classification, it achieves an average compression\nratio of 60% and accuracy ranging from 60-80%.",
        "Task": "They focus on a time series classification task, evaluating performance on multiple real-world datasets from the UCR Time Series Classification Archive.",
        "Data_type": "ABBA-VSM is designed for univariate numeric time series data generated by IoT devices. It does not incorporate graph-based structures, multi-modal image-text pairs, or chemical representations like SMILES/fingerprints.",
        "Data_domain": "They rely on univariate time-series data drawn from the UCR Time Series Classification Archive, encompassing diverse domains such as sensor readings, spectrograms, and smart device outputs. Much of this data originates from IoT contexts, including environmental monitoring and smart city applications.",
        "ML_phase": "They compress time series data at the IoT device using an adaptive Brownian-bridge method, transmitting only reduced segments for symbolic representation at the Edge. The Edge then constructs a TF-IDF-based vector space model from these symbols and classifies new samples via cosine similarity."
    },
    "EdgeClassification/2409.08943v1.json": {
        "title": "Pushing Joint Image Denoising and Classification to the Edge",
        "abs": "In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.",
        "Task": "They investigate two supervised learning tasks: image denoising (restoring clean images from noisy inputs) and image classification (label prediction). Performance is evaluated on synthetic datasets with varying noise levels and subsets of ImageNet for both denoising quality and classification accuracy.",
        "Data_type": "They handle 2D image data (both synthetic MNIST-based and ImageNet) under varying levels of additive white Gaussian noise. The datasets include grayscale or color images for tasks of denoising and classification.",
        "Data_domain": "They use synthetic image data (e.g., MNIST overlays) and a subset of ImageNet, focusing on real-time processing of noisy images. The paper also targets broader image-based domains such as medical imaging, surveillance, and industrial inspections where noise poses significant challenges.",
        "ML_phase": "They first generate noisy data by adding synthetic Gaussian noise, then design an integrated framework that shares an encoder for both denoising and classification. The model is trained with a combined classification and denoising loss, and further refined through neural architecture search to meet specific latency targets."
    },
    "EdgeClassification/2010.13737v2.json": {
        "title": "Real-Time Edge Classification: Optimal Offloading under Token Bucket  Constraints",
        "abs": "To deploy machine learning-based algorithms for real-time applications with\nstrict latency constraints, we consider an edge-computing setting where a\nsubset of inputs are offloaded to the edge for processing by an accurate but\nresource-intensive model, and the rest are processed only by a less-accurate\nmodel on the device itself. Both models have computational costs that match\navailable compute resources, and process inputs with low-latency. But\noffloading incurs network delays, and to manage these delays to meet\napplication deadlines, we use a token bucket to constrain the average rate and\nburst length of transmissions from the device. We introduce a Markov Decision\nProcess-based framework to make offload decisions under these constraints,\nbased on the local model's confidence and the token bucket state, with the goal\nof minimizing a specified error measure for the application. Beyond isolated\ndecisions for individual devices, we also propose approaches to allow multiple\ndevices connected to the same access switch to share their bursting allocation.\nWe evaluate and analyze the policies derived using our framework on the\nstandard ImageNet image classification benchmark.",
        "Task": "They tackle real-time image classification, using policies that decide whether to process inputs locally or offload them while respecting strict transmission constraints. Their framework is empirically validated on the ImageNet dataset to assess classification performance.",
        "Data_type": "They handle single-modality image data, specifically from the ImageNet benchmark, with 1000 classes. No other data modalities (e.g., text or graph-structured inputs) are addressed.",
        "Data_domain": "The data are large-scale images drawn from the ImageNet dataset, a canonical academic benchmark for visual recognition research. They cover a wide range of image classes commonly used to evaluate classification tasks in machine learning.",
        "ML_phase": "They employ pre-trained weak and strong classifiers, use ImageNet data for calibration, and derive an offloading metric from the weak classifier’s entropy. An MDP-based threshold policy is then trained offline via value iteration to optimize offloading decisions under token-bucket constraints."
    },
    "EdgeClassification/1612.01337v2.json": {
        "title": "Classification With an Edge: Improving Semantic Image Segmentation with  Boundary Detection",
        "abs": "We present an end-to-end trainable deep convolutional neural network (DCNN)\nfor semantic segmentation with built-in awareness of semantically meaningful\nboundaries. Semantic segmentation is a fundamental remote sensing task, and\nmost state-of-the-art methods rely on DCNNs as their workhorse. A major reason\nfor their success is that deep networks learn to accumulate contextual\ninformation over very large windows (receptive fields). However, this success\ncomes at a cost, since the associated loss of effecive spatial resolution\nwashes out high-frequency details and leads to blurry object boundaries. Here,\nwe propose to counter this effect by combining semantic segmentation with\nsemantically informed edge detection, thus making class-boundaries explicit in\nthe model, First, we construct a comparatively simple, memory-efficient model\nby adding boundary detection to the Segnet encoder-decoder architecture.\nSecond, we also include boundary detection in FCN-type models and set up a\nhigh-end classifier ensemble. We show that boundary detection significantly\nimproves semantic segmentation with CNNs. Our high-end ensemble achieves > 90%\noverall accuracy on the ISPRS Vaihingen benchmark.",
        "Task": "They tackle a pixel-level semantic segmentation task on high-resolution aerial imagery, assigning land-cover labels to every pixel. Their approach is evaluated on the ISPRS Vaihingen and Potsdam datasets to benchmark segmentation performance.",
        "Data_type": "They use high-resolution aerial imagery, often with four spectral bands (RGB and near-infrared) plus derived height channels (DSM/nDSM). This multi-channel 2D grid input captures both spectral and elevation information for pixel-wise semantic segmentation.",
        "Data_domain": "These experiments focus on remote sensing data, specifically very high‐resolution aerial imagery and corresponding height data (DSM/nDSM) from urban regions. The datasets come from the ISPRS Vaihingen and Potsdam benchmarks, which provide annotated ground truth for land cover mapping tasks.",
        "ML_phase": "They employ a multi-stage pipeline that first trains a boundary detection (HED-like) network and then integrates it into a segmentation model (SegNet or FCN-based), using tile-based inputs, data augmentation (scaling, rotation, reflections), and carefully tuned hyperparameters. Each component is assembled end-to-end, with multi-scale fusion and optional ensemble averaging of separately trained networks to enhance overall accuracy."
    },
    "EdgeClassification/llm_2410.10285v2.json": {
        "title": "ABBA-VSM: Time Series Classification using Symbolic Representation on  the Edge",
        "abs": "In recent years, Edge AI has become more prevalent with applications across\nvarious industries, from environmental monitoring to smart city management.\nEdge AI facilitates the processing of Internet of Things (IoT) data and\nprovides privacy-enabled and latency-sensitive services to application users\nusing Machine Learning (ML) algorithms, e.g., Time Series Classification (TSC).\nHowever, existing TSC algorithms require access to full raw data and demand\nsubstantial computing resources to train and use them effectively in runtime.\nThis makes them impractical for deployment in resource-constrained Edge\nenvironments. To address this, in this paper, we propose an Adaptive Brownian\nBridge-based Symbolic Aggregation Vector Space Model (ABBA-VSM). It is a new\nTSC model designed for classification services on Edge. Here, we first\nadaptively compress the raw time series into symbolic representations, thus\ncapturing the changing trends of data. Subsequently, we train the\nclassification model directly on these symbols. ABBA-VSM reduces communication\ndata between IoT and Edge devices, as well as computation cycles, in the\ndevelopment of resource-efficient TSC services on Edge. We evaluate our\nsolution with extensive experiments using datasets from the UCR time series\nclassification archive. The results demonstrate that the ABBA-VSM achieves up\nto 80% compression ratio and 90-100% accuracy for binary classification.\nWhereas, for non-binary classification, it achieves an average compression\nratio of 60% and accuracy ranging from 60-80%.",
        "Task": "They address a time series classification task, aiming to assign discrete labels to univariate sequences. The method is evaluated on multiple binary and multiclass datasets from the UCR Time Series Classification Archive.",
        "Data_type": "They exclusively handle univariate numerical time-series data, compressing and symbolizing raw IoT sensor outputs. No multi-modal, graph-structured, or text-annotated data is addressed, as the approach remains focused on numeric time-series signals.",
        "Data_domain": "They work with time series data originating from various real-world IoT applications, such as environmental monitoring and smart city management. Additionally, experiments draw on multiple datasets from the UCR Time Series Classification Archive, covering diverse domains (e.g., biological, agricultural, and industrial).",
        "ML_phase": "ABBA-VSM’s pipeline begins with adaptive data reduction on the IoT device, followed by symbolic encoding and bag-of-words generation on the Edge for classification. The training framework uses TF-IDF weighting to construct a vector space model, enabling efficient similarity-based classification on compressed representations."
    },
    "EdgeClassification/2002.04180v2.json": {
        "title": "LoCEC: Local Community-based Edge Classification in Large Online Social  Networks",
        "abs": "Relationships in online social networks often imply social connections in the\nreal world. An accurate understanding of relationship types benefits many\napplications, e.g. social advertising and recommendation. Some recent attempts\nhave been proposed to classify user relationships into predefined types with\nthe help of pre-labeled relationships or abundant interaction features on\nrelationships. Unfortunately, both relationship feature data and label data are\nvery sparse in real social platforms like WeChat, rendering existing methods\ninapplicable. In this paper, we present an in-depth analysis of WeChat\nrelationships to identify the major challenges for the relationship\nclassification task. To tackle the challenges, we propose a Local\nCommunity-based Edge Classification (LoCEC) framework that classifies user\nrelationships in a social network into real-world social connection types.\nLoCEC enforces a three-phase processing, namely local community detection,\ncommunity classification and relationship classification, to address the\nsparsity issue of relationship features and relationship labels. Moreover,\nLoCEC is designed to handle large-scale networks by allowing parallel and\ndistributed processing. We conduct extensive experiments on the real-world\nWeChat network with hundreds of billions of edges to validate the effectiveness\nand efficiency of LoCEC.",
        "Task": "No information is provided to identify any machine learning task. The paper excerpt does not mention a specific task or related datasets.",
        "Data_type": "No data types are specified in the provided text, so the paper’s handling of any particular input modality cannot be determined.",
        "Data_domain": "No information about the data’s origin is provided in the text. Consequently, the data domain cannot be determined.",
        "ML_phase": "No information is provided about the ML pipeline stages. The text does not describe data processing, model architecture, or any training framework."
    },
    "EdgeClassification/2205.11322v2.json": {
        "title": "Revisiting the role of heterophily in graph representation learning: An  edge classification perspective",
        "abs": "Graph representation learning aim at integrating node contents with graph\nstructure to learn nodes/graph representations. Nevertheless, it is found that\nmany existing graph learning methods do not work well on data with high\nheterophily level that accounts for a large proportion of edges between\ndifferent class labels. Recent efforts to this problem focus on improving the\nmessage passing mechanism. However, it remains unclear whether heterophily\ntruly does harm to the performance of graph neural networks (GNNs). The key is\nto unfold the relationship between a node and its immediate neighbors, e.g.,\nare they heterophilous or homophilious? From this perspective, here we study\nthe role of heterophily in graph representation learning before/after the\nrelationships between connected nodes are disclosed. In particular, we propose\nan end-to-end framework that both learns the type of edges (i.e.,\nheterophilous/homophilious) and leverage edge type information to improve the\nexpressiveness of graph neural networks. We implement this framework in two\ndifferent ways. Specifically, to avoid messages passing through heterophilous\nedges, we can optimize the graph structure to be homophilious by dropping\nheterophilous edges identified by an edge classifier. Alternatively, it is\npossible to exploit the information about the presence of heterophilous\nneighbors for feature learning, so a hybrid message passing approach is devised\nto aggregate homophilious neighbors and diversify heterophilous neighbors based\non edge classification. Extensive experiments demonstrate the remarkable\nperformance improvement of GNNs with the proposed framework on multiple\ndatasets across the full spectrum of homophily level.",
        "Task": "They study node-level classification on graphs of varying homophily, aiming to improve accuracy by identifying and leveraging heterophilious edges. Multiple benchmark graph datasets are used to evaluate node classification performance.",
        "Data_type": "They handle graph-structured data where each node is accompanied by feature embeddings (e.g., textual attributes), and edges can be homophilious or heterophilous. The paper’s experiments focus on benchmark graphs with node-level attributes and adjacency matrices, such as citation networks and web networks.",
        "Data_domain": "These experiments are conducted on diverse real-world graph datasets, including academic citation networks (Cora, Citeseer, Pubmed), web page link networks (Cornell, Texas, Wisconsin, Chameleon, Squirrel), and a film co-occurrence network (Film). This provides a comprehensive coverage of different homophily levels across multiple domains.",
        "ML_phase": "They first train a binary classifier to label edges as homophilous or heterophilous, then either prune or separate these edges for customized message passing within a GNN. The entire framework is optimized end-to-end (via a Gumbel-softmax-based mechanism) so that edge-type identification and node-level classification are jointly learned."
    },
    "EdgeClassification/2412.08310v1.json": {
        "title": "Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic  Graphs without Message Passing",
        "abs": "Message Passing Neural Networks (MPNNs) have demonstrated remarkable success\nin node classification on homophilic graphs. It has been shown that they do not\nsolely rely on homophily but on neighborhood distributions of nodes, i.e.,\nconsistency of the neighborhood label distribution within the same class.\nMLP-based models do not use message passing, \\eg Graph-MLP incorporates the\nneighborhood in a separate loss function. These models are faster and more\nrobust to edge noise. Graph-MLP maps adjacent nodes closer in the embedding\nspace but is unaware of the neighborhood pattern of the labels, i.e., relies\nsolely on homophily. Edge Splitting GNN (ES-GNN) is a model specialized for\nheterophilic graphs and splits the edges into task-relevant and\ntask-irrelevant, respectively. To mitigate the limitations of Graph-MLP on\nheterophilic graphs, we propose ES-MLP that combines Graph-MLP with an\nedge-splitting mechanism from ES-GNN. It incorporates the edge splitting into\nthe loss of Graph-MLP to learn two separate adjacency matrices based on\nrelevant and irrelevant feature pairs. Our experiments on seven datasets with\nsix baselines show that ES-MLP is on par with homophilic and heterophilic\nmodels on all datasets without using edges during inference. We show that\nES-MLP is robust to multiple types of edge noise during inference and that its\ninference time is two to five times faster than that of commonly used MPNNs.\nThe source code is available at https://github.com/MatthiasKohn/ES-MLP.",
        "Task": "They tackle a node-level classification task on graphs exhibiting both homophily and heterophily. To evaluate performance, they use multiple real-world datasets (Cora, CiteSeer, PubMed, Actor, Amazon, Roman, Minesweeper) and a synthetic Contextual Stochastic Block Model.",
        "Data_type": "They address node-level classification tasks on homogeneous graphs, where nodes can have textual or numeric attributes (e.g., bag-of-words, TF-IDF, or one-hot-encoded features). The paper’s experiments include citation networks, co-purchase graphs, social co-occurrence networks, and synthetic graphs, covering both homophilic and heterophilic settings.",
        "Data_domain": "They use real-world graph datasets spanning citation networks (Cora, CiteSeer, PubMed) and multiple social or interaction-based networks (Wikipedia co-occurrences, Amazon co-purchases, Minesweeper grid). Additionally, a synthetic dataset from the Contextual Stochastic Block Model is employed to vary homophily levels.",
        "ML_phase": "They use real-world graphs and generate synthetic data via a Contextual Stochastic Block Model, then split each edge into task-relevant or irrelevant sets before training. The ES-MLP architecture applies a neighborhood contrastive loss over these sets to learn MLP-based node embeddings, requiring adjacency only for training and not at inference."
    },
    "kdd22/2205.11327v1.json": {
        "title": "HyperLogLogLog: Cardinality Estimation With One Log More",
        "abs": "We present HyperLogLogLog, a practical compression of the HyperLogLog sketch\nthat compresses the sketch from $O(m\\log\\log n)$ bits down to $m\n\\log_2\\log_2\\log_2 m + O(m+\\log\\log n)$ bits for estimating the number of\ndistinct elements~$n$ using $m$~registers. The algorithm works as a drop-in\nreplacement that preserves all estimation properties of the HyperLogLog sketch,\nit is possible to convert back and forth between the compressed and\nuncompressed representations, and the compressed sketch maintains mergeability\nin the compressed domain. The compressed sketch can be updated in amortized\nconstant time, assuming $n$ is sufficiently larger than $m$. We provide a C++\nimplementation of the sketch, and show by experimental evaluation against\nwell-known implementations by Google and Apache that our implementation\nprovides small sketches while maintaining competitive update and merge times.\nConcretely, we observed approximately a 40% reduction in the sketch size.\nFurthermore, we obtain as a corollary a theoretical algorithm that compresses\nthe sketch down to $m\\log_2\\log_2\\log_2\\log_2 m+O(m\\log\\log\\log m/\\log\\log\nm+\\log\\log n)$ bits.",
        "Task": "They address the problem of cardinality estimation in data streams, aiming to approximate the number of distinct elements. They evaluate performance on large-scale datasets by measuring how accurately the sketch-based approach estimates these distinct counts.",
        "Data_type": "They handle large-scale streams of distinct elements, such as 64-bit integers or ASCII strings, by mapping inputs to uniform hash values. The approach is domain-agnostic and only requires the data to be hashable.",
        "Data_domain": "The paper deals with data streams originating from diverse sources such as network traffic, large-scale datasets, genome data, and social networks. In particular, it encompasses large-scale industrial applications (e.g., distinct Google search queries) and other high-volume data domains.",
        "ML_phase": "This work presents a memory-efficient sketching method for counting distinct elements during data ingestion or preprocessing steps, particularly suited for large-scale or streaming scenarios. By compactly estimating cardinalities, it supports more efficient downstream feature engineering and reduces overhead in subsequent ML tasks."
    },
    "kdd22/2206.00449v1.json": {
        "title": "Ultrahyperbolic Knowledge Graph Embeddings",
        "abs": "Recent knowledge graph (KG) embeddings have been advanced by hyperbolic\ngeometry due to its superior capability for representing hierarchies. The\ntopological structures of real-world KGs, however, are rather heterogeneous,\ni.e., a KG is composed of multiple distinct hierarchies and non-hierarchical\ngraph structures. Therefore, a homogeneous (either Euclidean or hyperbolic)\ngeometry is not sufficient for fairly representing such heterogeneous\nstructures. To capture the topological heterogeneity of KGs, we present an\nultrahyperbolic KG embedding (UltraE) in an ultrahyperbolic (or\npseudo-Riemannian) manifold that seamlessly interleaves hyperbolic and\nspherical manifolds. In particular, we model each relation as a\npseudo-orthogonal transformation that preserves the pseudo-Riemannian bilinear\nform. The pseudo-orthogonal transformation is decomposed into various operators\n(i.e., circular rotations, reflections and hyperbolic rotations), allowing for\nsimultaneously modeling heterogeneous structures as well as complex relational\npatterns. Experimental results on three standard KGs show that UltraE\noutperforms previous Euclidean- and hyperbolic-based approaches.",
        "Task": "They address a link prediction task on knowledge graphs, aiming to predict missing links (triples) between entities. Their approach is evaluated on standard KG completion benchmarks including WN18RR, FB15k-237, and YAGO3-10.",
        "Data_type": "They operate on large-scale knowledge graphs with hierarchical and non-hierarchical relations, as exemplified by textual-based datasets like WN18RR, FB15k-237, and YAGO3-10. Nodes are entities (often with textual identifiers), and edges capture multi-relational structured data in a standard textual format.",
        "Data_domain": "These datasets are knowledge graphs originating from lexical sources (e.g., WordNet), general knowledge bases (e.g., Freebase), and encyclopedic resources (e.g., YAGO). They capture hierarchical and non-hierarchical relationships among entities.",
        "ML_phase": "UltraE embeds entities in an ultrahyperbolic manifold and defines relations via pseudo-orthogonal transformations (decomposed into circular and hyperbolic rotations), providing a heterogeneous geometry for hierarchical and non-hierarchical KG structures. It is trained by generating negative samples (corrupted heads or tails) and optimizing a Manhattan-like distance-based objective through standard gradient descent on a differentiable diffeomorphism from Euclidean space."
    },
    "kdd22/2208.07239v1.json": {
        "title": "ROLAND: Graph Learning Framework for Dynamic Graphs",
        "abs": "Graph Neural Networks (GNNs) have been successfully applied to many\nreal-world static graphs. However, the success of static graphs has not fully\ntranslated to dynamic graphs due to the limitations in model design, evaluation\nsettings, and training strategies. Concretely, existing dynamic GNNs do not\nincorporate state-of-the-art designs from static GNNs, which limits their\nperformance. Current evaluation settings for dynamic GNNs do not fully reflect\nthe evolving nature of dynamic graphs. Finally, commonly used training methods\nfor dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph\nrepresentation learning framework for real-world dynamic graphs. At its core,\nthe ROLAND framework can help researchers easily repurpose any static GNN to\ndynamic graphs. Our insight is to view the node embeddings at different GNN\nlayers as hierarchical node states and then recurrently update them over time.\nWe then introduce a live-update evaluation setting for dynamic graphs that\nmimics real-world use cases, where GNNs are making predictions and being\nupdated on a rolling basis. Finally, we propose a scalable and efficient\ntraining approach for dynamic GNNs via incremental training and meta-learning.\nWe conduct experiments over eight different dynamic graph datasets on future\nlink prediction tasks. Models built using the ROLAND framework achieve on\naverage 62.7% relative mean reciprocal rank (MRR) improvement over\nstate-of-the-art baselines under the standard evaluation settings on three\ndatasets. We find state-of-the-art baselines experience out-of-memory errors\nfor larger datasets, while ROLAND can easily scale to dynamic graphs with 56\nmillion edges. After re-implementing these baselines using the ROLAND training\nstrategy, ROLAND models still achieve on average 15.5% relative MRR improvement\nover the baselines.",
        "Task": "They address a link-level predictive task on dynamic graphs, specifically forecasting future edges given historical snapshots. Their experiments evaluate link prediction performance on eight real-world dynamic graph datasets.",
        "Data_type": "They focus on dynamic graph-structured data with per-node attributes (e.g., up to five categorical features) and per-edge features (e.g., timestamps, transaction amounts, or trust scores). The framework scales to large dynamic networks (up to 56 million edges) spanning financial transactions, social trust links, and internet traffic flows.",
        "Data_domain": "These dynamic graph datasets originate from diverse domains, primarily encompassing financial transaction networks (e.g., Bank of Slovenia), social trust networks (e.g., Bitcoin-OTC/Alpha), internet router connectivity (AS-733), private message exchanges (UCI-Message), and online forum hyperlinks (Reddit). Consequently, the data span both economic and social network settings, capturing real-world temporal changes across multiple contexts.",
        "ML_phase": "They process data as sequential graph snapshots and reuse static GNN designs with newly introduced update modules to maintain hierarchical node states. Training proceeds incrementally via truncated backpropagation, augmented by meta-learning for rapid adaptation, and is evaluated through a live-update procedure mirroring real-world model updates."
    },
    "kdd22/2206.11477v1.json": {
        "title": "RetroGraph: Retrosynthetic Planning with Graph Search",
        "abs": "Retrosynthetic planning, which aims to find a reaction pathway to synthesize\na target molecule, plays an important role in chemistry and drug discovery.\nThis task is usually modeled as a search problem. Recently, data-driven methods\nhave attracted many research interests and shown promising results for\nretrosynthetic planning. We observe that the same intermediate molecules are\nvisited many times in the searching process, and they are usually independently\ntreated in previous tree-based methods (e.g., AND-OR tree search, Monte Carlo\ntree search). Such redundancies make the search process inefficient. We propose\na graph-based search policy that eliminates the redundant explorations of any\nintermediate molecules. As searching over a graph is more complicated than over\na tree, we further adopt a graph neural network to guide the search over\ngraphs. Meanwhile, our method can search a batch of targets together in the\ngraph and remove the inter-target duplication in the tree-based search methods.\nExperimental results on two datasets demonstrate the effectiveness of our\nmethod. Especially on the widely used USPTO benchmark, we improve the search\nsuccess rate to 99.47%, advancing previous state-of-the-art performance for 2.6\npoints.",
        "Task": "They address a multi-step retrosynthetic planning problem by combining a template-based single-step retrosynthesis model (treated as a multi-class classification task) and a GNN-based node-level scoring approach for guiding graph search. Performance is evaluated on the USPTO and USPTOext datasets.",
        "Data_type": "They handle graph-structured molecular data where nodes represent unique molecules (using SMILES and Morgan fingerprints) and edges represent reactions. The approach focuses on textual-based chemical information without incorporating multimodal data.",
        "Data_domain": "The available data are chemical reactions and molecule structures drawn from USPTO (United States Patent Office) patent data and extended molecules from PubChem. This work centers on synthesizing novel molecules within the chemical and drug discovery domain.",
        "ML_phase": "They train a template-based single-step MLP (using USPTO-derived data) to generate candidate reactions and construct a unified graph where repeated molecule nodes are merged. A graph neural network (GNN) is then trained offline on intermediate search graphs to guide node expansions in a bottom-up planning framework."
    },
    "kdd22/2110.14890v2.json": {
        "title": "SMORE: Knowledge Graph Completion and Multi-hop Reasoning in Massive  Knowledge Graphs",
        "abs": "Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail\ntriples and are a crucial component in many AI systems. There are two important\nreasoning tasks on KGs: (1) single-hop knowledge graph completion, which\ninvolves predicting individual links in the KG; and (2), multi-hop reasoning,\nwhere the goal is to predict which KG entities satisfy a given logical query.\nEmbedding-based methods solve both tasks by first computing an embedding for\neach entity and relation, then using them to form predictions. However,\nexisting scalable KG embedding frameworks only support single-hop knowledge\ngraph completion and cannot be applied to the more challenging multi-hop\nreasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first\ngeneral framework for both single-hop and multi-hop reasoning in KGs. Using a\nsingle machine SMORE can perform multi-hop reasoning in Freebase KG (86M\nentities, 338M edges), which is 1,500x larger than previously considered KGs.\nThe key to SMORE's runtime performance is a novel bidirectional rejection\nsampling that achieves a square root reduction of the complexity of online\ntraining data generation. Furthermore, SMORE exploits asynchronous scheduling,\noverlapping CPU-based data sampling, GPU-based embedding computation, and\nfrequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over\nprior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB\nfor training 400-dim embeddings on 86M-node Freebase) and achieves near linear\nspeed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge\ngraph completion task SMORE achieves comparable or even better runtime\nperformance to state-of-the-art frameworks on both single GPU and multi-GPU\nsettings.",
        "Task": "They address single-hop link prediction (knowledge graph completion) and multi-hop logical query answering, both framed as link-level reasoning tasks on knowledge graphs. Their approach is evaluated on multiple large-scale KG benchmarks (e.g., FB400k, ogbl-wikikg2, and a full Freebase dataset with 86M entities) to measure predictive performance.",
        "Data_type": "They handle heterogeneous graph-structured data, where nodes represent entities (potentially with textual attributes) and edges represent typed relations. The approach is specifically designed for large-scale knowledge graphs with head–relation–tail triples.",
        "Data_domain": "They use large-scale knowledge graphs collected from real-world knowledge bases, including Freebase, NELL, and Wikikg2. These open-domain KGs cover broad factual knowledge ranging from general-purpose to academically oriented sources.",
        "ML_phase": "They introduce a pipeline for large-scale multi-hop knowledge graph reasoning that includes on-the-fly query instantiation (reverse sampling), bidirectional negative sampling using optimal node cuts, and asynchronous multi-GPU training with shared CPU memory for embeddings. This design handles data generation (query and negative example sampling), integrates neural logical operators for model updates, and achieves near-linear speedup in distributed training without graph partitioning."
    },
    "kdd22/2205.15896v2.json": {
        "title": "FedWalk: Communication Efficient Federated Unsupervised Node Embedding  with Differential Privacy",
        "abs": "Node embedding aims to map nodes in the complex graph into low-dimensional\nrepresentations. The real-world large-scale graphs and difficulties of labeling\nmotivate wide studies of unsupervised node embedding problems. Nevertheless,\nprevious effort mostly operates in a centralized setting where a complete graph\nis given. With the growing awareness of data privacy, data holders who are only\naware of one vertex and its neighbours demand greater privacy protection. In\nthis paper, we introduce FedWalk, a random-walk-based unsupervised node\nembedding algorithm that operates in such a node-level visibility graph with\nraw graph information remaining locally. FedWalk is designed to offer\ncentralized competitive graph representation capability with data privacy\nprotection and great communication efficiency. FedWalk instantiates the\nprevalent federated paradigm and contains three modules. We first design a\nhierarchical clustering tree (HCT) constructor to extract the structural\nfeature of each node. A dynamic time warping algorithm seamlessly handles the\nstructural heterogeneity across different nodes. Based on the constructed HCT,\nwe then design a random walk generator, wherein a sequence encoder is designed\nto preserve privacy and a two-hop neighbor predictor is designed to save\ncommunication cost. The generated random walks are then used to update node\nembedding based on a SkipGram model. Extensive experiments on two large graphs\ndemonstrate that Fed-Walk achieves competitive representativeness as a\ncentralized node embedding algorithm does with only up to 1.8% Micro-F1 score\nand 4.4% Marco-F1 score loss while reducing about 6.7 times of inter-device\ncommunication per walk.",
        "Task": "They tackle an unsupervised node embedding problem to learn representations for downstream multi-label classification (node-level). Performance is evaluated on Blogcatalog and Flickr datasets via Micro-F1 and Macro-F1 metrics.",
        "Data_type": "They handle homogeneous graph-structured data at the node level, using node–edge information (adjacency relationships). The paper’s approach produces 128-dimensional embeddings for each node.",
        "Data_domain": "They use large-scale social networks as the data domain, with nodes representing users and edges signifying user connections. Specifically, the paper’s experiments rely on datasets such as BlogCatalog and Flickr to evaluate node embedding performance.",
        "ML_phase": "FedWalk processes data by first constructing a hierarchical clustering tree using locally computed, differentially private degree vectors, then generates random-walk sequences (with an optional two-hop neighbor predictor) that are encoded and fed to a Skip-Gram model. The training framework thus involves local data partitioning, tree-based sequence encoding, and iterative updates via the unsupervised Skip-Gram objective in a federated manner."
    },
    "kdd22/2108.07926v3.json": {
        "title": "Collaboration Equilibrium in Federated Learning",
        "abs": "Federated learning (FL) refers to the paradigm of learning models over a\ncollaborative research network involving multiple clients without sacrificing\nprivacy. Recently, there have been rising concerns on the distributional\ndiscrepancies across different clients, which could even cause\ncounterproductive consequences when collaborating with others. While it is not\nnecessarily that collaborating with all clients will achieve the best\nperformance, in this paper, we study a rational collaboration called\n``collaboration equilibrium'' (CE), where smaller collaboration coalitions are\nformed. Each client collaborates with certain members who maximally improve the\nmodel learning and isolates the others who make little contribution. We propose\nthe concept of benefit graph which describes how each client can benefit from\ncollaborating with other clients and advance a Pareto optimization approach to\nidentify the optimal collaborators. Then we theoretically prove that we can\nreach a CE from the benefit graph through an iterative graph operation. Our\nframework provides a new way of setting up collaborations in a research\nnetwork. Experiments on both synthetic and real world data sets are provided to\ndemonstrate the effectiveness of our method.",
        "Task": "They tackle both regression (on synthetic data) and classification (on UCI Adult, CIFAR-10, and eICU) tasks under a federated or collaborative learning setting. The performance is evaluated by metrics such as MSE for regression and accuracy/AUC for classification.",
        "Data_type": "They handle structured tabular data (e.g., UCI Adult, eICU EHR) and image data (e.g., CIFAR10). They also use synthetic datasets to illustrate performance but do not address textual or multi-modal inputs.",
        "Data_domain": "They conduct experiments on synthetic data, UCI Adult (economic), CIFAR10 (academic image), and real-world electronic health record data from multiple hospitals (eICU). Thus, the paper spans both simulated and practical healthcare/economic domains.",
        "ML_phase": "They employ a hypernetwork-based multi-objective training procedure that generates and refines each client’s target model parameters through iterative gradient-based optimization. This workflow involves constructing a Pareto front on distributed data, then using Tarjan’s algorithm to identify stable collaborator sets and finalize personalized model partitions."
    },
    "kdd22/2209.01491v1.json": {
        "title": "Learning Differential Operators for Interpretable Time Series Modeling",
        "abs": "Modeling sequential patterns from data is at the core of various time series\nforecasting tasks. Deep learning models have greatly outperformed many\ntraditional models, but these black-box models generally lack explainability in\nprediction and decision making. To reveal the underlying trend with\nunderstandable mathematical expressions, scientists and economists tend to use\npartial differential equations (PDEs) to explain the highly nonlinear dynamics\nof sequential patterns. However, it usually requires domain expert knowledge\nand a series of simplified assumptions, which is not always practical and can\ndeviate from the ever-changing world. Is it possible to learn the differential\nrelations from data dynamically to explain the time-evolving dynamics? In this\nwork, we propose an learning framework that can automatically obtain\ninterpretable PDE models from sequential data. Particularly, this framework is\ncomprised of learnable differential blocks, named $P$-blocks, which is proved\nto be able to approximate any time-evolving complex continuous functions in\ntheory. Moreover, to capture the dynamics shift, this framework introduces a\nmeta-learning controller to dynamically optimize the hyper-parameters of a\nhybrid PDE model. Extensive experiments on times series forecasting of\nfinancial, engineering, and health data show that our model can provide\nvaluable interpretability and achieve comparable performance to\nstate-of-the-art models. From empirical studies, we find that learning a few\ndifferential operators may capture the major trend of sequential dynamics\nwithout massive computational complexity.",
        "Task": "They tackle a regression task for multivariate time series forecasting, using both synthetic and real-world datasets (Orderbook, MISO, PhysioNet). Their evaluation focuses on predicting future values (single-step or multi-step) to assess forecasting performance.",
        "Data_type": "They handle multivariate time series data, where each sample has k correlated variables and a time dimension. The paper’s experiments span smooth or irregular temporal sequences (e.g., asset prices, physiologic signals, electricity load), all treated as continuous or discretely sampled time series.",
        "Data_domain": "They examine synthetic wave-based data, high-frequency trading orderbook data (finance), real energy price and load data (energy), and ICU physiological measurements (healthcare). Across these sources, the paper covers economic, energy, and medical domains for time series analysis.",
        "ML_phase": "They construct a recurrent PDE-based approach in which a novel P-block (Polynomial-block) extracts differential operators via convolutional layers and element-wise multiplications, then uses sparse regression to ensure an interpretable solution. A meta-learning controller dynamically updates hyperparameters (e.g., time spans, sampling rates, PDE weights) during training to capture shifting sequential dynamics."
    },
    "kdd22/2207.05584v2.json": {
        "title": "Multi-Behavior Hypergraph-Enhanced Transformer for Sequential  Recommendation",
        "abs": "Learning dynamic user preference has become an increasingly important\ncomponent for many online platforms (e.g., video-sharing sites, e-commerce\nsystems) to make sequential recommendations. Previous works have made many\nefforts to model item-item transitions over user interaction sequences, based\non various architectures, e.g., recurrent neural networks and self-attention\nmechanism. Recently emerged graph neural networks also serve as useful backbone\nmodels to capture item dependencies in sequential recommendation scenarios.\nDespite their effectiveness, existing methods have far focused on item sequence\nrepresentation with singular type of interactions, and thus are limited to\ncapture dynamic heterogeneous relational structures between users and items\n(e.g., page view, add-to-favorite, purchase). To tackle this challenge, we\ndesign a Multi-Behavior Hypergraph-enhanced Transformer framework (MBHT) to\ncapture both short-term and long-term cross-type behavior dependencies.\nSpecifically, a multi-scale Transformer is equipped with low-rank\nself-attention to jointly encode behavior-aware sequential patterns from\nfine-grained and coarse-grained levels. Additionally, we incorporate the global\nmulti-behavior dependency into the hypergraph neural architecture to capture\nthe hierarchical long-range item correlations in a customized manner.\nExperimental results demonstrate the superiority of our MBHT over various\nstate-of-the-art recommendation solutions across different settings. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe new MBHT framework. Our implementation code is released at:\nhttps://github.com/yuh-yang/MBHT-KDD22.",
        "Task": "They address the next-item prediction task in a sequential recommendation setting. Performance is evaluated on real-world e-commerce datasets capturing multi-behavior user interactions.",
        "Data_type": "They handle sequential user–item interaction logs with multiple behavior types (e.g., page view, add-to-favorite, purchase), represented as item nodes and multi-behavior edges in a hypergraph. The node features are ID-based embeddings (no explicit text/image modalities), focusing on behavior-aware connections in a homogeneous graph structure.",
        "Data_domain": "These datasets are drawn from large-scale e-commerce platforms, specifically Taobao, Retailrocket, and IJCAI (Tmall), covering user behavioral logs like page view, add-to-favorite, add-to-cart, and purchase. They represent multi-typed user-item interaction data commonly found in online retail scenarios.",
        "ML_phase": "They process user interaction data by masking target behaviors, then construct a multi-scale Transformer for local sequence encoding and a hypergraph-based module for global multi-behavior dependency modeling. The entire framework is trained with a Cloze-style objective, integrating hypergraph message passing and multi-grained self-attention to predict the next-item."
    },
    "kdd22/2205.13947v2.json": {
        "title": "Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge  Transfer",
        "abs": "Spatio-temporal graph learning is a key method for urban computing tasks,\nsuch as traffic flow, taxi demand and air quality forecasting. Due to the high\ncost of data collection, some developing cities have few available data, which\nmakes it infeasible to train a well-performed model. To address this challenge,\ncross-city knowledge transfer has shown its promise, where the model learned\nfrom data-sufficient cities is leveraged to benefit the learning process of\ndata-scarce cities. However, the spatio-temporal graphs among different cities\nshow irregular structures and varied features, which limits the feasibility of\nexisting Few-Shot Learning (\\emph{FSL}) methods. Therefore, we propose a\nmodel-agnostic few-shot learning framework for spatio-temporal graph called\nST-GFSL. Specifically, to enhance feature extraction by transfering cross-city\nknowledge, ST-GFSL proposes to generate non-shared parameters based on\nnode-level meta knowledge. The nodes in target city transfer the knowledge via\nparameter matching, retrieving from similar spatio-temporal characteristics.\nFurthermore, we propose to reconstruct the graph structure during\nmeta-learning. The graph reconstruction loss is defined to guide\nstructure-aware learning, avoiding structure deviation among different\ndatasets. We conduct comprehensive experiments on four traffic speed prediction\nbenchmarks and the results demonstrate the effectiveness of ST-GFSL compared\nwith state-of-the-art methods.",
        "Task": "They address a spatio-temporal regression task on graph-structured data (traffic speed forecasting) under few-shot conditions. The approach is evaluated on four real-world traffic datasets (METR-LA, PEMS-BAY, Didi-Chengdu, Didi-Shenzhen).",
        "Data_type": "They handle numeric spatio-temporal graph data, where nodes are sensor/road segments with traffic speed features, and edges capture spatial connectivity. The node attributes are time-series signals (e.g., traffic speed), and the adjacency matrix encodes road-network structures.",
        "Data_domain": "The data is sourced from urban traffic networks, capturing spatio-temporal measurements of road sensors in multiple cities. Specifically, it includes traffic speed and flow data from four public datasets (METR-LA, PEMS-BAY, Didi-Chengdu, and Didi-Shenzhen).",
        "ML_phase": "They preprocess spatio-temporal data with Z-score normalization, build adjacency matrices from road network distances, and sample few-shot tasks for meta-learning. The authors then construct a model-agnostic meta-learning pipeline that generates non-shared feature-extractor parameters and incorporates a structure-aware training objective (graph reconstruction) to enable fast adaptation in data-scarce target cities."
    },
    "kdd22/2207.04049v1.json": {
        "title": "Learning Causal Effects on Hypergraphs",
        "abs": "Hypergraphs provide an effective abstraction for modeling multi-way group\ninteractions among nodes, where each hyperedge can connect any number of nodes.\nDifferent from most existing studies which leverage statistical dependencies,\nwe study hypergraphs from the perspective of causality. Specifically, in this\npaper, we focus on the problem of individual treatment effect (ITE) estimation\non hypergraphs, aiming to estimate how much an intervention (e.g., wearing face\ncovering) would causally affect an outcome (e.g., COVID-19 infection) of each\nindividual node. Existing works on ITE estimation either assume that the\noutcome on one individual should not be influenced by the treatment assignments\non other individuals (i.e., no interference), or assume the interference only\nexists between pairs of connected individuals in an ordinary graph. We argue\nthat these assumptions can be unrealistic on real-world hypergraphs, where\nhigher-order interference can affect the ultimate ITE estimations due to the\npresence of group interactions. In this work, we investigate high-order\ninterference modeling, and propose a new causality learning framework powered\nby hypergraph neural networks. Extensive experiments on real-world hypergraphs\nverify the superiority of our framework over existing baselines.",
        "Task": "They tackle an individual treatment effect estimation task (regression) on hypergraphs, where each node’s potential outcomes under different treatments must be predicted. The approach is evaluated on multiple real-world hypergraph datasets with observational data, focusing on node-level outcome predictions under high-order interference.",
        "Data_type": "They handle hypergraph-structured data with node-level features (e.g., textual or numeric attributes), where each hyperedge can connect an arbitrary number of nodes. Their experiments span real-world datasets (like GoodReads book descriptions or Microsoft Teams user attributes) encoding multi-way relations through these hyperedges.",
        "Data_domain": "The paper’s datasets span social and academic contexts, including a high school contact network, GoodReads (book reviews and authors), and Microsoft Teams (workplace communication). These data sources capture user-level attributes and multiway group interactions.",
        "ML_phase": "They build a pipeline where node features (possibly simulated) and hypergraph structures are used to learn confounder representations, then a hypergraph neural module propagates treatments across groups for interference modeling, and finally a prediction head estimates potential outcomes. The training framework includes representation balancing to minimize confounding biases, using hypergraph convolutions plus attention."
    },
    "kdd22/2112.11136v2.json": {
        "title": "Adversarial Gradient Driven Exploration for Deep Click-Through Rate  Prediction",
        "abs": "Exploration-Exploitation (E{\\&}E) algorithms are commonly adopted to deal\nwith the feedback-loop issue in large-scale online recommender systems. Most of\nexisting studies believe that high uncertainty can be a good indicator of\npotential reward, and thus primarily focus on the estimation of model\nuncertainty. We argue that such an approach overlooks the subsequent effect of\nexploration on model training. From the perspective of online learning, the\nadoption of an exploration strategy would also affect the collecting of\ntraining data, which further influences model learning. To understand the\ninteraction between exploration and training, we design a Pseudo-Exploration\nmodule that simulates the model updating process after a certain item is\nexplored and the corresponding feedback is received. We further show that such\na process is equivalent to adding an adversarial perturbation to the model\ninput, and thereby name our proposed approach as an the Adversarial Gradient\nDriven Exploration (AGE). For production deployment, we propose a dynamic\ngating unit to pre-determine the utility of an exploration. This enables us to\nutilize the limited amount of resources for exploration, and avoid wasting\npageview resources on ineffective exploration. The effectiveness of AGE was\nfirstly examined through an extensive number of ablation studies on an academic\ndataset. Meanwhile, AGE has also been deployed to one of the world-leading\ndisplay advertising platforms, and we observe significant improvements on\nvarious top-line evaluation metrics.",
        "Task": "They address a binary classification problem for click-through rate prediction in online recommendation. Performance is evaluated using the Yahoo! R6B dataset and large-scale online A/B tests.",
        "Data_type": "They handle large-scale user-item interaction logs with multi-hot or ID-based data features (e.g., user demographics, item/article identifiers) for CTR prediction. The data primarily consists of tabular attributes from Yahoo!’s R6B dataset and real-world e-commerce display advertising logs, each containing user clicks as binary labels.",
        "Data_domain": "The paper’s data comes from both a large-scale e-commerce advertising platform and the public Yahoo R6B dataset, capturing user visits, clicks, and impressions. The overall domain is online display advertising and recommendation.",
        "ML_phase": "They position their method in the ranking stage of an online CTR prediction pipeline, where impression data and user feedback are continuously collected for model updates. The system applies a pseudo-exploration module (adding adversarial gradients) and a dynamic gating unit before final ranking, enabling iterative training and long-term improvement in recommendation performance."
    },
    "kdd22/2206.11064v1.json": {
        "title": "An Embedded Feature Selection Framework for Control",
        "abs": "Reducing sensor requirements while keeping optimal control performance is\ncrucial to many industrial control applications to achieve robust, low-cost,\nand computation-efficient controllers. However, existing feature selection\nsolutions for the typical machine learning domain can hardly be applied in the\ndomain of control with changing dynamics. In this paper, a novel framework,\nnamely the Dual-world embedded Attentive Feature Selection (D-AFS), can\nefficiently select the most relevant sensors for the system under dynamic\ncontrol. Rather than the one world used in most Deep Reinforcement Learning\n(DRL) algorithms, D-AFS has both the real world and its virtual peer with\ntwisted features. By analyzing the DRL's response in two worlds, D-AFS can\nquantitatively identify respective features' importance towards control. A\nwell-known active flow control problem, cylinder drag reduction, is used for\nevaluation. Results show that D-AFS successfully finds an optimized five-probes\nlayout with 18.7\\% drag reduction than the state-of-the-art solution with 151\nprobes and 49.2\\% reduction than five-probes layout by human experts. We also\napply this solution to four OpenAI classical control cases. In all cases, D-AFS\nachieves the same or better sensor configurations than originally provided\nsolutions. Results highlight, we argued, a new way to achieve efficient and\noptimal sensor designs for experimental or industrial systems. Our source codes\nare made publicly available at https://github.com/G-AILab/DAFSFluid.",
        "Task": "They address an embedded feature selection (sensor selection) task in a reinforcement learning control setting, using environments such as a 2D flow control simulator with multiple sensor probes as the dataset. The goal is to identify minimal, high-impact sensor subsets that preserve or improve the policy’s performance.",
        "Data_type": "They primarily work with numeric time-series sensor data collected from a fluid simulation environment (e.g., velocity and pressure readings) and similarly structured real-valued observations in classical control tasks. No graph, textual, or multimodal image-text data types are involved.",
        "Data_domain": "These data come from a fluid dynamics domain, where sensor measurements (e.g., velocity, pressure) are collected in a 2D computational flow control environment. Specifically, they capture the behavior of fluid flow around a cylinder, with numerous probes placed to monitor the evolving flow field.",
        "ML_phase": "D-AFS embeds an attention-based sensor-selection mechanism into standard DRL pipelines, creating a “virtual world” input for the critic while the actor uses full sensor data. It integrates with PPO, DQN, and DDPG, training both the control policy and feature-weighting module in tandem via policy gradients and value function loss."
    },
    "kdd22/2207.10307v1.json": {
        "title": "Knowledge-enhanced Black-box Attacks for Recommendations",
        "abs": "Recent studies have shown that deep neural networks-based recommender systems\nare vulnerable to adversarial attacks, where attackers can inject carefully\ncrafted fake user profiles (i.e., a set of items that fake users have\ninteracted with) into a target recommender system to achieve malicious\npurposes, such as promote or demote a set of target items. Due to the security\nand privacy concerns, it is more practical to perform adversarial attacks under\nthe black-box setting, where the architecture/parameters and training data of\ntarget systems cannot be easily accessed by attackers. However, generating\nhigh-quality fake user profiles under black-box setting is rather challenging\nwith limited resources to target systems. To address this challenge, in this\nwork, we introduce a novel strategy by leveraging items' attribute information\n(i.e., items' knowledge graph), which can be publicly accessible and provide\nrich auxiliary knowledge to enhance the generation of fake user profiles. More\nspecifically, we propose a knowledge graph-enhanced black-box attacking\nframework (KGAttack) to effectively learn attacking policies through deep\nreinforcement learning techniques, in which knowledge graph is seamlessly\nintegrated into hierarchical policy networks to generate fake user profiles for\nperforming adversarial black-box attacks. Comprehensive experiments on various\nreal-world datasets demonstrate the effectiveness of the proposed attacking\nframework under the black-box setting.",
        "Task": "They address a black-box adversarial recommendation task that aims to promote a target item’s position in top-k rankings by injecting fake user profiles. The performance of these attacks is evaluated on real-world recommendation datasets (MovieLens-1M, Book-Crossing, and Last.FM).",
        "Data_type": "They handle knowledge-graph data with node textual attributes, representing items and item attributes as interconnected entities in a directed heterogeneous graph. Additionally, they incorporate user–item interaction data to support recommendation tasks.",
        "Data_domain": "They use real-world datasets from movie recommendations (MovieLens-1M), a book-crossing community (Book-Crossing), and an online music system (Last.FM), each linked with item-related knowledge from Microsoft Satori. Thus, the data domain spans multimedia recommendations and user-item interactions in entertainment and literature platforms.",
        "ML_phase": "They pre-train item embeddings with TransE, then refine them via GNN-based neighbor aggregation and leverage an RNN for state representation, forming the basis of a hierarchical RL pipeline. The framework uses two policy networks for anchor item selection and item picking, and is trained end-to-end with a PPO-based reinforcement learning scheme to generate and inject fake user profiles."
    },
    "kdd22/2206.13816v1.json": {
        "title": "Learning the Evolutionary and Multi-scale Graph Structure for  Multivariate Time Series Forecasting",
        "abs": "Recent studies have shown great promise in applying graph neural networks for\nmultivariate time series forecasting, where the interactions of time series are\ndescribed as a graph structure and the variables are represented as the graph\nnodes. Along this line, existing methods usually assume that the graph\nstructure (or the adjacency matrix), which determines the aggregation manner of\ngraph neural network, is fixed either by definition or self-learning. However,\nthe interactions of variables can be dynamic and evolutionary in real-world\nscenarios. Furthermore, the interactions of time series are quite different if\nthey are observed at different time scales. To equip the graph neural network\nwith a flexible and practical graph structure, in this paper, we investigate\nhow to model the evolutionary and multi-scale interactions of time series. In\nparticular, we first provide a hierarchical graph structure cooperated with the\ndilated convolution to capture the scale-specific correlations among time\nseries. Then, a series of adjacency matrices are constructed under a recurrent\nmanner to represent the evolving correlations at each layer. Moreover, a\nunified neural network is provided to integrate the components above to get the\nfinal prediction. In this way, we can capture the pair-wise correlations and\ntemporal dependency simultaneously. Finally, experiments on both single-step\nand multi-step forecasting tasks demonstrate the superiority of our method over\nthe state-of-the-art approaches.",
        "Task": "They address a multivariate time series forecasting (regression) task. They evaluate performance on real-world single-step (Solar-Energy, Electricity, Exchange Rate, Wind) and multi-step (NYC-Bike, NYC-Taxi) datasets.",
        "Data_type": "They handle numerical multivariate time-series data by representing each variable as a node in a graph with evolving adjacency matrices. No textual or multi-modal information is involved, as the node features exclusively consist of time-series signals.",
        "Data_domain": "The datasets come from real-world energy (solar power, electricity, wind), finance (exchange rates), and transportation (NYC bike and taxi) domains. Thus, the data domain spans multiple sectors involving both municipal services (transportation) and broader economic/energy contexts.",
        "ML_phase": "They devise a hierarchical pipeline where raw time series data first pass through a fully connected layer and multi-scale dilated convolution modules for feature extraction, then an evolving graph structure learner (gated recurrent framework) generates time-varying adjacency matrices at each scale, which feed into graph convolutions. The entire process, including the evolving graph learner and multi-scale layers, is trained end-to-end to output the final forecasts."
    },
    "kdd22/2206.12401v2.json": {
        "title": "Debiasing Learning for Membership Inference Attacks Against Recommender  Systems",
        "abs": "Learned recommender systems may inadvertently leak information about their\ntraining data, leading to privacy violations. We investigate privacy threats\nfaced by recommender systems through the lens of membership inference. In such\nattacks, an adversary aims to infer whether a user's data is used to train the\ntarget recommender. To achieve this, previous work has used a shadow\nrecommender to derive training data for the attack model, and then predicts the\nmembership by calculating difference vectors between users' historical\ninteractions and recommended items. State-of-the-art methods face two\nchallenging problems: (1) training data for the attack model is biased due to\nthe gap between shadow and target recommenders, and (2) hidden states in\nrecommenders are not observational, resulting in inaccurate estimations of\ndifference vectors. To address the above limitations, we propose a Debiasing\nLearning for Membership Inference Attacks against recommender systems (DL-MIA)\nframework that has four main components: (1) a difference vector generator, (2)\na disentangled encoder, (3) a weight estimator, and (4) an attack model. To\nmitigate the gap between recommenders, a variational auto-encoder (VAE) based\ndisentangled encoder is devised to identify recommender invariant and specific\nfeatures. To reduce the estimation bias, we design a weight estimator,\nassigning a truth-level score for each difference vector to indicate estimation\naccuracy. We evaluate DL-MIA against both general recommenders and sequential\nrecommenders on three real-world datasets. Experimental results show that\nDL-MIA effectively alleviates training and estimation biases simultaneously,\nand achieves state-of-the-art attack performance.",
        "Task": "They study membership inference attacks against recommender systems as a binary classification task (member vs. non-member). The evaluation is conducted on MovieLens and Amazon datasets.",
        "Data_type": "They handle user-item rating interactions (tabular data) from large-scale public datasets and can incorporate textual item descriptions for embedding-based features. No graph-structured or SMILES/fingerprint inputs are used, focusing instead on numeric ratings and optional textual data.",
        "Data_domain": "The data come from two main domains: large-scale e-commerce product reviews (Amazon Digital Music/Beauty) and user-movie rating histories (MovieLens). These sources provide real-world user-item interaction records for evaluating membership inference attacks on recommender systems.",
        "ML_phase": "They first generate difference vectors by factorizing user-item matrices, then use a VAE-based encoder to disentangle invariant and specific features and a weight estimator to assign truth-level scores. Finally, an MLP attack model is trained via an alternating scheme that integrates BCE, ELBO, and reweighted losses."
    },
    "kdd22/2210.05959v1.json": {
        "title": "JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional  Networks",
        "abs": "Graph Convolutional Network (GCN) has exhibited strong empirical performance\nin many real-world applications. The vast majority of existing works on GCN\nprimarily focus on the accuracy while ignoring how confident or uncertain a GCN\nis with respect to its predictions. Despite being a cornerstone of trustworthy\ngraph mining, uncertainty quantification on GCN has not been well studied and\nthe scarce existing efforts either fail to provide deterministic quantification\nor have to change the training procedure of GCN by introducing additional\nparameters or architectures. In this paper, we propose the first\nfrequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,\nwhere the key idea is to quantify the uncertainty of a node as the width of\nconfidence interval by a jackknife estimator. Moreover, we leverage the\ninfluence functions to estimate the change in GCN parameters without\nre-training to scale up the computation. The proposed JuryGCN is capable of\nquantifying uncertainty deterministically without modifying the GCN\narchitecture or introducing additional parameters. We perform extensive\nexperimental evaluation on real-world datasets in the tasks of both active\nlearning and semi-supervised node classification, which demonstrate the\nefficacy of the proposed method.",
        "Task": "They focus on node-level classification tasks, specifically investigating semi-supervised and active learning scenarios. Their experiments evaluate performance on standard benchmark graph datasets (Cora, Citeseer, Pubmed) and a large social network dataset (Reddit).",
        "Data_type": "They address homogeneous graph-structured data where each node is enriched with textual attributes. Empirical evaluations focus on standard citation networks (Cora, Citeseer, Pubmed) and a large-scale social platform graph (Reddit).",
        "Data_domain": "These experiments use node-level data primarily drawn from academic citation networks (Cora, Citeseer, Pubmed) and a large-scale social network (Reddit). Additional references include financial datasets for fraud detection and other real-world graph scenarios such as traffic systems and drug discovery.",
        "ML_phase": "They introduce a post-hoc step in the ML pipeline after the GCN is trained, using influence functions to approximate leave-one-out parameters and construct node-level confidence intervals. This procedure does not alter the original training process and can directly plug into downstream tasks such as active learning or semi-supervised node classification."
    },
    "kdd22/2206.01909v1.json": {
        "title": "Toward Learning Robust and Invariant Representations with Alignment  Regularization and Data Augmentation",
        "abs": "Data augmentation has been proven to be an effective technique for developing\nmachine learning models that are robust to known classes of distributional\nshifts (e.g., rotations of images), and alignment regularization is a technique\noften used together with data augmentation to further help the model learn\nrepresentations invariant to the shifts used to augment the data. In this\npaper, motivated by a proliferation of options of alignment regularizations, we\nseek to evaluate the performances of several popular design choices along the\ndimensions of robustness and invariance, for which we introduce a new test\nprocedure. Our synthetic experiment results speak to the benefits of squared l2\nnorm regularization. Further, we also formally analyze the behavior of\nalignment regularization to complement our empirical study under assumptions we\nconsider realistic. Finally, we test this simple technique we identify\n(worst-case data augmentation with squared l2 norm alignment regularization)\nand show that the benefits of this method outrun those of the specially\ndesigned methods. We also release a software package in both TensorFlow and\nPyTorch for users to use the method with a couple of lines at\nhttps://github.com/jyanln/AlignReg.",
        "Task": "They address image classification. They evaluate models under various transformations to assess accuracy, robustness, and invariance.",
        "Data_type": "They focus on image data from standard classification benchmarks (e.g., MNIST, CIFAR10, ImageNet). Their methods involve various image transformations (texture, rotation, and contrast) for data augmentation.",
        "Data_domain": "The dataset is drawn from common computer vision benchmarks, specifically MNIST, CIFAR10, and ImageNet. These encompass handwritten digits, natural objects, and large-scale image categories.",
        "ML_phase": "They generate additional training data by applying label-preserving transformations (e.g., rotations or contrast adjustments) to existing samples. In the training framework, they add an alignment regularization term on the logits between original and augmented samples to enforce robustness and invariance."
    },
    "kdd22/2206.12104v1.json": {
        "title": "On Structural Explanation of Bias in Graph Neural Networks",
        "abs": "Graph Neural Networks (GNNs) have shown satisfying performance in various\ngraph analytical problems. Hence, they have become the \\emph{de facto} solution\nin a variety of decision-making scenarios. However, GNNs could yield biased\nresults against certain demographic subgroups. Some recent works have\nempirically shown that the biased structure of the input network is a\nsignificant source of bias for GNNs. Nevertheless, no studies have\nsystematically scrutinized which part of the input network structure leads to\nbiased predictions for any given node. The low transparency on how the\nstructure of the input network influences the bias in GNN outcome largely\nlimits the safe adoption of GNNs in various decision-critical scenarios. In\nthis paper, we study a novel research problem of structural explanation of bias\nin GNNs. Specifically, we propose a novel post-hoc explanation framework to\nidentify two edge sets that can maximally account for the exhibited bias and\nmaximally contribute to the fairness level of the GNN prediction for any given\nnode, respectively. Such explanations not only provide a comprehensive\nunderstanding of bias/fairness of GNN predictions but also have practical\nsignificance in building an effective yet fair GNN model. Extensive experiments\non real-world datasets validate the effectiveness of the proposed framework\ntowards delivering effective structural explanations for the bias of GNNs.\nOpen-source code can be found at https://github.com/yushundong/REFEREE.",
        "Task": "They address a node‐level classification task on real‐world attributed networks, such as predicting credit risk or bail decisions. Performance is evaluated using multiple datasets (German Credit, Recidivism, Credit Defaulter).",
        "Data_type": "They handle homogeneous graph-structured data with node attributes (e.g., numeric or textual) and edges that define the computation graph. The framework focuses on node-level information within such attributed networks.",
        "Data_domain": "The paper’s datasets come from financial and judicial contexts, covering bank clients’ credit risk, credit card default, and bail decisions. Hence, the data domain is primarily economic and legal.",
        "ML_phase": "This work operates post-training, where the framework is applied after a GNN has been trained on graph data. It introduces two contrastive explainer modules that learn edge importance scores through a specialized objective, ensuring fidelity to the original predictions while targeting bias- or fairness-related factors."
    },
    "kdd22/2106.00761v4.json": {
        "title": "Motif Prediction with Graph Neural Networks",
        "abs": "Link prediction is one of the central problems in graph mining. However,\nrecent studies highlight the importance of higher-order network analysis, where\ncomplex structures called motifs are the first-class citizens. We first show\nthat existing link prediction schemes fail to effectively predict motifs. To\nalleviate this, we establish a general motif prediction problem and we propose\nseveral heuristics that assess the chances for a specified motif to appear. To\nmake the scores realistic, our heuristics consider - among others -\ncorrelations between links, i.e., the potential impact of some arriving links\non the appearance of other links in a given motif. Finally, for highest\naccuracy, we develop a graph neural network (GNN) architecture for motif\nprediction. Our architecture offers vertex features and sampling schemes that\ncapture the rich structural properties of motifs. While our heuristics are fast\nand do not need any training, GNNs ensure highest accuracy of predicting\nmotifs, both for dense (e.g., k-cliques) and for sparse ones (e.g., k-stars).\nWe consistently outperform the best available competitor by more than 10% on\naverage and up to 32% in area under the curve. Importantly, the advantages of\nour approach over schemes based on uncorrelated link prediction increase with\nthe increasing motif size and complexity. We also successfully apply our\narchitecture for predicting more arbitrary clusters and communities,\nillustrating its potential for graph mining beyond motif analysis.",
        "Task": "They address a motif prediction task, which can be viewed as subgraph-level classification to determine whether a set of vertices will form a specified pattern. They validate performance on various graph datasets, measuring the likelihood that the motif appears.",
        "Data_type": "They operate on graph-structured data (undirected, homogeneous graphs) with node and edge information represented via adjacency matrices (A) and optionally node feature matrices (X). No multi-modal (e.g., image-text) or specialized molecular (e.g., SMILES/fingerprint) formats are discussed.",
        "Data_domain": "The paper utilizes datasets from real-world graph domains, including social networks, biological networks, brain networks, and air traffic networks. These sources span computational biology, chemistry, and various other fields where motifs and higher-order structures play critical roles.",
        "ML_phase": "They introduce a GNN-based method (SEAM) that extends the SEAL framework by extracting subgraphs around candidate motifs (including negative samples) to capture correlations among edges. Training involves learning from these local subgraphs to predict the appearance of higher-order motifs, with inference then operating on similarly extracted subgraphs in the pipeline."
    },
    "kdd22/2205.10798v2.json": {
        "title": "PAC-Wrap: Semi-Supervised PAC Anomaly Detection",
        "abs": "Anomaly detection is essential for preventing hazardous outcomes for\nsafety-critical applications like autonomous driving. Given their\nsafety-criticality, these applications benefit from provable bounds on various\nerrors in anomaly detection. To achieve this goal in the semi-supervised\nsetting, we propose to provide Probably Approximately Correct (PAC) guarantees\non the false negative and false positive detection rates for anomaly detection\nalgorithms. Our method (PAC-Wrap) can wrap around virtually any existing\nsemi-supervised and unsupervised anomaly detection method, endowing it with\nrigorous guarantees. Our experiments with various anomaly detectors and\ndatasets indicate that PAC-Wrap is broadly effective.",
        "Task": "They address a binary anomaly-versus-normal classification task with PAC-style guarantees on false positives and false negatives. They evaluate performance on datasets including Thyroid, NASA telemetry data, and SMD, measuring error rates under controlled constraints.",
        "Data_type": "They handle numeric tabular data from real (thyroid) and synthetic distributions, as well as numeric time-series data from NASA and SMD. No multi-modal, textual, or graph-structured inputs are used, focusing strictly on numeric features for anomaly detection.",
        "Data_domain": "The paper’s datasets originate from multiple domains, including a medical thyroid dataset, server machine logs (SMD) from a large Internet company, NASA telemetry data, and synthetic distributions. These sources encompass both real-world and artificially generated anomaly detection scenarios.",
        "ML_phase": "They split data into training, calibration, and testing sets (potentially sampled from different distributions), then use the calibration set to adjust threshold-based anomaly predictors under PAC constraints. The pipeline includes bounding false positive/negative rates by solving an optimization problem on the calibration set and evaluating the final thresholds on real or synthetic test data."
    },
    "kdd22/2206.09113v2.json": {
        "title": "Pre-training Enhanced Spatial-temporal Graph Neural Network for  Multivariate Time Series Forecasting",
        "abs": "Multivariate Time Series (MTS) forecasting plays a vital role in a wide range\nof applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have\nbecome increasingly popular MTS forecasting methods. STGNNs jointly model the\nspatial and temporal patterns of MTS through graph neural networks and\nsequential models, significantly improving the prediction accuracy. But limited\nby model complexity, most STGNNs only consider short-term historical MTS data,\nsuch as data over the past one hour. However, the patterns of time series and\nthe dependencies between them (i.e., the temporal and spatial patterns) need to\nbe analyzed based on long-term historical MTS data. To address this issue, we\npropose a novel framework, in which STGNN is Enhanced by a scalable time series\nPre-training model (STEP). Specifically, we design a pre-training model to\nefficiently learn temporal patterns from very long-term history time series\n(e.g., the past two weeks) and generate segment-level representations. These\nrepresentations provide contextual information for short-term time series input\nto STGNNs and facilitate modeling dependencies between time series. Experiments\non three public real-world datasets demonstrate that our framework is capable\nof significantly enhancing downstream STGNNs, and our pre-training model aptly\ncaptures temporal patterns.",
        "Task": "They address a multivariate time series forecasting task, predicting future values based on historical signals. Performance is evaluated on three real-world traffic datasets (METR-LA, PEMS-BAY, and PEMS04).",
        "Data_type": "They handle multivariate time series in a spatial-temporal graph form, where each of the N sensors (nodes) yields time series data of length T and C channels, producing a tensor of shape T×N×C. The adjacency matrix encodes edges among these sensor nodes, capturing the non-Euclidean dependencies for forecasting.",
        "Data_domain": "The data in this paper originates from real-world traffic systems, specifically sensor networks monitoring road networks (e.g., in Los Angeles County or the Bay Area). It focuses on transportation data captured at regular intervals, recording traffic speed/flow for multivariate time series forecasting.",
        "ML_phase": "They adopt a two-stage pipeline: first, a masked autoencoding pre-training phase on long time series (TSFormer) to learn segment-level representations, then a downstream forecasting phase where these representations enhance the STGNN. Graph structure learning is integrated into the training loop, sampling a discrete adjacency matrix based on the pre-trained embeddings."
    },
    "kdd22/2206.04153v2.json": {
        "title": "Unsupervised Key Event Detection from Massive Text Corpora",
        "abs": "Automated event detection from news corpora is a crucial task towards mining\nfast-evolving structured knowledge. As real-world events have different\ngranularities, from the top-level themes to key events and then to event\nmentions corresponding to concrete actions, there are generally two lines of\nresearch: (1) theme detection identifies from a news corpus major themes (e.g.,\n\"2019 Hong Kong Protests\" vs. \"2020 U.S. Presidential Election\") that have very\ndistinct semantics; and (2) action extraction extracts from one document\nmention-level actions (e.g., \"the police hit the left arm of the protester\")\nthat are too fine-grained for comprehending the event. In this paper, we\npropose a new task, key event detection at the intermediate level, aiming to\ndetect from a news corpus key events (e.g., \"HK Airport Protest on Aug.\n12-14\"), each happening at a particular time/location and focusing on the same\ntopic. This task can bridge event understanding and structuring and is\ninherently challenging because of the thematic and temporal closeness of key\nevents and the scarcity of labeled data due to the fast-evolving nature of news\narticles. To address these challenges, we develop an unsupervised key event\ndetection framework, EvMine, that (1) extracts temporally frequent peak phrases\nusing a novel ttf-itf score, (2) merges peak phrases into event-indicative\nfeature sets by detecting communities from our designed peak phrase graph that\ncaptures document co-occurrences, semantic similarities, and temporal closeness\nsignals, and (3) iteratively retrieves documents related to each key event by\ntraining a classifier with automatically generated pseudo labels from the\nevent-indicative feature sets and refining the detected key events using the\nretrieved documents. Extensive experiments and case studies show EvMine\noutperforms all the baseline methods and its ablations on two real-world news\ncorpora.",
        "Task": "They introduce an unsupervised key event detection task to cluster thematically and temporally coherent documents into distinct real-world events from a theme-specific news corpus. They evaluate performance on two labeled news corpora (Hong Kong Protests and Ebola) by measuring how well the discovered clusters match known key events.",
        "Data_type": "They use purely text-based data from news articles that is transformed into a homogeneous graph, where each node represents a “peak phrase” (i.e., textual phrase) and edges capture co-occurrence or semantic similarity. No multi-modal or molecule-related (e.g., SMILES/fingerprint) data is involved, and all analysis revolves around textual node attributes in this graph structure.",
        "Data_domain": "The corpus comes from mainstream news outlets, consisting of time-stamped articles covering real-world events. Specifically, the datasets include two corpora focused on the 2019 Hong Kong Protests and the 2014 Ebola Outbreak.",
        "ML_phase": "They first perform unsupervised phrase mining and filtering to obtain candidate phrases, then apply a novel ttf-itf measure to detect temporally salient “peak phrases.” Next, they cluster these phrases via a topic-time integrated graph and iteratively train binary classifiers with pseudo-labels to refine document-level key events without human annotations."
    },
    "kdd22/2206.09363v1.json": {
        "title": "Towards Unified Conversational Recommender Systems via  Knowledge-Enhanced Prompt Learning",
        "abs": "Conversational recommender systems (CRS) aim to proactively elicit user\npreference and recommend high-quality items through natural language\nconversations. Typically, a CRS consists of a recommendation module to predict\npreferred items for users and a conversation module to generate appropriate\nresponses. To develop an effective CRS, it is essential to seamlessly integrate\nthe two modules. Existing works either design semantic alignment strategies, or\nshare knowledge resources and representations between the two modules. However,\nthese approaches still rely on different architectures or techniques to develop\nthe two modules, making it difficult for effective module integration.\n  To address this problem, we propose a unified CRS model named UniCRS based on\nknowledge-enhanced prompt learning. Our approach unifies the recommendation and\nconversation subtasks into the prompt learning paradigm, and utilizes\nknowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to\nfulfill both subtasks in a unified approach. In the prompt design, we include\nfused knowledge representations, task-specific soft tokens, and the dialogue\ncontext, which can provide sufficient contextual information to adapt the PLM\nfor the CRS task. Besides, for the recommendation subtask, we also incorporate\nthe generated response template as an important part of the prompt, to enhance\nthe information interaction between the two subtasks. Extensive experiments on\ntwo public CRS datasets have demonstrated the effectiveness of our approach.",
        "Task": "They present a unified framework to jointly perform item recommendation and natural language response generation (two distinct tasks) in a conversational setting. Evaluation is conducted on the ReDial and INSPIRED datasets using metrics specific to recommendation accuracy and response quality.",
        "Data_type": "They handle textual dialogues in which user utterances are mapped to knowledge-graph entities (node-level information) for recommendation. They thus integrate text-based data (words) with graph-structured data (entities and relations) from an external KG to capture semantic associations.",
        "Data_domain": "They use conversational data centered on movie recommendations, collected from platforms like Amazon Mechanical Turk and Reddit. All datasets involve user–system interactions discussing and suggesting films, placing the work squarely in the entertainment domain.",
        "ML_phase": "They first pre-train a semantic fusion module to align textual and entity embeddings, then prompt a frozen PLM (DialoGPT) for both recommendation and conversation subtasks. The training pipeline fixes the PLM parameters while tuning subtask-specific prompt tokens and leveraging a knowledge-enhanced prompt design for generating unified outputs."
    },
    "kdd22/2109.06429v2.json": {
        "title": "Robust Inverse Framework using Knowledge-guided Self-Supervised  Learning: An application to Hydrology",
        "abs": "Machine Learning is beginning to provide state-of-the-art performance in a\nrange of environmental applications such as streamflow prediction in a\nhydrologic basin. However, building accurate broad-scale models for streamflow\nremains challenging in practice due to the variability in the dominant\nhydrologic processes, which are best captured by sets of process-related basin\ncharacteristics. Existing basin characteristics suffer from noise and\nuncertainty, among many other things, which adversely impact model performance.\nTo tackle the above challenges, in this paper, we propose a novel\nKnowledge-guided Self-Supervised Learning (KGSSL) inverse framework to extract\nsystem characteristics from driver and response data. This first-of-its-kind\nframework achieves robust performance even when characteristics are corrupted.\nWe show that KGSSL achieves state-of-the-art results for streamflow modeling\nfor CAMELS (Catchment Attributes and MEteorology for Large-sample Studies)\nwhich is a widely used hydrology benchmark dataset. Specifically, KGSSL\noutperforms other methods by up to 16 \\% in reconstructing characteristics.\nFurthermore, we show that KGSSL is relatively more robust to distortion than\nbaseline methods, and outperforms the baseline model by 35\\% when plugging in\nKGSSL inferred characteristics.",
        "Task": "They formulate a regression task to infer static basin/catchment characteristics (real-valued attributes) from time-varying driver-response data. They further validate performance on imputation under missing and corrupted attributes.",
        "Data_type": "They handle multivariate time‐series (meteorological drivers and streamflow values) alongside numeric static attributes (catchment characteristics). The data are purely numeric/tabular and do not include graph-structured, image, or text modalities.",
        "Data_domain": "The paper leverages hydrological data, specifically daily meteorological drivers and streamflow observations from the CAMELS dataset. This domain captures basin-scale physical processes, where static basin characteristics (e.g., soil or geomorphology) interact with weather drivers to determine streamflow.",
        "ML_phase": "They split the time-varying driver-response data into windows and feed them into a bidirectional LSTM encoder-decoder architecture for embedding generation and sequence reconstruction. The model is trained using a combination of reconstruction, contrastive, and pseudo-inverse losses, enabling robust static characteristic inference under noisy or missing data."
    },
    "kdd22/2206.05682v1.json": {
        "title": "Balancing Bias and Variance for Active Weakly Supervised Learning",
        "abs": "As a widely used weakly supervised learning scheme, modern multiple instance\nlearning (MIL) models achieve competitive performance at the bag level.\nHowever, instance-level prediction, which is essential for many important\napplications, remains largely unsatisfactory. We propose to conduct novel\nactive deep multiple instance learning that samples a small subset of\ninformative instances for annotation, aiming to significantly boost the\ninstance-level prediction. A variance regularized loss function is designed to\nproperly balance the bias and variance of instance-level predictions, aiming to\neffectively accommodate the highly imbalanced instance distribution in MIL and\nother fundamental challenges. Instead of directly minimizing the variance\nregularized loss that is non-convex, we optimize a distributionally robust bag\nlevel likelihood as its convex surrogate. The robust bag likelihood provides a\ngood approximation of the variance based MIL loss with a strong theoretical\nguarantee. It also automatically balances bias and variance, making it\neffective to identify the potentially positive instances to support active\nsampling. The robust bag likelihood can be naturally integrated with a deep\narchitecture to support deep model training using mini-batches of\npositive-negative bag pairs. Finally, a novel P-F sampling function is\ndeveloped that combines a probability vector and predicted instance scores,\nobtained by optimizing the robust bag likelihood. By leveraging the key MIL\nassumption, the sampling function can explore the most challenging bags and\neffectively detect their positive instances for annotation, which significantly\nimproves the instance-level prediction. Experiments conducted over multiple\nreal-world datasets clearly demonstrate the state-of-the-art instance-level\nprediction achieved by the proposed model.",
        "Task": "They aim to perform instance-level classification within a Multiple Instance Learning setting, where each bag has a binary label and the goal is to identify positive instances with minimal annotation. The authors evaluate this classification task on textual (20NewsGroup) and image-based (Cifar10, Cifar100, Pascal VOC) datasets.",
        "Data_type": "They handle textual data (20NewsGroup) and image data (Cifar10, Cifar100, Pascal VOC) in a multiple instance learning context. No graph-structured or SMILES/fingerprint data are mentioned.",
        "Data_domain": "These experiments use textual data from the 20NewsGroup corpus and high-dimensional image data from CIFAR10, CIFAR100, and Pascal VOC. Thus, the datasets span both text-based topics and diverse visual object categories.",
        "ML_phase": "They develop a distributionally robust MIL framework that integrates an active sampling strategy into the training loop for instance-level supervision. The approach uses a deep architecture trained in mini-batches with a hybrid loss (bag-level plus instance-level), ensuring both robust feature learning and adaptive instance annotation."
    },
    "kdd22/2206.04516v2.json": {
        "title": "Accurate Node Feature Estimation with Structured Variational Graph  Autoencoder",
        "abs": "Given a graph with partial observations of node features, how can we estimate\nthe missing features accurately? Feature estimation is a crucial problem for\nanalyzing real-world graphs whose features are commonly missing during the data\ncollection process. Accurate estimation not only provides diverse information\nof nodes but also supports the inference of graph neural networks that require\nthe full observation of node features. However, designing an effective approach\nfor estimating high-dimensional features is challenging, since it requires an\nestimator to have large representation power, increasing the risk of\noverfitting. In this work, we propose SVGA (Structured Variational Graph\nAutoencoder), an accurate method for feature estimation. SVGA applies strong\nregularization to the distribution of latent variables by structured\nvariational inference, which models the prior of variables as Gaussian Markov\nrandom field based on the graph structure. As a result, SVGA combines the\nadvantages of probabilistic inference and graph neural networks, achieving\nstate-of-the-art performance in real datasets.",
        "Task": "They address the task of missing node feature estimation on eight real-world graph datasets, measuring performance with metrics such as recall, nDCG, RMSE, and correlation. Additionally, they evaluate how well the reconstructed features support node classification accuracy on the same datasets.",
        "Data_type": "They consider homogeneous graph-structured data with node-level binary or continuous features, where each feature vector can reach thousands of dimensions. The graph’s adjacency encodes edges, and some nodes may also have optional label information for additional supervision.",
        "Data_domain": "They study graph-structured data spanning social, e-commerce, streaming-service, and academic citation networks, where node features are partially observed. These real-world graphs model relationships among users, items, or documents in sparse, high-dimensional settings.",
        "ML_phase": "They build a GNN-based encoder (using identity node features) coupled with linear decoders for features and labels, and then train it end-to-end via a structured variational inference framework. The method regularizes latent representations with a GMRF prior and further stabilizes training through deterministic modeling, ensuring robust feature reconstruction."
    },
    "kdd22/2112.07030v4.json": {
        "title": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
        "abs": "We study a variant of classical clustering formulations in the context of\nalgorithmic fairness, known as diversity-aware clustering. In this variant we\nare given a collection of facility subsets, and a solution must contain at\nleast a specified number of facilities from each subset while simultaneously\nminimizing the clustering objective ($k$-median or $k$-means). We investigate\nthe fixed-parameter tractability of these problems and show several negative\nhardness and inapproximability results, even when we afford exponential running\ntime with respect to some parameters.\n  Motivated by these results we identify natural parameters of the problem, and\npresent fixed-parameter approximation algorithms with approximation ratios\n$\\big(1 + \\frac{2}{e} +\\epsilon \\big)$ and $\\big(1 + \\frac{8}{e}+ \\epsilon\n\\big)$ for diversity-aware $k$-median and diversity-aware $k$-means\nrespectively, and argue that these ratios are essentially tight assuming the\ngap-exponential time hypothesis. We also present a simple and more practical\nbicriteria approximation algorithm with better running time bounds. We finally\npropose efficient and practical heuristics. We evaluate the scalability and\neffectiveness of our methods in a wide variety of rigorously conducted\nexperiments, on both real and synthetic data.",
        "Task": "They address a clustering task, specifically a variant of the k-median or k-means problem called “diversity-aware k-median,” which imposes minimum representation requirements on the chosen centers. The performance is evaluated on real and synthetic datasets.",
        "Data_type": "They focus on data points in a metric space (often numeric vectors), with additional group-based attributes but no explicit multi-modal or graph-structured representation. Hence, the methodology is designed for classical numeric (potentially categorical-converted) datasets without specialized handling of text, images, or molecular fingerprints.",
        "Data_domain": "They work with both real datasets from the UCI Machine Learning Repository and synthetic datasets generated using standard tools (e.g., scikit-learn). Hence, the data domain is largely academic with controlled simulation components.",
        "ML_phase": "These methods occur in the model-building phase of the ML pipeline, where the paper proposes specialized clustering approaches incorporating coreset construction, constraint satisfaction enumeration, and dynamic/linear programming-based heuristics. After data collection and basic preprocessing, the algorithm is trained (via local search or approximate methods) to select committee members (facilities) that satisfy diversity constraints."
    },
    "kdd22/2205.09721v1.json": {
        "title": "HyperAid: Denoising in hyperbolic spaces for tree-fitting and  hierarchical clustering",
        "abs": "The problem of fitting distances by tree-metrics has received significant\nattention in the theoretical computer science and machine learning communities\nalike, due to many applications in natural language processing, phylogeny,\ncancer genomics and a myriad of problem areas that involve hierarchical\nclustering. Despite the existence of several provably exact algorithms for\ntree-metric fitting of data that inherently obeys tree-metric constraints, much\nless is known about how to best fit tree-metrics for data whose structure\nmoderately (or substantially) differs from a tree. For such noisy data, most\navailable algorithms perform poorly and often produce negative edge weights in\nrepresentative trees. Furthermore, it is currently not known how to choose the\nmost suitable approximation objective for noisy fitting. Our contributions are\nas follows. First, we propose a new approach to tree-metric denoising\n(HyperAid) in hyperbolic spaces which transforms the original data into data\nthat is ``more'' tree-like, when evaluated in terms of Gromov's $\\delta$\nhyperbolicity. Second, we perform an ablation study involving two choices for\nthe approximation objective, $\\ell_p$ norms and the Dasgupta loss. Third, we\nintegrate HyperAid with schemes for enforcing nonnegative edge-weights. As a\nresult, the HyperAid platform outperforms all other existing methods in the\nliterature, including Neighbor Joining (NJ), TreeRep and T-REX, both on\nsynthetic and real-world data. Synthetic data is represented by edge-augmented\ntrees and shortest-distance metrics while the real-world datasets include Zoo,\nIris, Glass, Segmentation and SpamBase; on these datasets, the average\nimprovement with respect to NJ is $125.94\\%$.",
        "Task": "They address an unsupervised learning problem of fitting tree metrics and performing hierarchical clustering on real-world datasets. The goal is to produce accurate tree-based partitions by reducing noise in the input distances.",
        "Data_type": "They operate on datasets that can be represented via pairwise distance (or dissimilarity) measures, often derived from numeric features (e.g., UCI repository data) or synthetic graphs/trees. They do not specifically address image-text multi-modal inputs, SMILES strings, or textual node attributes, focusing instead on generic numeric inputs transformable into distance matrices.",
        "Data_domain": "They evaluate both synthetic datasets generated by injecting noise into random trees and real-world datasets from the UCI Machine Learning repository (e.g., Zoo, Iris, Glass, Segmentation, Spambase). This covers domains ranging from biological data (phylogenetics) to general classification tasks.",
        "ML_phase": "They first embed the noisy pairwise distances into hyperbolic space using a gradient-based (Riemannian) optimization procedure to reduce hyperbolicity distortion. Then, the resulting denoised distances are decoded into tree or ultrametric structures (e.g., via Neighbor Joining, TreeRep, or linkage) for hierarchical clustering or metric-tree fitting."
    },
    "kdd22/2207.11122v1.json": {
        "title": "Solving the Batch Stochastic Bin Packing Problem in Cloud: A  Chance-constrained Optimization Approach",
        "abs": "This paper investigates a critical resource allocation problem in the first\nparty cloud: scheduling containers to machines. There are tens of services and\neach service runs a set of homogeneous containers with dynamic resource usage;\ncontainers of a service are scheduled daily in a batch fashion. This problem\ncan be naturally formulated as Stochastic Bin Packing Problem (SBPP). However,\ntraditional SBPP research often focuses on cases of empty machines, whose\nobjective, i.e., to minimize the number of used machines, is not well-defined\nfor the more common reality with nonempty machines. This paper aims to close\nthis gap. First, we define a new objective metric, Used Capacity at Confidence\n(UCaC), which measures the maximum used resources at a probability and is\nproved to be consistent for both empty and nonempty machines, and reformulate\nthe SBPP under chance constraints. Second, by modeling the container resource\nusage distribution in a generative approach, we reveal that UCaC can be\napproximated with Gaussian, which is verified by trace data of real-world\napplications. Third, we propose an exact solver by solving the equivalent\ncutting stock variant as well as two heuristics-based solvers -- UCaC best fit,\nbi-level heuristics. We experimentally evaluate these solvers on both synthetic\ndatasets and real application traces, demonstrating our methodology's advantage\nover traditional SBPP optimal solver minimizing the number of used machines,\nwith a low rate of resource violations.",
        "Task": "They formulate and solve a resource scheduling task as a Stochastic Bin Packing Problem that optimizes container placements under probabilistic constraints. Performance is evaluated on both synthetic datasets and real container usage traces, measuring resource utilization and violation rates.",
        "Data_type": "They handle numeric data of container CPU usage in a cloud cluster, typically modeled as Gaussian to capture resource demand variability. Memory and storage are also recognized as deterministic constraints, but CPU usage distributions form the core focus.",
        "Data_domain": "All data is drawn from a commercial cloud environment hosting web application services. It comprises container usage traces collected from real clusters under industrial-scale workloads.",
        "ML_phase": "They first perform data analysis on real container usage to validate a Gaussian generative assumption, then translate this into a stochastic bin packing formulation with chance constraints. Their “training” phase is effectively an optimization stage, using heuristics (Best-Fit, bi-level) or a cutting-stock framework (CSP) to find resource allocation solutions under the modeled distributions."
    },
    "kdd22/2205.14526v1.json": {
        "title": "Group-wise Reinforcement Feature Generation for Optimal and Explainable  Representation Space Reconstruction",
        "abs": "Representation (feature) space is an environment where data points are\nvectorized, distances are computed, patterns are characterized, and geometric\nstructures are embedded. Extracting a good representation space is critical to\naddress the curse of dimensionality, improve model generalization, overcome\ndata sparsity, and increase the availability of classic models. Existing\nliterature, such as feature engineering and representation learning, is limited\nin achieving full automation (e.g., over heavy reliance on intensive labor and\nempirical experiences), explainable explicitness (e.g., traceable\nreconstruction process and explainable new features), and flexible optimal\n(e.g., optimal feature space reconstruction is not embedded into downstream\ntasks). Can we simultaneously address the automation, explicitness, and optimal\nchallenges in representation space reconstruction for a machine learning task?\nTo answer this question, we propose a group-wise reinforcement generation\nperspective. We reformulate representation space reconstruction into an\ninteractive process of nested feature generation and selection, where feature\ngeneration is to generate new meaningful and explicit features, and feature\nselection is to eliminate redundant features to control feature sizes. We\ndevelop a cascading reinforcement learning method that leverages three\ncascading Markov Decision Processes to learn optimal generation policies to\nautomate the selection of features and operations and the feature crossing. We\ndesign a group-wise generation strategy to cross a feature group, an operation,\nand another feature group to generate new features and find the strategy that\ncan enhance exploration efficiency and augment reward signals of cascading\nagents. Finally, we present extensive experiments to demonstrate the\neffectiveness, efficiency, traceability, and explicitness of our system.",
        "Task": "They automatically reconstruct an explainable feature space to enhance classification and regression tasks. Experiments on 24 real-world datasets measure performance using F1 for classification and 1-RAE for regression.",
        "Data_type": "They focus on structured, tabular data with numeric features and a single target label (regression or classification). The paper’s experiments center on UCI/Kaggle/OpenML datasets, and while it suggests broader applicability, it does not explicitly handle graph-structured, textual, or multi-modal image data.",
        "Data_domain": "These experiments use 24 publicly available tabular datasets (both classification and regression) gathered from UCI, LibSVM, Kaggle, and OpenML. The data domain is therefore academic/public repository data, covering multiple types of real-world tasks for benchmark evaluation.",
        "ML_phase": "They position their method in the feature engineering and representation learning stages of the ML pipeline by iteratively generating, clustering, and selecting features via reinforcement learning. In particular, their approach refines the feature space before model training, creating an optimal and explainable representation for downstream predictive tasks."
    },
    "kdd22/2209.08767v1.json": {
        "title": "Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs",
        "abs": "Two-view knowledge graphs (KGs) jointly represent two components: an ontology\nview for abstract and commonsense concepts, and an instance view for specific\nentities that are instantiated from ontological concepts. As such, these KGs\ncontain heterogeneous structures that are hierarchical, from the ontology-view,\nand cyclical, from the instance-view. Despite these various structures in KGs,\nmost recent works on embedding KGs assume that the entire KG belongs to only\none of the two views but not both simultaneously. For works that seek to put\nboth views of the KG together, the instance and ontology views are assumed to\nbelong to the same geometric space, such as all nodes embedded in the same\nEuclidean space or non-Euclidean product space, an assumption no longer\nreasonable for two-view KGs where different portions of the graph exhibit\ndifferent structures. To address this issue, we define and construct a\ndual-geometric space embedding model (DGS) that models two-view KGs using a\ncomplex non-Euclidean geometric space, by embedding different portions of the\nKG in different geometric spaces. DGS utilizes the spherical space, hyperbolic\nspace, and their intersecting space in a unified framework for learning\nembeddings. Furthermore, for the spherical space, we propose novel closed\nspherical space operators that directly operate in the spherical space without\nthe need for mapping to an approximate tangent space. Experiments on public\ndatasets show that DGS significantly outperforms previous state-of-the-art\nbaseline models on KG completion tasks, demonstrating its ability to better\nmodel heterogeneous structures in KGs.",
        "Task": "They focus on two tasks: (1) triple completion (link-level prediction) in both the instance and ontology views of the KG, and (2) entity typing (classification) to predict the concepts of entities. These tasks are evaluated on the YAGO26K-906 and DB111K-174 datasets.",
        "Data_type": "They handle graph-structured data in the form of two-view knowledge graphs, consisting of entity nodes with entity-entity links (instance view), concept nodes with concept-concept links (ontology view), and entity-concept links bridging both views. This setup captures large-scale cyclical and hierarchical relations for tasks like triple completion and entity typing.",
        "Data_domain": "These experiments use datasets derived from large-scale encyclopedic knowledge graphs (YAGO and DBpedia). In particular, they capture real-world entity and concept information spanning diverse general knowledge domains.",
        "ML_phase": "They first split the two-view KG into instance and ontology components, initialize entity/concept embeddings in spherical/hyperbolic spaces, and construct special intersection representations for bridge nodes. Then, they train these embeddings using Riemannian optimization and hinge-loss objectives, applying closed-form spherical operations and hyperbolic transformations without mapping to tangent spaces."
    },
    "kdd22/2206.15005v1.json": {
        "title": "Continuous-Time and Multi-Level Graph Representation Learning for  Origin-Destination Demand Prediction",
        "abs": "Traffic demand forecasting by deep neural networks has attracted widespread\ninterest in both academia and industry society. Among them, the pairwise\nOrigin-Destination (OD) demand prediction is a valuable but challenging problem\ndue to several factors: (i) the large number of possible OD pairs, (ii)\nimplicitness of spatial dependence, and (iii) complexity of traffic states. To\naddress the above issues, this paper proposes a Continuous-time and Multi-level\ndynamic graph representation learning method for Origin-Destination demand\nprediction (CMOD). Firstly, a continuous-time dynamic graph representation\nlearning framework is constructed, which maintains a dynamic state vector for\neach traffic node (metro stations or taxi zones). The state vectors keep\nhistorical transaction information and are continuously updated according to\nthe most recently happened transactions. Secondly, a multi-level structure\nlearning module is proposed to model the spatial dependency of station-level\nnodes. It can not only exploit relations between nodes adaptively from data,\nbut also share messages and representations via cluster-level and area-level\nvirtual nodes. Lastly, a cross-level fusion module is designed to integrate\nmulti-level memories and generate comprehensive node representations for the\nfinal prediction. Extensive experiments are conducted on two real-world\ndatasets from Beijing Subway and New York Taxi, and the results demonstrate the\nsuperiority of our model against the state-of-the-art approaches.",
        "Task": "They address a link-level regression task: predicting future passenger volume on each origin-destination pair (edge) in a transportation network. The model is evaluated on two real-world datasets (BJSubway and NYTaxi), forecasting the continuous passenger flows from each node to another over time.",
        "Data_type": "They use a continuous-time dynamic graph setting where nodes represent traffic stations (or zones) and edges are time-stamped passenger transactions. Node features are numeric (e.g., one-hot vectors), and there is no use of textual, image, or other multi-modal data in their approach.",
        "Data_domain": "The data comes from the transportation domain, specifically large-scale public transit (subways) and taxi ride transactions. This encompasses historical passenger flow records in urban regions.",
        "ML_phase": "The framework converts continuous-time transaction records into a dynamic graph and updates node memories via streaming event representations, then applies an adaptive multi-level structure (station-level, cluster-level, and area-level) with cross-level fusion to produce final node embeddings. It is trained end-to-end using a specialized loss that addresses data sparsity, enabling fine-grained temporal modeling and automatic discovery of spatial dependencies."
    },
    "kdd22/2205.13954v2.json": {
        "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype  Representation",
        "abs": "With the tremendous expansion of graphs data, node classification shows its\ngreat importance in many real-world applications. Existing graph neural network\nbased methods mainly focus on classifying unlabeled nodes within fixed classes\nwith abundant labeling. However, in many practical scenarios, graph evolves\nwith emergence of new nodes and edges. Novel classes appear incrementally along\nwith few labeling due to its newly emergence or lack of exploration. In this\npaper, we focus on this challenging but practical graph few-shot\nclass-incremental learning (GFSCIL) problem and propose a novel method called\nGeometer. Instead of replacing and retraining the fully connected neural\nnetwork classifer, Geometer predicts the label of a node by finding the nearest\nclass prototype. Prototype is a vector representing a class in the metric\nspace. With the pop-up of novel classes, Geometer learns and adjusts the\nattention-based prototypes by observing the geometric proximity, uniformity and\nseparability. Teacher-student knowledge distillation and biased sampling are\nfurther introduced to mitigate catastrophic forgetting and unbalanced labeling\nproblem respectively. Experimental results on four public datasets demonstrate\nthat Geometer achieves a substantial improvement of 9.46% to 27.60% over\nstate-of-the-art methods.",
        "Task": "They address a graph few-shot class-incremental node classification task, where new classes emerge over time under limited labeled data. Evaluation is conducted on four real-world datasets (Cora-ML, Flickr, Amazon, and Cora-Full) to measure classification performance.",
        "Data_type": "They handle graph-structured data with homogeneous node–edge relationships, where each node is annotated by textual or bag-of-words features (feature dimensions range from 767 to 8,710). These dynamic graphs evolve over time with new nodes, edges, and classes, focusing on incremental node classification tasks.",
        "Data_domain": "They employ node classification tasks on four real-world datasets spanning multiple domains: academic networks (Cora-ML and Cora-Full), social networks (Flickr), and e-commerce (Amazon). These datasets include citation relationships among research papers, user connections in a photo-sharing platform, and co-purchasing records of products.",
        "ML_phase": "They propose a 2-layer graph attention network for node embeddings and use class-level multi-head attention to form prototypes, then adopt an episodic meta-learning pipeline with biased sampling in pretraining and fine-tuning. The training framework combines geometric metric constraints with teacher-student knowledge distillation to mitigate catastrophic forgetting as novel classes incrementally appear."
    },
    "kdd22/2208.07638v1.json": {
        "title": "Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex  Logical Queries",
        "abs": "Knowledge graph (KG) embeddings have been a mainstream approach for reasoning\nover incomplete KGs. However, limited by their inherently shallow and static\narchitectures, they can hardly deal with the rising focus on complex logical\nqueries, which comprise logical operators, imputed edges, multiple source\nentities, and unknown intermediate entities. In this work, we present the\nKnowledge Graph Transformer (kgTransformer) with masked pre-training and\nfine-tuning strategies. We design a KG triple transformation method to enable\nTransformer to handle KGs, which is further strengthened by the\nMixture-of-Experts (MoE) sparse activation. We then formulate the complex\nlogical queries as masked prediction and introduce a two-stage masked\npre-training strategy to improve transferability and generalizability.\nExtensive experiments on two benchmarks demonstrate that kgTransformer can\nconsistently outperform both KG embedding-based baselines and advanced encoders\non nine in-domain and out-of-domain reasoning tasks. Additionally,\nkgTransformer can reason with explainability via providing the full reasoning\npaths to interpret given answers.",
        "Task": "They tackle complex query answering on knowledge graphs, which effectively requires identifying or ranking the correct target entities (node-level retrieval) given multi-hop and logical constraints. Performance is evaluated on standard knowledge graph benchmarks (e.g., FB15k-237, NELL995) by comparing predicted target nodes against ground-truth answers.",
        "Data_type": "They process knowledge graphs as node-edge structured data, where nodes are entities and edges are relations. They do not incorporate textual attributes, multi-modal inputs, or chemical fingerprints, focusing strictly on entity-relation graphs (e.g., FB15k-237, NELL995).",
        "Data_domain": "They use data from publicly available knowledge graphs, specifically FB15k-237 (derived from Freebase) and NELL995 (from the NELL project). The domain is factual knowledge bases covering diverse real-world entities and relations.",
        "ML_phase": "They transform KG triples into relation-nodes, then apply a Transformer-based GNN (with Mixture-of-Experts) and employ masked subgraph sampling for two-stage pre-training followed by fine-tuning. Specifically, dense subgraphs are first sampled to initialize the model and then sparse meta-graphs refine it before the final fine-tuning for query reasoning."
    },
    "kdd22/2206.02663v1.json": {
        "title": "TransBO: Hyperparameter Optimization via Two-Phase Transfer Learning",
        "abs": "With the extensive applications of machine learning models, automatic\nhyperparameter optimization (HPO) has become increasingly important. Motivated\nby the tuning behaviors of human experts, it is intuitive to leverage auxiliary\nknowledge from past HPO tasks to accelerate the current HPO task. In this\npaper, we propose TransBO, a novel two-phase transfer learning framework for\nHPO, which can deal with the complementary nature among source tasks and\ndynamics during knowledge aggregation issues simultaneously. This framework\nextracts and aggregates source and target knowledge jointly and adaptively,\nwhere the weights can be learned in a principled manner. The extensive\nexperiments, including static and dynamic transfer learning settings and neural\narchitecture search, demonstrate the superiority of TransBO over the\nstate-of-the-arts.",
        "Task": "They address hyperparameter optimization (HPO) tasks, primarily on classification problems across multiple real-world datasets and in neural architecture search settings. Their experiments evaluate performance on 30 classification benchmarks from OpenML and a NAS benchmark (NAS-Bench201).",
        "Data_type": "They handle tabular classification datasets with up to 8,192 rows (e.g., from OpenML) and include categorical, numerical, and conditional hyperparameters. They also support image-classification benchmarks for neural architecture search tasks (CIFAR-10, CIFAR-100, ImageNet16-120).",
        "Data_domain": "They employ real-world classification datasets from the OpenML repository, encompassing diverse features and scales. Additionally, the paper includes NAS experiments with CIFAR and ImageNet data, representing the image classification domain.",
        "ML_phase": "This method is applied during the hyperparameter optimization phase of training, where it builds a two-phase transfer learning surrogate to guide Bayesian optimization. It iteratively updates this surrogate with target-task observations to select the next hyperparameter configuration in the model training loop."
    },
    "kdd22/2206.00783v1.json": {
        "title": "Core-periphery Models for Hypergraphs",
        "abs": "We introduce a random hypergraph model for core-periphery structure. By\nleveraging our model's sufficient statistics, we develop a novel statistical\ninference algorithm that is able to scale to large hypergraphs with runtime\nthat is practically linear wrt. the number of nodes in the graph after a\npreprocessing step that is almost linear in the number of hyperedges, as well\nas a scalable sampling algorithm. Our inference algorithm is capable of\nlearning embeddings that correspond to the reputation (rank) of a node within\nthe hypergraph. We also give theoretical bounds on the size of the core of\nhypergraphs generated by our model. We experiment with hypergraph data that\nrange to $\\sim 10^5$ hyperedges mined from the Microsoft Academic Graph, Stack\nExchange, and GitHub and show that our model outperforms baselines wrt.\nproducing good fits.",
        "Task": "They introduce a node-level ranking task aimed at inferring continuous core-periphery structures in hypergraphs. The paper evaluates performance on real and synthetic hypergraph datasets using likelihood-based metrics.",
        "Data_type": "They handle hypergraph-structured data with node-level attributes that can be real-valued (e.g., in ℝ^d), supporting continuous or discrete features. The paper’s experiments focus on higher-order networks (e.g., coauthorship or QA forums) where each node may include numeric attributes, but there is no treatment of image-text, SMILES, or other multi-modal data.",
        "Data_domain": "The datasets span multiple real-world domains, including academic coauthorship (e.g., MAG-KDD), social Q&A forums (e.g., Stack Exchange), software collaboration (e.g., GitHub), and economic networks (e.g., world-trade). This variety ensures a comprehensive coverage of hypergraph data across academic, social, and economic contexts.",
        "ML_phase": "They introduce a hierarchical hypergraph model that leverages node ranks and partitions edges for tractable likelihood computation, enabling near-linear-time preprocessing and exact inference. Training employs gradient-based optimization (MLE/MAP or Bayesian) with efficient sampling procedures, including negative sampling for logistic baselines."
    },
    "kdd22/2206.03364v1.json": {
        "title": "KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular  Property Prediction",
        "abs": "Designing accurate deep learning models for molecular property prediction\nplays an increasingly essential role in drug and material discovery. Recently,\ndue to the scarcity of labeled molecules, self-supervised learning methods for\nlearning generalizable and transferable representations of molecular graphs\nhave attracted lots of attention. In this paper, we argue that there exist two\nmajor issues hindering current self-supervised learning methods from obtaining\ndesired performance on molecular property prediction, that is, the ill-defined\npre-training tasks and the limited model capacity. To this end, we introduce\nKnowledge-guided Pre-training of Graph Transformer (KPGT), a novel\nself-supervised learning framework for molecular graph representation learning,\nto alleviate the aforementioned issues and improve the performance on the\ndownstream molecular property prediction tasks. More specifically, we first\nintroduce a high-capacity model, named Line Graph Transformer (LiGhT), which\nemphasizes the importance of chemical bonds and is mainly designed to model the\nstructural information of molecular graphs. Then, a knowledge-guided\npre-training strategy is proposed to exploit the additional knowledge of\nmolecules to guide the model to capture the abundant structural and semantic\ninformation from large-scale unlabeled molecular graphs. Extensive\ncomputational tests demonstrated that KPGT can offer superior performance over\ncurrent state-of-the-art methods on several molecular property prediction\ntasks.",
        "Task": "They investigate a knowledge-guided pre-training strategy for graph transformers, primarily targeting node-level classification tasks. Their evaluations employ benchmark datasets that measure classification performance.",
        "Data_type": "They use graph-structured data with node and edge relational information, incorporating textual descriptions within nodes. No other modalities, such as images or SMILES/fingerprints, are discussed.",
        "Data_domain": "The paper’s data domain is knowledge graphs, which generally span multiple real-world or academic sources. No specific domain (e.g., biomedical, social media) is explicitly stated in the text.",
        "ML_phase": "They propose a knowledge-guided pre-training procedure for graph transformer models, integrating domain knowledge into the data processing and embedding steps. The training framework adopts a specialized pre-training objective followed by fine-tuning on downstream tasks."
    },
    "kdd22/2211.06684v1.json": {
        "title": "A Generalized Doubly Robust Learning Framework for Debiasing Post-Click  Conversion Rate Prediction",
        "abs": "Post-click conversion rate (CVR) prediction is an essential task for\ndiscovering user interests and increasing platform revenues in a range of\nindustrial applications. One of the most challenging problems of this task is\nthe existence of severe selection bias caused by the inherent self-selection\nbehavior of users and the item selection process of systems. Currently, doubly\nrobust (DR) learning approaches achieve the state-of-the-art performance for\ndebiasing CVR prediction. However, in this paper, by theoretically analyzing\nthe bias, variance and generalization bounds of DR methods, we find that\nexisting DR approaches may have poor generalization caused by inaccurate\nestimation of propensity scores and imputation errors, which often occur in\npractice. Motivated by such analysis, we propose a generalized learning\nframework that not only unifies existing DR methods, but also provides a\nvaluable opportunity to develop a series of new debiasing techniques to\naccommodate different application scenarios. Based on the framework, we propose\ntwo new DR methods, namely DR-BIAS and DR-MSE. DR-BIAS directly controls the\nbias of DR loss, while DR-MSE balances the bias and variance flexibly, which\nachieves better generalization performance. In addition, we propose a novel\ntri-level joint learning optimization method for DR-MSE in CVR prediction, and\nan efficient training algorithm correspondingly. We conduct extensive\nexperiments on both real-world and semi-synthetic datasets, which validate the\neffectiveness of our proposed methods.",
        "Task": "They address a binary classification task of predicting whether a user will convert (post-click conversion) given features of user-item interactions. The evaluation is conducted on both real-world and semi-synthetic datasets to assess prediction performance.",
        "Data_type": "They utilize tabular user–item interaction data, where each instance contains features of a user–item pair (e.g., IDs, contextual features), a click indicator, and a binary conversion label. No image, text, or graph-structured inputs are mentioned, as the focus remains on tabular feedback data for CVR prediction.",
        "Data_domain": "They use datasets from an e-commerce coat shopping platform (MNAR ratings from Amazon Mechanical Turkers), a music platform (Yahoo! R3 with user-song ratings), and a large-scale industrial product advertising system. Overall, the data domain spans crowdsourced product ratings, music recommendations, and commercial advertising logs.",
        "ML_phase": "They employ both real-world product logs (plus missing-at-random benchmarks) and semi-synthetic data generated by matrix factorization to obtain ground-truth click/conversion labels. The model architecture integrates a multi-task design (e.g., DCN or FM for CTR/CVR/error imputation) and is trained via a tri-level joint learning framework that iteratively updates each component in a unified optimization procedure."
    },
    "kdd22/2109.07628v3.json": {
        "title": "Connecting Low-Loss Subspace for Personalized Federated Learning",
        "abs": "Due to the curse of statistical heterogeneity across clients, adopting a\npersonalized federated learning method has become an essential choice for the\nsuccessful deployment of federated learning-based services. Among diverse\nbranches of personalization techniques, a model mixture-based personalization\nmethod is preferred as each client has their own personalized model as a result\nof federated learning. It usually requires a local model and a federated model,\nbut this approach is either limited to partial parameter exchange or requires\nadditional local updates, each of which is helpless to novel clients and\nburdensome to the client's computational capacity. As the existence of a\nconnected subspace containing diverse low-loss solutions between two or more\nindependent deep networks has been discovered, we combined this interesting\nproperty with the model mixture-based personalized federated learning method\nfor improved performance of personalization. We proposed SuPerFed, a\npersonalized federated learning method that induces an explicit connection\nbetween the optima of the local and the federated model in weight space for\nboosting each other. Through extensive experiments on several benchmark\ndatasets, we demonstrated that our method achieves consistent gains in both\npersonalization performance and robustness to problematic scenarios possible in\nrealistic services.",
        "Task": "They primarily tackle multi-class classification tasks under a personalized federated learning setting, validated on various benchmark datasets (MNIST, CIFAR10/100, TinyImageNet, FEMNIST) as well as a next-character prediction task (Shakespeare). Performance is measured through top-1/top-5 accuracy and calibration metrics across heterogeneous client data distributions.",
        "Data_type": "They demonstrate experiments primarily on image classification datasets (MNIST, CIFAR10, CIFAR100, TinyImageNet, FEMNIST) and a text-based dataset (Shakespeare). There is no mention of handling graph-structured data, multi-modal inputs, or SMILES/fingerprint representations.",
        "Data_domain": "They use standard academic benchmark datasets for image classification (MNIST, CIFAR-10, CIFAR-100, TinyImageNet, FEMNIST) and text-based prediction (Shakespeare). All data originate from publicly available, widely adopted academic sources.",
        "ML_phase": "They propose a training framework for federated learning in which each client receives a global model from the server, jointly updates it alongside a locally maintained model through a connectivity-based mixing approach, and then only returns the federated portion to the server for aggregation. This process is iterated across multiple communication rounds until convergence, enabling individualized models on clients while maintaining a centralized global model."
    },
    "kdd22/2206.04838v3.json": {
        "title": "In Defense of Core-set: A Density-aware Core-set Selection for Active  Learning",
        "abs": "Active learning enables the efficient construction of a labeled dataset by\nlabeling informative samples from an unlabeled dataset. In a real-world active\nlearning scenario, considering the diversity of the selected samples is crucial\nbecause many redundant or highly similar samples exist. Core-set approach is\nthe promising diversity-based method selecting diverse samples based on the\ndistance between samples. However, the approach poorly performs compared to the\nuncertainty-based approaches that select the most difficult samples where\nneural models reveal low confidence. In this work, we analyze the feature space\nthrough the lens of the density and, interestingly, observe that locally sparse\nregions tend to have more informative samples than dense regions. Motivated by\nour analysis, we empower the core-set approach with the density-awareness and\npropose a density-aware core-set (DACS). The strategy is to estimate the\ndensity of the unlabeled samples and select diverse samples mainly from sparse\nregions. To reduce the computational bottlenecks in estimating the density, we\nalso introduce a new density approximation based on locality-sensitive hashing.\nExperimental results clearly demonstrate the efficacy of DACS in both\nclassification and regression tasks and specifically show that DACS can produce\nstate-of-the-art performance in a practical scenario. Since DACS is weakly\ndependent on neural architectures, we present a simple yet effective\ncombination method to show that the existing methods can be beneficially\ncombined with DACS.",
        "Task": "They tackle both classification and regression tasks under active learning, evaluating on standard image datasets (CIFAR-10, RMNIST) for classification accuracy and a drug-protein interaction dataset (Davis) for MSE-based regression performance. These experiments demonstrate how their approach improves model performance with limited labeling budgets for both task types.",
        "Data_type": "They evaluate their method on image classification tasks using standard 2D image data (CIFAR-10 with 32×32×3 inputs, RMNIST with 28×28×1 inputs) and on a regression task for drug–protein interactions (Davis) that typically relies on SMILES or sequence-based representations. Hence, the approach covers both image-based data and textual/sequence data (e.g., SMILES) within active learning experiments.",
        "Data_domain": "Data is drawn from image classification tasks (CIFAR-10, Repeated MNIST) and from a drug-protein interaction task (Davis). It thus spans both computer vision and biochemical domains.",
        "ML_phase": "During the active learning phase, the paper proposes estimating local densities of unlabeled samples via an auxiliary classifier and LSH, then performing region-wise core-set selection to form a more informative labeled set. This procedure seamlessly integrates with standard iterative training pipelines by repeatedly updating the labeled pool and retraining the model."
    },
    "kdd22/2205.10803v3.json": {
        "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
        "abs": "Self-supervised learning (SSL) has been extensively explored in recent years.\nParticularly, generative SSL has seen emerging success in natural language\nprocessing and other AI fields, such as the wide adoption of BERT and GPT.\nDespite this, contrastive learning-which heavily relies on structural data\naugmentation and complicated training strategies-has been the dominant approach\nin graph SSL, while the progress of generative SSL on graphs, especially graph\nautoencoders (GAEs), has thus far not reached the potential as promised in\nother fields. In this paper, we identify and examine the issues that negatively\nimpact the development of GAEs, including their reconstruction objective,\ntraining robustness, and error metric. We present a masked graph autoencoder\nGraphMAE that mitigates these issues for generative self-supervised graph\npretraining. Instead of reconstructing graph structures, we propose to focus on\nfeature reconstruction with both a masking strategy and scaled cosine error\nthat benefit the robust training of GraphMAE. We conduct extensive experiments\non 21 public datasets for three different graph learning tasks. The results\nmanifest that GraphMAE-a simple graph autoencoder with careful designs-can\nconsistently generate outperformance over both contrastive and generative\nstate-of-the-art baselines. This study provides an understanding of graph\nautoencoders and demonstrates the potential of generative self-supervised\npre-training on graphs.",
        "Task": "They focus on self-supervised graph representation learning for node-level classification, graph-level classification, and transfer learning (molecular property prediction) tasks. Performance is evaluated on multiple standard benchmarks, including Cora, Citeseer, PubMed for node classification, IMDB-B, MUTAG for graph classification, and eight MoleculeNet datasets for transfer learning.",
        "Data_type": "They handle graph-structured data where each node has either continuous or discrete features (e.g., text, molecular, or degree/label attributes), and edges encode adjacency relationships. The framework applies to both node-level and graph-level tasks in primarily homogeneous graphs with masked node features.",
        "Data_domain": "They employ multiple real-world graph datasets spanning academic citation networks (Cora, Citeseer, PubMed, Ogbn-arxiv), social platforms (Reddit, IMDB), and biological or molecular contexts (PPI, chemistry). This includes both node-level and graph-level benchmarks covering diverse domains such as academic, social, and biochemical data.",
        "ML_phase": "They mask node features and feed the corrupted graph into a GNN encoder, then re-mask the encoder output for a GNN decoder that reconstructs the original features. Training is driven by a scaled cosine error objective, ensuring stable optimization with large mask ratios and no need for negative samples."
    },
    "kdd22/2208.03903v1.json": {
        "title": "Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema  Linking Graph",
        "abs": "The generalizability to new databases is of vital importance to Text-to-SQL\nsystems which aim to parse human utterances into SQL statements. Existing works\nachieve this goal by leveraging the exact matching method to identify the\nlexical matching between the question words and the schema items. However,\nthese methods fail in other challenging scenarios, such as the synonym\nsubstitution in which the surface form differs between the corresponding\nquestion words and schema items. In this paper, we propose a framework named\nISESL-SQL to iteratively build a semantic enhanced schema-linking graph between\nquestion tokens and database schemas. First, we extract a schema linking graph\nfrom PLMs through a probing procedure in an unsupervised manner. Then the\nschema linking graph is further optimized during the training process through a\ndeep graph learning method. Meanwhile, we also design an auxiliary task called\ngraph regularization to improve the schema information mentioned in the\nschema-linking graph. Extensive experiments on three benchmarks demonstrate\nthat ISESL-SQL could consistently outperform the baselines and further\ninvestigations show its generalizability and robustness.",
        "Task": "No machine learning task is described in the provided text. The content focuses on LaTeX formatting and does not present any classification, regression, or other predictive tasks.",
        "Data_type": "No specific data modalities or structures are described in the provided text. The paper focuses on LaTeX formatting instructions rather than handling graph-structured or multi-modal data.",
        "Data_domain": "No data domain is specified in this paper. It only provides formatting and structural guidelines, with no references to any particular data source or domain.",
        "ML_phase": "No explicit mention is made of data preprocessing, model architecture design, or training frameworks within the text. The document instead focuses on formatting guidelines and manuscript structure, providing no ML pipeline details."
    },
    "kdd22/2206.05558v1.json": {
        "title": "Communication-Efficient Robust Federated Learning with Noisy Labels",
        "abs": "Federated learning (FL) is a promising privacy-preserving machine learning\nparadigm over distributed located data. In FL, the data is kept locally by each\nuser. This protects the user privacy, but also makes the server difficult to\nverify data quality, especially if the data are correctly labeled. Training\nwith corrupted labels is harmful to the federated learning task; however,\nlittle attention has been paid to FL in the case of label noise. In this paper,\nwe focus on this problem and propose a learning-based reweighting approach to\nmitigate the effect of noisy labels in FL. More precisely, we tuned a weight\nfor each training sample such that the learned model has optimal generalization\nperformance over a validation set. More formally, the process can be formulated\nas a Federated Bilevel Optimization problem. Bilevel optimization problem is a\ntype of optimization problem with two levels of entangled problems. The\nnon-distributed bilevel problems have witnessed notable progress recently with\nnew efficient algorithms. However, solving bilevel optimization problems under\nthe Federated Learning setting is under-investigated. We identify that the high\ncommunication cost in hypergradient evaluation is the major bottleneck. So we\npropose \\textit{Comm-FedBiO} to solve the general Federated Bilevel\nOptimization problems; more specifically, we propose two\ncommunication-efficient subroutines to estimate the hypergradient. Convergence\nanalysis of the proposed algorithms is also provided. Finally, we apply the\nproposed algorithms to solve the noisy label problem. Our approach has shown\nsuperior performance on several real-world datasets compared to various\nbaselines.",
        "Task": "They address a federated image classification task under noisy label conditions. They evaluate performance on real-world classification datasets (MNIST, CIFAR-10, and FEMNIST).",
        "Data_type": "They handle standard image classification data (e.g., MNIST, CIFAR-10, FEMNIST), where the model parameter dimension is on the order of 10^5 for convolutional networks. No textual, graph-based, or other multi-modal data types are discussed.",
        "Data_domain": "These experiments use image datasets (MNIST, CIFAR-10, and FEMNIST) involving handwritten digits and general objects in a classification setting. The data are drawn from a computer vision domain.",
        "ML_phase": "They present a federated training framework where a bilevel optimization procedure reweights data samples to mitigate label noise during local and global model updates. The algorithm incorporates a communication-efficient hypergradient estimation method directly into the training loop to handle mislabeled data in a privacy-preserving setting."
    },
    "kdd22/2009.13092v2.json": {
        "title": "Learning Classifiers under Delayed Feedback with a Time Window  Assumption",
        "abs": "We consider training a binary classifier under delayed feedback (\\emph{DF\nlearning}). For example, in the conversion prediction in online ads, we\ninitially receive negative samples that clicked the ads but did not buy an\nitem; subsequently, some samples among them buy an item then change to\npositive. In the setting of DF learning, we observe samples over time, then\nlearn a classifier at some point. We initially receive negative samples;\nsubsequently, some samples among them change to positive. This problem is\nconceivable in various real-world applications such as online advertisements,\nwhere the user action takes place long after the first click. Owing to the\ndelayed feedback, naive classification of the positive and negative samples\nreturns a biased classifier. One solution is to use samples that have been\nobserved for more than a certain time window assuming these samples are\ncorrectly labeled. However, existing studies reported that simply using a\nsubset of all samples based on the time window assumption does not perform\nwell, and that using all samples along with the time window assumption improves\nempirical performance. We extend these existing studies and propose a method\nwith the unbiased and convex empirical risk that is constructed from all\nsamples under the time window assumption. To demonstrate the soundness of the\nproposed method, we provide experimental results on a synthetic and open\ndataset that is the real traffic log datasets in online advertising.",
        "Task": "They address a binary classification task under delayed feedback, predicting whether a click eventually leads to a positive outcome. The models are evaluated on both synthetic datasets and real-world advertising log data (e.g., Criteo).",
        "Data_type": "They primarily work with tabular or event-log data, where each instance has a feature vector (e.g., binary or numeric attributes) and a time dimension indicating delayed labels. The paper’s experiments are demonstrated on synthetic datasets and real-world click/conversion logs (Criteo) rather than on graph-structured or multi-modal data.",
        "Data_domain": "The data predominantly come from the online advertising domain, as demonstrated by both synthetic ad-click data and real-world conversion logs (Criteo dataset). Specifically, it involves user click events, associated features, and delayed purchase (conversion) outcomes for training and evaluating predictive models.",
        "ML_phase": "They propose an offline training procedure that first partitions data into a “biased” set (earlier arrivals) and an “oracle” set (later arrivals) under time-window and stationarity assumptions, then corrects mislabeled samples via a unified empirical risk formulation. Model training is conducted using convex or non-negative-corrected empirical risk minimization (e.g., logistic loss) with gradient-based optimization methods."
    },
    "kdd22/2206.03426v2.json": {
        "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage",
        "abs": "Graph Neural Networks (GNNs) have shown great power in learning node\nrepresentations on graphs. However, they may inherit historical prejudices from\ntraining data, leading to discriminatory bias in predictions. Although some\nwork has developed fair GNNs, most of them directly borrow fair representation\nlearning techniques from non-graph domains without considering the potential\nproblem of sensitive attribute leakage caused by feature propagation in GNNs.\nHowever, we empirically observe that feature propagation could vary the\ncorrelation of previously innocuous non-sensitive features to the sensitive\nones. This can be viewed as a leakage of sensitive information which could\nfurther exacerbate discrimination in predictions. Thus, we design two feature\nmasking strategies according to feature correlations to highlight the\nimportance of considering feature propagation and correlation variation in\nalleviating discrimination. Motivated by our analysis, we propose Fair View\nGraph Neural Network (FairVGNN) to generate fair views of features by\nautomatically identifying and masking sensitive-correlated features considering\ncorrelation variation after feature propagation. Given the learned fair views,\nwe adaptively clamp weights of the encoder to avoid using sensitive-related\nfeatures. Experiments on real-world datasets demonstrate that FairVGNN enjoys a\nbetter trade-off between model utility and fairness. Our code is publicly\navailable at https://github.com/YuWVandy/FairVGNN.",
        "Task": "They address a node-level classification problem on graph-structured data, where each node’s label (e.g., credit risk, default status, bail outcome) must be predicted. The paper evaluates performance on real-world datasets (German, Credit, Bail) and focuses on mitigating unfairness in these node classification tasks.",
        "Data_type": "They operate on homogeneous graph-structured data with tabular (numeric and categorical) node attributes. No multi-modal (image-text) or SMILES/fingerprint data is addressed.",
        "Data_domain": "They employ financial datasets (German Credit and Credit Defaulter) and a criminal justice dataset (Bail/Recidivism). Hence, the paper’s data domain spans banking/credit and bail-related legal records.",
        "ML_phase": "They generate multiple feature views by masking sensitive-correlated channels (via Gumbel-Softmax sampling) and feed these masked views into a GNN-based encoder. During training, an adversarial discriminator identifies residual sensitive signals to guide fair view learning, and an adaptive weight-clamping step further reduces the encoder’s reliance on sensitive features."
    },
    "kdd22/2206.05809v2.json": {
        "title": "Geometric Policy Iteration for Markov Decision Processes",
        "abs": "Recently discovered polyhedral structures of the value function for finite\nstate-action discounted Markov decision processes (MDP) shed light on\nunderstanding the success of reinforcement learning. We investigate the value\nfunction polytope in greater detail and characterize the polytope boundary\nusing a hyperplane arrangement. We further show that the value space is a union\nof finitely many cells of the same hyperplane arrangement and relate it to the\npolytope of the classical linear programming formulation for MDPs. Inspired by\nthese geometric properties, we propose a new algorithm, Geometric Policy\nIteration (GPI), to solve discounted MDPs. GPI updates the policy of a single\nstate by switching to an action that is mapped to the boundary of the value\nfunction polytope, followed by an immediate update of the value function. This\nnew update rule aims at a faster value improvement without compromising\ncomputational efficiency. Moreover, our algorithm allows asynchronous updates\nof state values which is more flexible and advantageous compared to traditional\npolicy iteration when the state set is large. We prove that the complexity of\nGPI achieves the best known bound $\\mathcal{O}\\left(\\frac{|\\mathcal{A}|}{1 -\n\\gamma}\\log \\frac{1}{1-\\gamma}\\right)$ of policy iteration and empirically\ndemonstrate the strength of GPI on MDPs of various sizes.",
        "Task": "They tackle a reinforcement learning control problem by finding the optimal policy in a discounted Markov Decision Process with finite states and actions. Performance is evaluated on randomly generated MDPs of varying sizes.",
        "Data_type": "They focus on finite-state, finite-action Markov decision processes, where each state-action pair yields discrete transitions and rewards. No additional data modalities (e.g., text, images, or graphs) are addressed in this work.",
        "Data_domain": "The paper primarily uses synthetic data generated from randomized finite Markov Decision Processes. No specific real-world domain (e.g., finance, social networks) is used, as the experiments rely entirely on these synthetic MDP configurations.",
        "ML_phase": "It sits in the reinforcement learning training framework, focusing on iterative policy updates (state-wise or asynchronous) and re-evaluation of the value function. Through these incremental updates and geometric insights, the method ensures faster and more stable convergence during MDP training."
    },
    "kdd22/2206.14017v2.json": {
        "title": "Proton: Probing Schema Linking Information from Pre-trained Language  Models for Text-to-SQL Parsing",
        "abs": "The importance of building text-to-SQL parsers which can be applied to new\ndatabases has long been acknowledged, and a critical step to achieve this goal\nis schema linking, i.e., properly recognizing mentions of unseen columns or\ntables when generating SQLs. In this work, we propose a novel framework to\nelicit relational structures from large-scale pre-trained language models\n(PLMs) via a probing procedure based on Poincar\\'e distance metric, and use the\ninduced relations to augment current graph-based parsers for better schema\nlinking. Compared with commonly-used rule-based methods for schema linking, we\nfound that probing relations can robustly capture semantic correspondences,\neven when surface forms of mentions and entities differ. Moreover, our probing\nprocedure is entirely unsupervised and requires no additional parameters.\nExtensive experiments show that our framework sets new state-of-the-art\nperformance on three benchmarks. We empirically verify that our probing\nprocedure can indeed find desired relational structures through qualitative\nanalysis. Our code can be found at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI.",
        "Task": "They tackle a text-to-SQL semantic parsing task, which is a structured program generation problem converting natural language queries into SQL. Performance is benchmarked on three datasets (Spider, SYN, and DK) to assess domain generalization.",
        "Data_type": "They handle graph-structured data where nodes and edges represent natural language tokens and database schema elements (tables/columns), all encoded as textual attributes. Their focus is on text-to-SQL parsing, linking question tokens to schema items in a homogeneous graph.",
        "Data_domain": "The datasets consist of cross-domain relational databases covering diverse real-world topics. They are collected from multiple sources to represent heterogeneous domains for text-to-SQL parsing tasks.",
        "ML_phase": "They introduce an unsupervised probing step prior to model training, extracting discrete relational structures (schema linking cues) from large-scale PLMs. These structures are then integrated into graph-based encoders (e.g., RAT-SQL, LGESQL) during the training phase to improve text-to-SQL performance and domain generalization."
    },
    "kdd22/2206.00242v5.json": {
        "title": "CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation",
        "abs": "Bundle recommendation aims to recommend a bundle of related items to users,\nwhich can satisfy the users' various needs with one-stop convenience. Recent\nmethods usually take advantage of both user-bundle and user-item interactions\ninformation to obtain informative representations for users and bundles,\ncorresponding to bundle view and item view, respectively. However, they either\nuse a unified view without differentiation or loosely combine the predictions\nof two separate views, while the crucial cooperative association between the\ntwo views' representations is overlooked. In this work, we propose to model the\ncooperative association between the two different views through cross-view\ncontrastive learning. By encouraging the alignment of the two separately\nlearned views, each view can distill complementary information from the other\nview, achieving mutual enhancement. Moreover, by enlarging the dispersion of\ndifferent users/bundles, the self-discrimination of representations is\nenhanced. Extensive experiments on three public datasets demonstrate that our\nmethod outperforms SOTA baselines by a large margin. Meanwhile, our method\nrequires minimal parameters of three set of embeddings (user, bundle, and item)\nand the computational costs are largely reduced due to more concise graph\nstructure and graph learning module. In addition, various ablation and model\nstudies demystify the working mechanism and justify our hypothesis. Codes and\ndatasets are available at https://github.com/mysbupt/CrossCBR.",
        "Task": "They address a top-K recommendation task focused on recommending bundles of items (e.g., playlists, outfits) to users. Their approach is evaluated on several real-world user-bundle interaction datasets from book, music, and fashion domains.",
        "Data_type": "They employ ID-based data forming multiple bipartite graphs (user–bundle, user–item, and bundle–item) with edges representing interactions or affiliations. No additional textual, visual, or other multi-modal attributes are used, making it purely a relational (graph-structured) dataset without external features.",
        "Data_domain": "The paper’s datasets span multiple real-world e-commerce platforms, including books (Youshu), music (NetEase), and fashion outfits (iFashion). As such, the data domain involves user-bundle consumption records in consumer-facing online services.",
        "ML_phase": "They construct bipartite graphs (U-B, U-I, and B-I) from user-bundle, user-item, and bundle-item interactions and then apply a LightGCN-based framework on each view (bundle and item). The learned representations are jointly optimized via a cross-view contrastive objective combined with a BPR loss to train the final recommendation model."
    },
    "kdd22/2208.06646v2.json": {
        "title": "Modeling Network-level Traffic Flow Transitions on Sparse Data",
        "abs": "Modeling how network-level traffic flow changes in the urban environment is\nuseful for decision-making in transportation, public safety and urban planning.\nThe traffic flow system can be viewed as a dynamic process that transits\nbetween states (e.g., traffic volumes on each road segment) over time. In the\nreal-world traffic system with traffic operation actions like traffic signal\ncontrol or reversible lane changing, the system's state is influenced by both\nthe historical states and the actions of traffic operations. In this paper, we\nconsider the problem of modeling network-level traffic flow under a real-world\nsetting, where the available data is sparse (i.e., only part of the traffic\nsystem is observed). We present DTIGNN, an approach that can predict\nnetwork-level traffic flows from sparse data. DTIGNN models the traffic system\nas a dynamic graph influenced by traffic signals, learns the transition models\ngrounded by fundamental transition equations from transportation, and predicts\nfuture traffic states with imputation in the process. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nmethods and can better support decision-making in transportation.",
        "Task": "They address a traffic flow forecasting task by predicting the future states of road segments in a network from sparse observations. The evaluation is performed on multiple real-world and synthetic datasets to measure predictive accuracy under missing data conditions.",
        "Data_type": "They handle dynamic graph-structured traffic data, where each node (road segment) has spatiotemporal volume features of dimension N×F per time step. The adjacency matrices (N×N) also vary over time based on traffic signal phases, and some node features are entirely missing.",
        "Data_domain": "The dataset is sourced from traffic flow in urban road networks, including real-world data from Hangzhou and New York as well as synthetic data generated by a city-scale traffic simulator.",
        "ML_phase": "They construct a spatiotemporal GNN pipeline where the base GNN outputs an attention matrix, then a neural transition layer integrates transport equations and dynamic signal-based adjacency to predict and impute missing traffic states. The model is trained end-to-end via iterative forward passes that refine unobserved data and update weights with a joint loss for prediction and imputation."
    },
    "kdd22/2206.05483v3.json": {
        "title": "Bilateral Dependency Optimization: Defending Against Model-inversion  Attacks",
        "abs": "Through using only a well-trained classifier, model-inversion (MI) attacks\ncan recover the data used for training the classifier, leading to the privacy\nleakage of the training data. To defend against MI attacks, previous work\nutilizes a unilateral dependency optimization strategy, i.e., minimizing the\ndependency between inputs (i.e., features) and outputs (i.e., labels) during\ntraining the classifier. However, such a minimization process conflicts with\nminimizing the supervised loss that aims to maximize the dependency between\ninputs and outputs, causing an explicit trade-off between model robustness\nagainst MI attacks and model utility on classification tasks. In this paper, we\naim to minimize the dependency between the latent representations and the\ninputs while maximizing the dependency between latent representations and the\noutputs, named a bilateral dependency optimization (BiDO) strategy. In\nparticular, we use the dependency constraints as a universally applicable\nregularizer in addition to commonly used losses for deep neural networks (e.g.,\ncross-entropy), which can be instantiated with appropriate dependency criteria\naccording to different tasks. To verify the efficacy of our strategy, we\npropose two implementations of BiDO, by using two different dependency\nmeasures: BiDO with constrained covariance (BiDO-COCO) and BiDO with\nHilbert-Schmidt Independence Criterion (BiDO-HSIC). Experiments show that BiDO\nachieves the state-of-the-art defense performance for a variety of datasets,\nclassifiers, and MI attacks while suffering a minor classification-accuracy\ndrop compared to the well-trained classifier with no defense, which lights up a\nnovel road to defend against MI attacks.",
        "Task": "They address classification tasks on CelebA (face recognition), MNIST (digit classification), and CIFAR-10 (object classification). The paper evaluates the classification performance while defending against model-inversion attacks on these datasets.",
        "Data_type": "They employ standard labeled image datasets, including facial images (CelebA), digit images (MNIST), and object images (CIFAR-10). All data are in the form of 2D images with class labels.",
        "Data_domain": "They use face images from CelebA, digit images from MNIST, and object images from CIFAR-10. Hence, the data domain is image-based, covering face recognition, handwritten digits, and general object recognition tasks.",
        "ML_phase": "They incorporate a bilateral dependency optimization strategy directly into the training framework, adding regularization terms that limit input-to-latent redundancy and preserve label-relevant features throughout forward/backward propagation. COCO- or HSIC-based dependency metrics are computed at each iteration and integrated with standard supervised losses to guide parameter updates."
    },
    "kdd22/2207.01117v1.json": {
        "title": "Saliency-Regularized Deep Multi-Task Learning",
        "abs": "Multitask learning is a framework that enforces multiple learning tasks to\nshare knowledge to improve their generalization abilities. While shallow\nmultitask learning can learn task relations, it can only handle predefined\nfeatures. Modern deep multitask learning can jointly learn latent features and\ntask sharing, but they are obscure in task relation. Also, they predefine which\nlayers and neurons should share across tasks and cannot learn adaptively. To\naddress these challenges, this paper proposes a new multitask learning\nframework that jointly learns latent features and explicit task relations by\ncomplementing the strength of existing shallow and deep multitask learning\nscenarios. Specifically, we propose to model the task relation as the\nsimilarity between task input gradients, with a theoretical analysis of their\nequivalency. In addition, we innovatively propose a multitask learning\nobjective that explicitly learns task relations by a new regularizer.\nTheoretical analysis shows that the generalizability error has been reduced\nthanks to the proposed regularizer. Extensive experiments on several multitask\nlearning and image classification benchmarks demonstrate the proposed method\neffectiveness, efficiency as well as reasonableness in the learned task\nrelation patterns.",
        "Task": "They address multi-task learning that covers both a controlled synthetic regression dataset and multiple image classification tasks on CIFAR-10, CelebA, and MS-COCO. Their evaluation measures predictive performance across these regression and classification tasks under a shared framework.",
        "Data_type": "Data_type: The paper primarily addresses pixel-level image data (e.g., CIFAR-10, CelebA, MS-COCO images) for multi-task classification. It focuses on raw image inputs rather than text/graph structures, showcasing deep MTL with visual saliency–based regularization.",
        "Data_domain": "The dataset is drawn from an image classification domain, encompassing both controlled synthetic data and large-scale real-world image collections (CIFAR-10, CelebA, and MS-COCO).",
        "ML_phase": "SRDML operates at the training phase with a shared convolutional backbone and task-specific heads, enforcing cross-task saliency alignment via an additional regularizer on their input gradients. The data processing includes synthetic data generation with controlled feature weights or preparation of real-world image datasets, and the entire model is trained end-to-end using gradient-based optimization on the proposed objective function."
    },
    "kdd22/2206.04792v1.json": {
        "title": "Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex  Evolving Data Stream",
        "abs": "Online anomaly detection from a data stream is critical for the safety and\nsecurity of many applications but is facing severe challenges due to complex\nand evolving data streams from IoT devices and cloud-based infrastructures.\nUnfortunately, existing approaches fall too short for these challenges; online\nanomaly detection methods bear the burden of handling the complexity while\noffline deep anomaly detection methods suffer from the evolving data\ndistribution. This paper presents a framework for online deep anomaly\ndetection, ARCUS, which can be instantiated with any autoencoder-based deep\nanomaly detection methods. It handles the complex and evolving data streams\nusing an adaptive model pooling approach with two novel techniques:\nconcept-driven inference and drift-aware model pool update; the former detects\nanomalies with a combination of models most appropriate for the complexity, and\nthe latter adapts the model pool dynamically to fit the evolving data streams.\nIn comprehensive experiments with ten data sets which are both high-dimensional\nand concept-drifted, ARCUS improved the anomaly detection accuracy of the\nstreaming variants of state-of-the-art autoencoder-based methods and that of\nthe state-of-the-art streaming anomaly detection methods by up to 22% and 37%,\nrespectively.",
        "Task": "They tackle an unsupervised anomaly detection task for evolving data streams, focusing on handling concept drift in high-dimensional data. To evaluate performance, they employ ten benchmark datasets (both synthetic and real) with various known or unknown drift types.",
        "Data_type": "They handle high-dimensional numeric or image-based data streams (e.g., MNIST/FMNIST with 784 dimensions, sensor readings with up to 128 features) under concept drift conditions. The paper does not address graph-structured inputs, textual attributes, or SMILES/fingerprint data.",
        "Data_domain": "They use high-dimensional, concept-drifting data streams encompassing real sensor domains (gas sensor values, optical sensor values, building imagery) and synthetic image data (MNIST, FMNIST). These sources primarily originate from IoT devices, monitoring systems, and benchmark image repositories.",
        "ML_phase": "ARCUS implements an online pipeline that initializes an AE-based model from the first batch and computes anomaly scores via concept-driven inference for each new batch. After evaluating model reliability, it either incrementally updates a selected model if reliable or creates a new model and merges similar ones to accommodate concept drifts."
    },
    "kdd22/2206.14384v1.json": {
        "title": "Framing Algorithmic Recourse for Anomaly Detection",
        "abs": "The problem of algorithmic recourse has been explored for supervised machine\nlearning models, to provide more interpretable, transparent and robust outcomes\nfrom decision support systems. An unexplored area is that of algorithmic\nrecourse for anomaly detection, specifically for tabular data with only\ndiscrete feature values. Here the problem is to present a set of\ncounterfactuals that are deemed normal by the underlying anomaly detection\nmodel so that applications can utilize this information for explanation\npurposes or to recommend countermeasures. We present an approach -- Context\npreserving Algorithmic Recourse for Anomalies in Tabular data (CARAT), that is\neffective, scalable, and agnostic to the underlying anomaly detection model.\nCARAT uses a transformer based encoder-decoder model to explain an anomaly by\nfinding features with low likelihood. Subsequently semantically coherent\ncounterfactuals are generated by modifying the highlighted features, using the\noverall context of features in the anomalous instance(s). Extensive experiments\nhelp demonstrate the efficacy of CARAT.",
        "Task": "They address the task of providing algorithmic recourse for unsupervised anomaly detection in tabular data with strictly categorical features. The performance is evaluated on real-world shipping datasets by measuring how effectively anomalies can be transformed into non-anomalous instances.",
        "Data_type": "They focus on tabular data with strictly categorical attributes, often of high cardinality, and represent these records as nodes and edges in a heterogeneous information network capturing co-occurrence relationships. No multi-modal or continuous-valued data types are addressed, as the approach centers on entity-based categorical features in tabular form.",
        "Data_domain": "Data is drawn from real-world shipping transactions, focusing on attributes like ports, carriers, and trade routes. Specifically, these records come from large-scale tabular trade logs (e.g., US Imports, Colombia Exports, Ecuador Exports) within the broader commerce and supply chain domain.",
        "ML_phase": "They first pretrain a Transformer-based encoder-decoder model on a masked entity reconstruction objective, then fix the encoder and train a second decoder to predict entity likelihoods in tabular records. Finally, they employ knowledge graph embeddings (DistMult) to identify plausible replacements for out-of-context entities, thus generating counterfactuals that lower anomaly scores."
    },
    "kdd22/2206.05941v1.json": {
        "title": "Towards Universal Sequence Representation Learning for Recommender  Systems",
        "abs": "In order to develop effective sequential recommenders, a series of sequence\nrepresentation learning (SRL) methods are proposed to model historical user\nbehaviors. Most existing SRL methods rely on explicit item IDs for developing\nthe sequence models to better capture user preference. Though effective to some\nextent, these methods are difficult to be transferred to new recommendation\nscenarios, due to the limitation by explicitly modeling item IDs. To tackle\nthis issue, we present a novel universal sequence representation learning\napproach, named UniSRec. The proposed approach utilizes the associated\ndescription text of items to learn transferable representations across\ndifferent recommendation scenarios. For learning universal item\nrepresentations, we design a lightweight item encoding architecture based on\nparametric whitening and mixture-of-experts enhanced adaptor. For learning\nuniversal sequence representations, we introduce two contrastive pre-training\ntasks by sampling multi-domain negatives. With the pre-trained universal\nsequence representation model, our approach can be effectively transferred to\nnew recommendation domains or platforms in a parameter-efficient way, under\neither inductive or transductive settings. Extensive experiments conducted on\nreal-world datasets demonstrate the effectiveness of the proposed approach.\nEspecially, our approach also leads to a performance improvement in a\ncross-platform setting, showing the strong transferability of the proposed\nuniversal SRL method. The code and pre-trained model are available at:\nhttps://github.com/RUCAIBox/UniSRec.",
        "Task": "They tackle a next-item prediction (ranking) task in sequential recommendation, aiming to identify which item a user will interact with next based on historical interaction sequences. Their experimental setup involves real-world cross-domain and cross-platform datasets to evaluate top-K recommendation performance.",
        "Data_type": "They handle textual item attributes from multiple domains (e.g., product titles, brands, descriptions). The approach learns universal embeddings solely from these text-based item representations without relying on explicit item IDs.",
        "Data_domain": "They gather user-item interaction data from multiple Amazon product categories (e-commerce reviews) and also include a separate UK-based online retail platform. Essentially, all datasets focus on user transaction records in the e-commerce domain.",
        "ML_phase": "They process multi-domain item text to create universal ID-agnostic embeddings through a parametric whitening transform and an MoE-enhanced adaptor, then feed these representations into a self-attentive sequence encoder. The model is pre-trained with sequence-item and sequence-sequence contrastive learning and finally fine-tuned in a parameter-efficient manner for new domains."
    },
    "kdd22/2206.05291v1.json": {
        "title": "ProActive: Self-Attentive Temporal Point Process Flows for Activity  Sequences",
        "abs": "Any human activity can be represented as a temporal sequence of actions\nperformed to achieve a certain goal. Unlike machine-made time series, these\naction sequences are highly disparate as the time taken to finish a similar\naction might vary between different persons. Therefore, understanding the\ndynamics of these sequences is essential for many downstream tasks such as\nactivity length prediction, goal prediction, etc. Existing neural approaches\nthat model an activity sequence are either limited to visual data or are task\nspecific, i.e., limited to next action or goal prediction. In this paper, we\npresent ProActive, a neural marked temporal point process (MTPP) framework for\nmodeling the continuous-time distribution of actions in an activity sequence\nwhile simultaneously addressing three high-impact problems -- next action\nprediction, sequence-goal prediction, and end-to-end sequence generation.\nSpecifically, we utilize a self-attention module with temporal normalizing\nflows to model the influence and the inter-arrival times between actions in a\nsequence. Moreover, for time-sensitive prediction, we perform an early\ndetection of sequence goal via a constrained margin-based optimization\nprocedure. This in-turn allows ProActive to predict the sequence goal using a\nlimited number of actions. Extensive experiments on sequences derived from\nthree activity recognition datasets show the significant accuracy boost of\nProActive over the state-of-the-art in terms of action and goal prediction, and\nthe first-ever application of end-to-end action sequence generation.",
        "Task": "They address three tasks on continuous-time action sequences: (i) next-action prediction (simultaneous category classification and time estimation), (ii) goal detection (sequence-level classification), and (iii) end-to-end sequence generation. These tasks are evaluated on real-world datasets featuring human activity events (e.g., cooking, sports, and social interactions).",
        "Data_type": "They use continuous-time action sequences derived from annotated video frames, where each action is labeled with a discrete category and its occurrence time. The data thus comprises time-stamped events (actions) with associated categorical labels drawn from real-world videos.",
        "Data_domain": "The paper’s datasets originate from real-world videos capturing cooking, sports, and assorted collective activities. They provide annotations of timestamped human actions and associated goals, forming continuous-time action sequences in these domains.",
        "ML_phase": "They first derive continuous-time action sequences from annotated frames, then embed them with a self-attention mechanism to capture inter-action dependencies. The model is trained end-to-end as a normalizing flow-based MTPP using joint likelihood for next-action prediction, margin-based losses for early goal detection, and log-normal flows for time sampling, with a discount-weighted cross-entropy goal classification."
    },
    "kdd22/2205.10218v3.json": {
        "title": "Learning Task-relevant Representations for Generalization via  Characteristic Functions of Reward Sequence Distributions",
        "abs": "Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.",
        "Task": "They address a reinforcement learning task for continuous control domains under visually dynamic distractions. Performance is evaluated on the Distracting Control Suite (e.g., cartpole-swingup) by measuring reward-based control efficacy.",
        "Data_type": "They process 2D image observations from visually complex reinforcement learning environments, incorporating dynamic backgrounds and color distractions. No textual, graph-structured, or other multimodal data types are addressed.",
        "Data_domain": "They employ visually rich continuous control tasks from the Distracting Control Suite, where real-world videos from the DAVIS 2017 dataset serve as dynamic background distractions. Hence, the data domain is simulated reinforcement learning environments augmented with real-world video content.",
        "ML_phase": "They first generate visual data under dynamic background and color distractions, then process it via a shared convolutional encoder. The approach employs an off-policy actor-critic training framework with an auxiliary characteristic function predictor to learn robust representations for reward sequences."
    },
    "kdd22/2205.09786v3.json": {
        "title": "Subset Node Anomaly Tracking over Large Dynamic Graphs",
        "abs": "Tracking a targeted subset of nodes in an evolving graph is important for\nmany real-world applications. Existing methods typically focus on identifying\nanomalous edges or finding anomaly graph snapshots in a stream way. However,\nedge-oriented methods cannot quantify how individual nodes change over time\nwhile others need to maintain representations of the whole graph all time, thus\ncomputationally inefficient.\n  This paper proposes \\textsc{DynAnom}, an efficient framework to quantify the\nchanges and localize per-node anomalies over large dynamic weighted-graphs.\nThanks to recent advances in dynamic representation learning based on\nPersonalized PageRank, \\textsc{DynAnom} is 1) \\textit{efficient}: the time\ncomplexity is linear to the number of edge events and independent on node size\nof the input graph; 2) \\textit{effective}: \\textsc{DynAnom} can successfully\ntrack topological changes reflecting real-world anomaly; 3) \\textit{flexible}:\ndifferent type of anomaly score functions can be defined for various\napplications. Experiments demonstrate these properties on both benchmark graph\ndatasets and a new large real-world dynamic graph. Specifically, an\ninstantiation method based on \\textsc{DynAnom} achieves the accuracy of 0.5425\ncompared with 0.2790, the best baseline, on the task of node-level anomaly\nlocalization while running 2.3 times faster than the baseline. We present a\nreal-world case study and further demonstrate the usability of \\textsc{DynAnom}\nfor anomaly discovery over large-scale graphs.",
        "Task": "They address temporal anomaly detection in dynamic graphs, focusing on both node-level and graph-level anomaly identification. The approach is evaluated using multiple real-world graph datasets (DARPA, EU-CORE-S/L, ENRON) and a newly constructed PERSON knowledge graph.",
        "Data_type": "They operate on dynamic, large-scale, weighted graph-structured data with multiple node types (e.g., persons, events, locations) and time-evolving edge weights. In particular, they focus on node-edge frequency information (weighted edges) across discrete snapshots to capture local and global changes in the network.",
        "Data_domain": "They examine large-scale dynamic graphs derived from several domains: network traffic data (DARPA), email communications (ENRON, EU-CORE), and person-event knowledge bases (EventKG). These datasets reflect social interactions, organizational exchanges, and real-world event participation.",
        "ML_phase": "They introduce an incremental representation learning process that updates approximate Personalized PageRank–based embeddings for each node after edge insertions/deletions in a dynamic weighted graph. Then they apply a local hashing-based dimensionality reduction step and compute anomaly scores from consecutive node representation snapshots in an efficient training framework that scales with the edge events."
    },
    "kdd22/2207.09051v1.json": {
        "title": "HICF: Hyperbolic Informative Collaborative Filtering",
        "abs": "Considering the prevalence of the power-law distribution in user-item\nnetworks, hyperbolic space has attracted considerable attention and achieved\nimpressive performance in the recommender system recently. The advantage of\nhyperbolic recommendation lies in that its exponentially increasing capacity is\nwell-suited to describe the power-law distributed user-item network whereas the\nEuclidean equivalent is deficient. Nonetheless, it remains unclear which kinds\nof items can be effectively recommended by the hyperbolic model and which\ncannot. To address the above concerns, we take the most basic recommendation\ntechnique, collaborative filtering, as a medium, to investigate the behaviors\nof hyperbolic and Euclidean recommendation models. The results reveal that (1)\ntail items get more emphasis in hyperbolic space than that in Euclidean space,\nbut there is still ample room for improvement; (2) head items receive modest\nattention in hyperbolic space, which could be considerably improved; (3) and\nnonetheless, the hyperbolic models show more competitive performance than\nEuclidean models. Driven by the above observations, we design a novel learning\nmethod, named hyperbolic informative collaborative filtering (HICF), aiming to\ncompensate for the recommendation effectiveness of the head item while at the\nsame time improving the performance of the tail item. The main idea is to adapt\nthe hyperbolic margin ranking learning, making its pull and push procedure\ngeometric-aware, and providing informative guidance for the learning of both\nhead and tail items. Extensive experiments back up the analytic findings and\nalso show the effectiveness of the proposed method. The work is valuable for\npersonalized recommendations since it reveals that the hyperbolic space\nfacilitates modeling the tail item, which often represents user-customized\npreferences or new products.",
        "Task": "They propose a top-K recommendation task focused on user–item interaction data from large-scale benchmarks (e.g., Amazon-CD, Amazon-Book, Yelp2020). Performance is evaluated via ranking-oriented metrics (Recall and NDCG) that measure how effectively the model retrieves relevant items for each user.",
        "Data_type": "They focus on user-item interaction data, representing it as a bipartite graph (user and item nodes connected by interaction edges). No additional textual, multimodal, or other attribute information is used, as the main input is the user-item adjacency structure.",
        "Data_domain": "The data are user-item interaction records from e-commerce platforms (Amazon-CD, Amazon-Book) and a crowd-sourced business review network (Yelp). These datasets capture large-scale user preferences, reflecting real-world economic and social domains.",
        "ML_phase": "They preprocess user-item data by binarizing ratings (≥4) and splitting into training/test sets, then build hyperbolic and Euclidean graph-based CF models (e.g., HGCF, LightGCN) incorporating multi-layer neighbor aggregation. Training uses a margin ranking loss (with geometry-aware margin) and a novel hyperbolic negative sampling approach, optimized via Riemannian SGD."
    },
    "kdd22/2206.02511v1.json": {
        "title": "Transfer Learning based Search Space Design for Hyperparameter Tuning",
        "abs": "The tuning of hyperparameters becomes increasingly important as machine\nlearning (ML) models have been extensively applied in data mining applications.\nAmong various approaches, Bayesian optimization (BO) is a successful\nmethodology to tune hyper-parameters automatically. While traditional methods\noptimize each tuning task in isolation, there has been recent interest in\nspeeding up BO by transferring knowledge across previous tasks. In this work,\nwe introduce an automatic method to design the BO search space with the aid of\ntuning history from past tasks. This simple yet effective approach can be used\nto endow many existing BO methods with transfer learning capabilities. In\naddition, it enjoys the three advantages: universality, generality, and\nsafeness. The extensive experiments show that our approach considerably boosts\nBO by designing a promising and compact search space instead of using the\nentire space, and outperforms the state-of-the-arts on a wide range of\nbenchmarks, including machine learning and deep learning tuning tasks, and\nneural architecture search.",
        "Task": "They tackle classification tasks across various tabular and image datasets to assess hyperparameter optimization performance. Specifically, they evaluate on OpenML, CIFAR, SVHN, and NAS-Bench-201 benchmarks, each focusing on classification metrics.",
        "Data_type": "They address tabular classification tasks (OpenML datasets) and image classification tasks (CIFAR-10, SVHN, Tiny-ImageNet), as well as purely categorical hyperparameter spaces in NASBench-201. No graph-based, textual, or multimodal data types are involved.",
        "Data_domain": "The data comes from multiple supervised learning benchmarks, including real-world classification tasks on OpenML datasets, standard vision problems (CIFAR-10, SVHN, Tiny-ImageNet), and neural architecture search settings (NAS-Bench201). These cover diverse domains and reflect typical machine-learning and deep-learning use cases for hyperparameter optimization.",
        "ML_phase": "In the ML pipeline, this approach inserts a specialized search-space design module into the hyperparameter optimization phase. It uses previously observed configurations from source tasks to construct a compact sub-region of the original space, then applies Bayesian optimization on that refined domain to accelerate tuning on the target task."
    },
    "kdd22/2206.05476v1.json": {
        "title": "Sampling-based Estimation of the Number of Distinct Values in  Distributed Environment",
        "abs": "In data mining, estimating the number of distinct values (NDV) is a\nfundamental problem with various applications. Existing methods for estimating\nNDV can be broadly classified into two categories: i) scanning-based methods,\nwhich scan the entire data and maintain a sketch to approximate NDV; and ii)\nsampling-based methods, which estimate NDV using sampling data rather than\naccessing the entire data warehouse. Scanning-based methods achieve a lower\napproximation error at the cost of higher I/O and more time. Sampling-based\nestimation is preferable in applications with a large data volume and a\npermissible error restriction due to its higher scalability. However, while the\nsampling-based method is more effective on a single machine, it is less\npractical in a distributed environment with massive data volumes. For obtaining\nthe final NDV estimators, the entire sample must be transferred throughout the\ndistributed system, incurring a prohibitive communication cost when the sample\nrate is significant. This paper proposes a novel sketch-based distributed\nmethod that achieves sub-linear communication costs for distributed\nsampling-based NDV estimation under mild assumptions. Our method leverages a\nsketch-based algorithm to estimate the sample's {\\em frequency of frequency} in\nthe {\\em distributed streaming model}, which is compatible with most classical\nsampling-based NDV estimators. Additionally, we provide theoretical evidence\nfor our method's ability to minimize communication costs in the worst-case\nscenario. Extensive experiments show that our method saves orders of magnitude\nin communication costs compared to existing sampling- and sketch-based methods.",
        "Task": "They address a numeric estimation task, specifically the problem of estimating the number of distinct values (NDV) from sampled data in large-scale, distributed settings. To evaluate performance, they use synthetic datasets (e.g., Poisson and Zipf distributions) and a real-world Star Schema Benchmark (SSB) dataset.",
        "Data_type": "They handle large-scale tabular data, where each column’s values (numeric or categorical) are represented via frequency vectors. The paper focuses on estimating the number of distinct entries (NDV) in such distributed tabular datasets.",
        "Data_domain": "The paper’s data primarily originates from large-scale database settings, encompassing both synthetic distributions (e.g., Poisson and Zipfian) and the Star Schema Benchmark (SSB) data warehouse workload. This places the work in the context of large-scale, distributed relational data.",
        "ML_phase": "This work is situated in the data preprocessing stage of the ML pipeline, focusing on constructing and merging lightweight sketches to estimate distinct value counts across distributed data. It proposes a framework to generate sample-based frequency summaries and merge them sub-linearly, thereby reducing communication and I/O costs before downstream model training or analysis."
    },
    "kdd22/2206.03644v2.json": {
        "title": "Neural Bandit with Arm Group Graph",
        "abs": "Contextual bandits aim to identify among a set of arms the optimal one with\nthe highest reward based on their contextual information. Motivated by the fact\nthat the arms usually exhibit group behaviors and the mutual impacts exist\namong groups, we introduce a new model, Arm Group Graph (AGG), where the nodes\nrepresent the groups of arms and the weighted edges formulate the correlations\namong groups. To leverage the rich information in AGG, we propose a bandit\nalgorithm, AGG-UCB, where the neural networks are designed to estimate rewards,\nand we propose to utilize graph neural networks (GNN) to learn the\nrepresentations of arm groups with correlations. To solve the\nexploitation-exploration dilemma in bandits, we derive a new upper confidence\nbound (UCB) built on neural networks (exploitation) for exploration.\nFurthermore, we prove that AGG-UCB can achieve a near-optimal regret bound with\nover-parameterized neural networks, and provide the convergence analysis of GNN\nwith fully-connected layers which may be of independent interest. In the end,\nwe conduct extensive experiments against state-of-the-art baselines on multiple\npublic data sets, showing the effectiveness of the proposed algorithm.",
        "Task": "They tackle a contextual bandit task, i.e., an online decision-making problem for selecting arms and maximizing cumulative reward. Their evaluation uses both recommendation datasets (MovieLens, Yelp) and classification data (MNIST-Aug, XRMB).",
        "Data_type": "They process graph-structured data where each context (arm) is in ℝ^(dₓ), and groups of arms are treated as nodes (N_c of them). An embedded matrix then has column dimension d͂ₓ = dₓ × N_c, capturing group-aware information for subsequent learning.",
        "Data_domain": "The data come from real-world rating platforms (MovieLens, Yelp), an image classification dataset (MNIST), and a multi-view classification dataset (XRMB). They cover user ratings, digit recognition, and multi-view speech classification tasks.",
        "ML_phase": "They construct a graph from accessible group information, then use GNN-based aggregation to obtain group-aware arm embeddings before passing them into a fully connected network for reward prediction. The entire model is trained under an iterative UCB-based procedure, updating parameter estimates and the arm group graph after each round via gradient descent."
    },
    "kdd22/2206.12626v1.json": {
        "title": "Multi-Variate Time Series Forecasting on Variable Subsets",
        "abs": "We formulate a new inference task in the domain of multivariate time series\nforecasting (MTSF), called Variable Subset Forecast (VSF), where only a small\nsubset of the variables is available during inference. Variables are absent\nduring inference because of long-term data loss (eg. sensor failures) or high\n-> low-resource domain shift between train / test. To the best of our\nknowledge, robustness of MTSF models in presence of such failures, has not been\nstudied in the literature. Through extensive evaluation, we first show that the\nperformance of state of the art methods degrade significantly in the VSF\nsetting. We propose a non-parametric, wrapper technique that can be applied on\ntop any existing forecast models. Through systematic experiments across 4\ndatasets and 5 forecast models, we show that our technique is able to recover\nclose to 95\\% performance of the models even when only 15\\% of the original\nvariables are present.",
        "Task": "No specific machine learning task is described in the provided excerpt. The text focuses on acknowledgments and appendices, offering no datasets or task-related details.",
        "Data_type": "No specific data type is mentioned or described in the provided text. Consequently, there is no indication of handling any particular structured or unstructured data (e.g., graph, image-text, SMILES) within this paper.",
        "Data_domain": "No specific data source is mentioned in the provided text. Hence, the data domain is not clearly identified or discussed within the excerpt.",
        "ML_phase": "No details are provided regarding data processing, model architecture, or training frameworks in the ML pipeline. The paper focuses solely on acknowledgments and appendices without discussing any machine learning phases."
    },
    "kdd22/2206.04726v2.json": {
        "title": "COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive  Learning",
        "abs": "Graph contrastive learning (GCL) improves graph representation learning,\nleading to SOTA on various downstream tasks. The graph augmentation step is a\nvital but scarcely studied step of GCL. In this paper, we show that the node\nembedding obtained via the graph augmentations is highly biased, somewhat\nlimiting contrastive models from learning discriminative features for\ndownstream tasks. Thus, instead of investigating graph augmentation in the\ninput space, we alternatively propose to perform augmentations on the hidden\nfeatures (feature augmentation). Inspired by so-called matrix sketching, we\npropose COSTA, a novel COvariance-preServing feaTure space Augmentation\nframework for GCL, which generates augmented features by maintaining a \"good\nsketch\" of original features. To highlight the superiority of feature\naugmentation with COSTA, we investigate a single-view setting (in addition to\nmulti-view one) which conserves memory and computations. We show that the\nfeature augmentation with COSTA achieves comparable/better results than graph\naugmentation based models.",
        "Task": "They address a node-level classification task on standard benchmark graphs (e.g., Cora, CiteSeer, PubMed, Amazon-Computers, Amazon-Photo). These datasets are used to evaluate classification accuracy of the learned node representations.",
        "Data_type": "They handle node-centric, homogeneous graph-structured data with textual (or numeric) node attributes from citation networks and co-purchase networks. No images, multi-modal, or SMILES/fingerprint data are used.",
        "Data_domain": "They use graph datasets drawn primarily from academic citation networks (Cora, CiteSeer, Pubmed, DBLP, Coauthor-CS, Coauthor-Physics) and social networks (Wiki-CS, Amazon-Computers, Amazon-Photo). These datasets represent publications, product co-purchases, or cross-references, providing node and edge information in graph form.",
        "ML_phase": "They replace standard graph augmentations with feature-level transformations (e.g., random projections) that preserve second-order statistics before feeding the node representations into a GNN encoder and projection head. Then, they train the model under a single- or multi-view contrastive framework, optimizing embeddings end-to-end with a contrastive loss."
    },
    "kdd22/2110.04866v1.json": {
        "title": "CoRGi: Content-Rich Graph Neural Networks with Attention",
        "abs": "Graph representations of a target domain often project it to a set of\nentities (nodes) and their relations (edges). However, such projections often\nmiss important and rich information. For example, in graph representations used\nin missing value imputation, items - represented as nodes - may contain rich\ntextual information. However, when processing graphs with graph neural networks\n(GNN), such information is either ignored or summarized into a single vector\nrepresentation used to initialize the GNN. Towards addressing this, we present\nCoRGi, a GNN that considers the rich data within nodes in the context of their\nneighbors. This is achieved by endowing CoRGi's message passing with a\npersonalized attention mechanism over the content of each node. This way, CoRGi\nassigns user-item-specific attention scores with respect to the words that\nappear in an item's content. We evaluate CoRGi on two edge-value prediction\ntasks and show that CoRGi is better at making edge-value predictions over\nexisting methods, especially on sparse regions of the graph.",
        "Task": "They conduct a regression task on the Goodreads dataset by predicting user ratings (evaluated via RMSE) and a classification task on the Eedi dataset by predicting correct or incorrect responses (evaluated via accuracy and related metrics). Both tasks leverage content information for performance improvement.",
        "Data_type": "They operate on graph-structured data (user–item bipartite) where item nodes include textual attributes (e.g., descriptions or OCR-extracted text). The method processes these textual features as sequences of words, integrating them into node representations on the graph.",
        "Data_domain": "The paper utilizes two real-world datasets: one from an educational platform (Eedi) and another from a social reading network (Goodreads). Eedi represents an academic domain with question text extracted via OCR, while Goodreads represents a social network domain centered on user-book interactions.",
        "ML_phase": "They preprocess textual item data (including filtering non-English book descriptions, converting text-based ratings to numeric values, and applying OCR for question images), truncate content sequences beyond a threshold T, and apply neighbor sampling for efficient batching. During training, they implement a GNN with content-attention edge embeddings, utilize a caching mechanism to reduce memory usage, and run experiments on single GPUs with baseline comparisons."
    },
    "kdd22/2105.08925v3.json": {
        "title": "Practical Lossless Federated Singular Vector Decomposition over  Billion-Scale Data",
        "abs": "With the enactment of privacy-preserving regulations, e.g., GDPR, federated\nSVD is proposed to enable SVD-based applications over different data sources\nwithout revealing the original data. However, many SVD-based applications\ncannot be well supported by existing federated SVD solutions. The crux is that\nthese solutions, adopting either differential privacy (DP) or homomorphic\nencryption (HE), suffer from accuracy loss caused by unremovable noise or\ndegraded efficiency due to inflated data.\n  In this paper, we propose FedSVD, a practical lossless federated SVD method\nover billion-scale data, which can simultaneously achieve lossless accuracy and\nhigh efficiency. At the heart of FedSVD is a lossless matrix masking scheme\ndelicately designed for SVD: 1) While adopting the masks to protect private\ndata, FedSVD completely removes them from the final results of SVD to achieve\nlossless accuracy; and 2) As the masks do not inflate the data, FedSVD avoids\nextra computation and communication overhead during the factorization to\nmaintain high efficiency. Experiments with real-world datasets show that FedSVD\nis over 10000 times faster than the HE-based method and has 10 orders of\nmagnitude smaller error than the DP-based solution on SVD tasks. We further\nbuild and evaluate FedSVD over three real-world applications: principal\ncomponents analysis (PCA), linear regression (LR), and latent semantic analysis\n(LSA), to show its superior performance in practice. On federated LR tasks,\ncompared with two state-of-the-art solutions: FATE and SecureML, FedSVD-LR is\n100 times faster than SecureML and 10 times faster than FATE.",
        "Task": "They address large-scale singular value decomposition (SVD) as well as downstream tasks of dimensionality reduction (PCA), regression (linear regression), and text embedding (LSA). Their experiments use multiple real-world (e.g., MNIST, Wine, MovieLens) and synthetic datasets to evaluate performance.",
        "Data_type": "They handle large-scale matrix-structured numeric data in R^(m×n), scaling up to billions of elements (e.g., 100k × 1M). The paper demonstrates applicability to horizontally or vertically partitioned data in scenarios such as PCA, LR, and LSA, without restricting specific modalities beyond their representation as matrix inputs.",
        "Data_domain": "The paper’s datasets span multiple real-world domains such as medical records, biometric data, marketing transactions, and user rating information. They encompass horizontally or vertically partitioned scenarios across different institutions.",
        "ML_phase": "The paper proposes a federated SVD framework that fits into the training stage of the ML pipeline by applying local random masking to each participant’s data, securely aggregating the masked matrices, and then performing standard SVD on the aggregated result. Afterward, each participant removes the masks to recover the exact decomposition, enabling downstream ML tasks (e.g., PCA, LR, LSA) without privacy compromise."
    },
    "kdd22/2206.04872v1.json": {
        "title": "Multi-fidelity Hierarchical Neural Processes",
        "abs": "Science and engineering fields use computer simulation extensively. These\nsimulations are often run at multiple levels of sophistication to balance\naccuracy and efficiency. Multi-fidelity surrogate modeling reduces the\ncomputational cost by fusing different simulation outputs. Cheap data generated\nfrom low-fidelity simulators can be combined with limited high-quality data\ngenerated by an expensive high-fidelity simulator. Existing methods based on\nGaussian processes rely on strong assumptions of the kernel functions and can\nhardly scale to high-dimensional settings. We propose Multi-fidelity\nHierarchical Neural Processes (MF-HNP), a unified neural latent variable model\nfor multi-fidelity surrogate modeling. MF-HNP inherits the flexibility and\nscalability of Neural Processes. The latent variables transform the\ncorrelations among different fidelity levels from observations to latent space.\nThe predictions across fidelities are conditionally independent given the\nlatent states. It helps alleviate the error propagation issue in existing\nmethods. MF-HNP is flexible enough to handle non-nested high dimensional data\nat different fidelity levels with varying input and output dimensions. We\nevaluate MF-HNP on epidemiology and climate modeling tasks, achieving\ncompetitive performance in terms of accuracy and uncertainty estimation. In\ncontrast to deep Gaussian Processes with only low-dimensional (< 10) tasks, our\nmethod shows great promise for speeding up high-dimensional complex simulations\n(over 7000 for epidemiology modeling and 45000 for climate modeling).",
        "Task": "They address a multi-fidelity regression task, generating continuous predictions of epidemiological and climate variables at varying fidelity levels. They evaluate performance on real-world datasets for infection incidence forecasting and temperature modeling.",
        "Data_type": "They can handle multi-fidelity input-output pairs with varying dimensions (e.g., 18- or 85-dimensional age-stratified infection counts, 14×14 or 87×87 climate grids). The approach accommodates both nested and non-nested data across different fidelity levels, enabling flexible input and output dimensionalities.",
        "Data_domain": "They use multi-fidelity data from age-stratified epidemic simulations (epidemiology) and temperature outputs in climate modeling. Specifically, the datasets span varying input-output dimensions across different fidelity levels in these two domains.",
        "ML_phase": "They introduce a hierarchical neural latent variable model that processes multi-fidelity data by assigning separate latent variables to each fidelity level, then trains all levels jointly under a single unified evidence lower bound (ELBO). The pipeline includes encoding context-target splits at each fidelity level, decoupling inputs with reduced dimensionality, and leveraging approximate inference (e.g., Monte Carlo or ancestral sampling) to learn shared parameters efficiently."
    },
    "kdd22/2206.11054v1.json": {
        "title": "S2RL: Do We Really Need to Perceive All States in Deep Multi-Agent  Reinforcement Learning?",
        "abs": "Collaborative multi-agent reinforcement learning (MARL) has been widely used\nin many practical applications, where each agent makes a decision based on its\nown observation. Most mainstream methods treat each local observation as an\nentirety when modeling the decentralized local utility functions. However, they\nignore the fact that local observation information can be further divided into\nseveral entities, and only part of the entities is helpful to model inference.\nMoreover, the importance of different entities may change over time. To improve\nthe performance of decentralized policies, the attention mechanism is used to\ncapture features of local information. Nevertheless, existing attention models\nrely on dense fully connected graphs and cannot better perceive important\nstates. To this end, we propose a sparse state based MARL (S2RL) framework,\nwhich utilizes a sparse attention mechanism to discard irrelevant information\nin local observations. The local utility functions are estimated through the\nself-attention and sparse attention mechanisms separately, then are combined\ninto a standard joint value function and auxiliary joint value function in the\ncentral critic. We design the S2RL framework as a plug-and-play module, making\nit general enough to be applied to various methods. Extensive experiments on\nStarCraft II show that S2RL can significantly improve the performance of many\nstate-of-the-art methods.",
        "Task": "They address a cooperative multi-agent reinforcement learning problem, focusing on learning policies for multiple agents under partial observability. They evaluate performance on the StarCraft Multi-Agent Challenge benchmark for complex micromanagement tasks.",
        "Data_type": "They handle numeric entity-based features representing each agent’s partial observations in a discrete multi-agent environment. Each entity is represented as a vector of attributes (e.g., location, type, health) that vary dynamically over time.",
        "Data_domain": "The data in this paper is collected from the StarCraft Multi-Agent Challenge (SMAC), a set of cooperative multi-agent tasks in the StarCraft II real-time strategy game environment. It specifically focuses on micromanagement challenges where multiple agents must coordinate in combat scenarios.",
        "ML_phase": "They propose a novel network architecture that augments each agent’s utility function with both dense and sparse attention modules, which are then combined in a central mixing network under the CTDE paradigm. This design, along with an auxiliary loss for sparse attention, forms an end-to-end training pipeline that stabilizes convergence while preserving decentralized execution."
    },
    "kdd22/2106.07767v4.json": {
        "title": "How does Heterophily Impact the Robustness of Graph Neural Networks?  Theoretical Connections and Practical Implications",
        "abs": "We bridge two research directions on graph neural networks (GNNs), by\nformalizing the relation between heterophily of node labels (i.e., connected\nnodes tend to have dissimilar labels) and the robustness of GNNs to adversarial\nattacks. Our theoretical and empirical analyses show that for homophilous graph\ndata, impactful structural attacks always lead to reduced homophily, while for\nheterophilous graph data the change in the homophily level depends on the node\ndegrees. These insights have practical implications for defending against\nattacks on real-world graphs: we deduce that separate aggregators for ego- and\nneighbor-embeddings, a design principle which has been identified to\nsignificantly improve prediction for heterophilous graph data, can also offer\nincreased robustness to GNNs. Our comprehensive experiments show that GNNs\nmerely adopting this design achieve improved empirical and certifiable\nrobustness compared to the best-performing unvaccinated model. Additionally,\ncombining this design with explicit defense mechanisms against adversarial\nattacks leads to an improved robustness with up to 18.33% performance increase\nunder attacks compared to the best-performing vaccinated model.",
        "Task": "They address semi-supervised node classification on various real-world graphs, evaluating robustness against adversarial structure perturbations. Multiple homophilous and heterophilous datasets are used to assess performance under targeted and untargeted attacks.",
        "Data_type": "They handle graph-structured data in homogeneous networks, with node-level attributes represented by real-valued or textual feature matrices. The paper focuses on semi-supervised node classification across diverse datasets (e.g., citation networks, social graphs) that exhibit varying degrees of homophily and heterophily.",
        "Data_domain": "The work analyzes graph data from both academic citation networks (Cora, Citeseer, Pubmed) and social/information networks (Facebook FB100 data and Snap Patents). These datasets span homophilous and heterophilous graph structures, reflecting different real-world domains such as scholarly publications, social relationships, and patent citations.",
        "ML_phase": "They introduce a GNN architecture that separates ego- and neighbor-embedding aggregators to handle heterophily, integrating this design into the standard message-passing pipeline. The models are trained for semi-supervised node classification on real-world graphs, with adversarial perturbations applied to the graph structure to evaluate and enhance robustness."
    },
    "kdd22/2207.08806v1.json": {
        "title": "Unified 2D and 3D Pre-Training of Molecular Representations",
        "abs": "Molecular representation learning has attracted much attention recently. A\nmolecule can be viewed as a 2D graph with nodes/atoms connected by edges/bonds,\nand can also be represented by a 3D conformation with 3-dimensional coordinates\nof all atoms. We note that most previous work handles 2D and 3D information\nseparately, while jointly leveraging these two sources may foster a more\ninformative representation. In this work, we explore this appealing idea and\npropose a new representation learning method based on a unified 2D and 3D\npre-training. Atom coordinates and interatomic distances are encoded and then\nfused with atomic representations through graph neural networks. The model is\npre-trained on three tasks: reconstruction of masked atoms and coordinates, 3D\nconformation generation conditioned on 2D graph, and 2D graph generation\nconditioned on 3D conformation. We evaluate our method on 11 downstream\nmolecular property prediction tasks: 7 with 2D information only and 4 with both\n2D and 3D information. Our method achieves state-of-the-art results on 10\ntasks, and the average improvement on 2D-only tasks is 8.3%. Our method also\nachieves significant improvement on two 3D conformation generation tasks.",
        "Task": "They address two main tasks: molecule-level classification/regression for property prediction (using datasets such as MoleculeNet, ogb-molpcba, and various toxicity benchmarks) and generative modeling for 3D conformation (using subsets of the GEOM dataset). Both tasks are evaluated to demonstrate performance gains in molecular property prediction and 3D conformer generation.",
        "Data_type": "They handle molecular data in multiple representations, specifically 2D graphs (with atom and bond features) and 3D conformations (atomic coordinates), as well as SMILES strings. This enables the model to fuse both topological (2D) and spatial (3D) information along with textual fingerprints.",
        "Data_domain": "The data come from the chemical domain, covering diverse molecular datasets with 2D graph structures and corresponding 3D conformations. These include large-scale resources such as PCQM4Mv2, MoleculeNet, and GEOM, used for tasks like property prediction and conformation generation.",
        "ML_phase": "They unify 2D molecular graphs and 3D coordinates (e.g., from PCQM4Mv2) by masking random subsets of atoms and coordinates, then embedding these inputs via a feed-forward network that encodes atomic positions and distances. The model architecture is a GNN trained under three objectives (masked atom/coordinate reconstruction, 2D→3D generation, and 3D→2D generation) with permutation- and roto-translation-invariant constraints."
    },
    "kdd22/2206.03739v1.json": {
        "title": "Disentangled Ontology Embedding for Zero-shot Learning",
        "abs": "Knowledge Graph (KG) and its variant of ontology have been widely used for\nknowledge representation, and have shown to be quite effective in augmenting\nZero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all\nneglect the intrinsic complexity of inter-class relationships represented in\nKGs. One typical feature is that a class is often related to other classes in\ndifferent semantic aspects. In this paper, we focus on ontologies for\naugmenting ZSL, and propose to learn disentangled ontology embeddings guided by\nontology properties to capture and utilize more fine-grained class\nrelationships in different aspects. We also contribute a new ZSL framework\nnamed DOZSL, which contains two new ZSL solutions based on generative models\nand graph propagation models, respectively, for effectively utilizing the\ndisentangled ontology embeddings. Extensive evaluations have been conducted on\nfive benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot\nKG completion (ZS-KGC). DOZSL often achieves better performance than the\nstate-of-the-art, and its components have been verified by ablation studies and\ncase studies. Our codes and datasets are available at\nhttps://github.com/zjukg/DOZSL.",
        "Task": "They address two distinct zero-shot tasks: image classification (predicting unseen classes) and knowledge graph completion (predicting missing triples, i.e., link prediction) on separate datasets. The first involves classifying new object categories in images, while the second involves inferring unseen relations in knowledge graphs.",
        "Data_type": "They handle image data (via CNN-extracted features) for classification and multi-relational graph-structured data (ontology-based node and edge information) for knowledge graph completion. These datasets include class hierarchies, entity-relation triples, and textual or attribute-level annotations, enabling zero-shot tasks in both image and KG domains.",
        "Data_domain": "The paper uses datasets from standard benchmarks in zero-shot image classification (e.g., Animals with Attributes, ImageNet-based sets) and from large-scale knowledge graphs (e.g., NELL, Wikidata). These data span real-world visual domains and relational facts for ontology-augmented tasks.",
        "ML_phase": "They propose a two-stage pipeline where the first stage learns disentangled class embeddings via property-guided neighborhood aggregation and triple scoring, and the second stage integrates these embeddings into two separate zero-shot learning frameworks (a GAN-based generative model and a GCN-based propagation model). The overall training procedure includes property-specific component extraction, adversarial generation of sample features or feature propagation through semantic graphs, and supervised optimization with seen-class data."
    },
    "kdd22/2206.08646v1.json": {
        "title": "Scalable Differentially Private Clustering via Hierarchically Separated  Trees",
        "abs": "We study the private $k$-median and $k$-means clustering problem in $d$\ndimensional Euclidean space. By leveraging tree embeddings, we give an\nefficient and easy to implement algorithm, that is empirically competitive with\nstate of the art non private methods. We prove that our method computes a\nsolution with cost at most $O(d^{3/2}\\log n)\\cdot OPT + O(k d^2 \\log^2 n /\n\\epsilon^2)$, where $\\epsilon$ is the privacy guarantee. (The dimension term,\n$d$, can be replaced with $O(\\log k)$ using standard dimension reduction\ntechniques.) Although the worst-case guarantee is worse than that of state of\nthe art private clustering methods, the algorithm we propose is practical, runs\nin near-linear, $\\tilde{O}(nkd)$, time and scales to tens of millions of\npoints. We also show that our method is amenable to parallelization in\nlarge-scale distributed computing environments. In particular we show that our\nprivate algorithms can be implemented in logarithmic number of MPC rounds in\nthe sublinear memory regime. Finally, we complement our theoretical analysis\nwith an empirical evaluation demonstrating the algorithm's efficiency and\naccuracy in comparison to other privacy clustering baselines.",
        "Task": "They tackle unsupervised geometric clustering, specifically k-median and k-means, under differential privacy. They evaluate performance on large-scale real-world datasets, measuring clustering costs.",
        "Data_type": "They handle real-valued point sets in d-dimensional Euclidean space. The approach also supports dimension-reduced inputs (e.g., via Johnson-Lindenstrauss) leading to O(log k)-dimensional embeddings.",
        "Data_domain": "They use datasets from the UCI Repository, covering diverse real-world domains such as image-based (skin), NASA shuttle data (shuttle), forest cover type classification (covtype), and high-energy physics (higgs). One synthetic dataset is also included for visualization purposes.",
        "ML_phase": "They position their work in the training stage of an unsupervised clustering pipeline, where a private data representation is constructed via randomly-shifted quadtrees. The approach then applies a dynamic program on these tree-structured counts and supports a massively parallel training implementation to finalize the cluster assignments."
    },
    "kdd22/2206.01379v1.json": {
        "title": "Instant Graph Neural Networks for Dynamic Graphs",
        "abs": "Graph Neural Networks (GNNs) have been widely used for modeling\ngraph-structured data. With the development of numerous GNN variants, recent\nyears have witnessed groundbreaking results in improving the scalability of\nGNNs to work on static graphs with millions of nodes. However, how to instantly\nrepresent continuous changes of large-scale dynamic graphs with GNNs is still\nan open problem. Existing dynamic GNNs focus on modeling the periodic evolution\nof graphs, often on a snapshot basis. Such methods suffer from two drawbacks:\nfirst, there is a substantial delay for the changes in the graph to be\nreflected in the graph representations, resulting in losses on the model's\naccuracy; second, repeatedly calculating the representation matrix on the\nentire graph in each snapshot is predominantly time-consuming and severely\nlimits the scalability. In this paper, we propose Instant Graph Neural Network\n(InstantGNN), an incremental computation approach for the graph representation\nmatrix of dynamic graphs. Set to work with dynamic graphs with the edge-arrival\nmodel, our method avoids time-consuming, repetitive computations and allows\ninstant updates on the representation and instant predictions. Graphs with\ndynamic structures and dynamic attributes are both supported. The upper bounds\nof time complexity of those updates are also provided. Furthermore, our method\nprovides an adaptive training strategy, which guides the model to retrain at\nmoments when it can make the greatest performance gains. We conduct extensive\nexperiments on several real-world and synthetic datasets. Empirical results\ndemonstrate that our model achieves state-of-the-art accuracy while having\norders-of-magnitude higher efficiency than existing methods.",
        "Task": "They address node-level multi-class classification on dynamic graphs. Their experiments evaluate how well the method maintains accurate node labels over time as the graph structure and features evolve.",
        "Data_type": "They handle large-scale graph-structured data with node features derived from text (e.g., BERT-based embeddings) and evolving edges over time. Their experiments cover both real-world (Arxiv, Products, Papers100M, Aminer) and synthetic SBM graphs, where node labels and features can also dynamically change.",
        "Data_domain": "They use large-scale real-world graphs from academic citation networks, e-commerce co-purchasing networks, and an academic coauthor network, plus synthetic graphs generated via a stochastic block model. These encompass both static and dynamically evolving node labels to reflect real-world and simulated graph changes.",
        "ML_phase": "They introduce a propagation-first framework that separates feature propagation from the neural prediction module, allowing scalable mini-batch training and incremental updates. In particular, the paper devises a dynamic pipeline where newly arrived graph events trigger local residual adjustments in the representation vectors, avoiding full retraining and ensuring efficiency on large-scale dynamic graphs."
    },
    "kdd22/2208.06129v1.json": {
        "title": "Multiplex Heterogeneous Graph Convolutional Network",
        "abs": "Heterogeneous graph convolutional networks have gained great popularity in\ntackling various network analytical tasks on heterogeneous network data,\nranging from link prediction to node classification. However, most existing\nworks ignore the relation heterogeneity with multiplex network between\nmulti-typed nodes and different importance of relations in meta-paths for node\nembedding, which can hardly capture the heterogeneous structure signals across\ndifferent relations. To tackle this challenge, this work proposes a Multiplex\nHeterogeneous Graph Convolutional Network (MHGCN) for heterogeneous network\nembedding. Our MHGCN can automatically learn the useful heterogeneous meta-path\ninteractions of different lengths in multiplex heterogeneous networks through\nmulti-layer convolution aggregation. Additionally, we effectively integrate\nboth multi-relation structural signals and attribute semantics into the learned\nnode embeddings with both unsupervised and semi-supervised learning paradigms.\nExtensive experiments on five real-world datasets with various network\nanalytical tasks demonstrate the significant superiority of MHGCN against\nstate-of-the-art embedding baselines in terms of all evaluation metrics.",
        "Task": "They address link prediction and node classification on multiplex heterogeneous networks. Both tasks are evaluated using multiple real-world datasets (e.g., Alibaba, Amazon, IMDB, AMiner, DBLP) to assess predictive performance.",
        "Data_type": "They focus on attributed multiplex heterogeneous networks encompassing multi-type nodes, multiple edge relations, and textual node attributes. In particular, the data is node-centric with each node carrying a feature vector (e.g., text-based) while edges capture diverse relational links.",
        "Data_domain": "The experimental evaluations involve five real-world graph datasets from both e-commerce (Alibaba, Amazon) and academic (AMiner, DBLP) domains, as well as the entertainment domain (IMDB). These datasets encompass diverse entity types such as users, products, authors, papers, and movies to represent multiplex heterogeneous networks.",
        "ML_phase": "They first decouple the attributed multiplex heterogeneous network into sub-graphs for each edge type and aggregate them (with learnable weights) into a unified adjacency matrix. Then, they apply a multi-layer (simplified) GCN framework—under either unsupervised or semi-supervised settings—to iteratively capture meta-path interactions, optimize learnable parameters (including relation weights), and produce the final node embeddings."
    },
    "kdd22/2206.04361v1.json": {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "abs": "Graph Neural Networks (GNNs) have achieved great success in various graph\nmining tasks.However, drastic performance degradation is always observed when a\nGNN is stacked with many layers. As a result, most GNNs only have shallow\narchitectures, which limits their expressive power and exploitation of deep\nneighborhoods.Most recent studies attribute the performance degradation of deep\nGNNs to the \\textit{over-smoothing} issue. In this paper, we disentangle the\nconventional graph convolution operation into two independent operations:\n\\textit{Propagation} (\\textbf{P}) and \\textit{Transformation}\n(\\textbf{T}).Following this, the depth of a GNN can be split into the\npropagation depth ($D_p$) and the transformation depth ($D_t$). Through\nextensive experiments, we find that the major cause for the performance\ndegradation of deep GNNs is the \\textit{model degradation} issue caused by\nlarge $D_t$ rather than the \\textit{over-smoothing} issue mainly caused by\nlarge $D_p$. Further, we present \\textit{Adaptive Initial Residual} (AIR), a\nplug-and-play module compatible with all kinds of GNN architectures, to\nalleviate the \\textit{model degradation} issue and the \\textit{over-smoothing}\nissue simultaneously. Experimental results on six real-world datasets\ndemonstrate that GNNs equipped with AIR outperform most GNNs with shallow\narchitectures owing to the benefits of both large $D_p$ and $D_t$, while the\ntime costs associated with AIR can be ignored.",
        "Task": "They address a node-level classification task, using semi-supervised node classification experiments on multiple real-world graph datasets (Cora, Citeseer, PubMed, and OGBN). They evaluate how model depth impacts performance on these node prediction benchmarks.",
        "Data_type": "They handle graph-structured data with node features derived from textual attributes (e.g., word frequencies) in homogeneous citation networks and large-scale co-purchasing or citation graphs. The focus is on node classification in these graphs, with both small (Cora, Citeseer, PubMed) and very large datasets (ogbn-arxiv, ogbn-products, ogbn-papers100M).",
        "Data_domain": "The datasets come from academic citation networks (Cora, Citeseer, PubMed, ogbn-arxiv, ogbn-papers100M) and an e-commerce co-purchasing network (ogbn-products). These domains represent scholarly publications and online retail transactions.",
        "ML_phase": "They propose a modular architecture design in the GNN pipeline, disentangling propagation and transformation layers to isolate depth effects. An adaptive residual framework is introduced to facilitate training deeper models by mitigating over-smoothing and model degradation."
    },
    "kdd22/2212.03306v1.json": {
        "title": "ERNet: Unsupervised Collective Extraction and Registration in  Neuroimaging Data",
        "abs": "Brain extraction and registration are important preprocessing steps in\nneuroimaging data analysis, where the goal is to extract the brain regions from\nMRI scans (i.e., extraction step) and align them with a target brain image\n(i.e., registration step). Conventional research mainly focuses on developing\nmethods for the extraction and registration tasks separately under supervised\nsettings. The performance of these methods highly depends on the amount of\ntraining samples and visual inspections performed by experts for error\ncorrection. However, in many medical studies, collecting voxel-level labels and\nconducting manual quality control in high-dimensional neuroimages (e.g., 3D\nMRI) are very expensive and time-consuming. Moreover, brain extraction and\nregistration are highly related tasks in neuroimaging data and should be solved\ncollectively. In this paper, we study the problem of unsupervised collective\nextraction and registration in neuroimaging data. We propose a unified\nend-to-end framework, called ERNet (Extraction-Registration Network), to\njointly optimize the extraction and registration tasks, allowing feedback\nbetween them. Specifically, we use a pair of multi-stage extraction and\nregistration modules to learn the extraction mask and transformation, where the\nextraction network improves the extraction accuracy incrementally and the\nregistration network successively warps the extracted image until it is\nwell-aligned with the target image. Experiment results on real-world datasets\nshow that our proposed method can effectively improve the performance on\nextraction and registration tasks in neuroimaging data. Our code and data can\nbe found at https://github.com/ERNetERNet/ERNet",
        "Task": "They tackle two core machine learning tasks on 3D brain MRI data: an unsupervised segmentation task (brain/skull extraction) and an unsupervised registration task (image alignment). Multiple public 3D MRI datasets (LPBA40, CC359, IBSR) are used to evaluate extraction (segmentation) accuracy and registration performance.",
        "Data_type": "They work with 3D T1-weighted volumetric brain MRI scans containing raw anatomical data and labeled brain masks or segmentations. Datasets include multiple public neuroimaging collections, all focusing on voxel-wise image information.",
        "Data_domain": "These experiments utilize T1-weighted 3D brain MRI scans drawn from publicly available neuroimaging datasets. The domain is medical imaging, specifically focusing on structural brain scans for neuroimaging studies.",
        "ML_phase": "They introduce an end-to-end, unsupervised pipeline that first applies a multi-stage 3D U-Net-based extraction module to iteratively remove non-brain tissues, then employs a multi-stage CNN registration module to predict affine transformations. The entire model is trained without labels, using differentiable spatial transformations and a similarity-based loss to jointly optimize both brain extraction and alignment."
    },
    "kdd22/2206.10581v1.json": {
        "title": "Nimble GNN Embedding with Tensor-Train Decomposition",
        "abs": "This paper describes a new method for representing embedding tables of graph\nneural networks (GNNs) more compactly via tensor-train (TT) decomposition. We\nconsider the scenario where (a) the graph data that lack node features, thereby\nrequiring the learning of embeddings during training; and (b) we wish to\nexploit GPU platforms, where smaller tables are needed to reduce host-to-GPU\ncommunication even for large-memory GPUs. The use of TT enables a compact\nparameterization of the embedding, rendering it small enough to fit entirely on\nmodern GPUs even for massive graphs. When combined with judicious schemes for\ninitialization and hierarchical graph partitioning, this approach can reduce\nthe size of node embedding vectors by 1,659 times to 81,362 times on large\npublicly available benchmark datasets, achieving comparable or better accuracy\nand significant speedups on multi-GPU systems. In some cases, our model without\nexplicit node features on input can even match the accuracy of models that use\nnode features.",
        "Task": "They focus on node-level classification tasks for large-scale graphs, evaluating performance on standard OGB node property prediction datasets.",
        "Data_type": "They focus on large-scale homogeneous graph-structured data with up to billions of nodes and edges, potentially including textual node attributes. The experiments span graphs from the Open Graph Benchmark with dimensions reaching over 100 million nodes.",
        "Data_domain": "They evaluate on large-scale graph datasets from the Open Graph Benchmark, including academic citation networks (e.g., ogbn-arxiv, ogbn-papers100M) and an e-commerce co-purchasing network (ogbn-products).",
        "ML_phase": "They reorder node IDs by hierarchical graph partitioning to align topological homophily with tensor-train–based compressed embeddings. The training pipeline incorporates orthogonal or TTM-based initialization into standard GNNs (e.g., GCN, GraphSage, GAT), enabling end-to-end updates on GPU or CPU–GPU configurations."
    },
    "kdd22/2206.12327v1.json": {
        "title": "Source Localization of Graph Diffusion via Variational Autoencoders for  Graph Inverse Problems",
        "abs": "Graph diffusion problems such as the propagation of rumors, computer viruses,\nor smart grid failures are ubiquitous and societal. Hence it is usually crucial\nto identify diffusion sources according to the current graph diffusion\nobservations. Despite its tremendous necessity and significance in practice,\nsource localization, as the inverse problem of graph diffusion, is extremely\nchallenging as it is ill-posed: different sources may lead to the same graph\ndiffusion patterns. Different from most traditional source localization\nmethods, this paper focuses on a probabilistic manner to account for the\nuncertainty of different candidate sources. Such endeavors require overcoming\nchallenges including 1) the uncertainty in graph diffusion source localization\nis hard to be quantified; 2) the complex patterns of the graph diffusion\nsources are difficult to be probabilistically characterized; 3) the\ngeneralization under any underlying diffusion patterns is hard to be imposed.\nTo solve the above challenges, this paper presents a generic framework: Source\nLocalization Variational AutoEncoder (SL-VAE) for locating the diffusion\nsources under arbitrary diffusion patterns. Particularly, we propose a\nprobabilistic model that leverages the forward diffusion estimation model along\nwith deep generative models to approximate the diffusion source distribution\nfor quantifying the uncertainty. SL-VAE further utilizes prior knowledge of the\nsource-observation pairs to characterize the complex patterns of diffusion\nsources by a learned generative prior. Lastly, a unified objective that\nintegrates the forward diffusion estimation model is derived to enforce the\nmodel to generalize under arbitrary diffusion patterns. Extensive experiments\nare conducted on 7 real-world datasets to demonstrate the superiority of SL-VAE\nin reconstructing the diffusion sources by excelling other methods on average\n20% in AUC score.",
        "Task": "This paper addresses a node-level classification task for source localization in graph diffusion, where each node is classified as a diffusion source or not. They evaluate performance on multiple real-world networks under various diffusion patterns (e.g., SI, SIR) and real-world scenarios.",
        "Data_type": "They work with graph-structured data where each node and edge represents the network’s topology and connectivity. The method accounts for node-level infection indicators but does not incorporate additional textual or multi-modal features.",
        "Data_domain": "The paper leverages multiple real-world network datasets spanning social media (Digg, Memetracker), academic citation networks (Cora-ML), collaboration networks (Jazz, Network Science), and infrastructure networks (Power Grid, Karate club). These datasets capture diverse domains where information diffusion occurs, including social, academic, and utility networks.",
        "ML_phase": "They first prepare node infection patterns either via epidemic simulations or real cascade observations, then feed these source–observation pairs into a variational autoencoder that models source distributions. The encoder–decoder VAE is jointly trained with a chosen forward diffusion model under a unified ELBO-based objective (with monotonic constraints), and inference is done via gradient-based optimization to locate unknown diffusion sources."
    },
    "kdd22/2308.06714v1.json": {
        "title": "Learning on Graphs with Out-of-Distribution Nodes",
        "abs": "Graph Neural Networks (GNNs) are state-of-the-art models for performing\nprediction tasks on graphs. While existing GNNs have shown great performance on\nvarious tasks related to graphs, little attention has been paid to the scenario\nwhere out-of-distribution (OOD) nodes exist in the graph during training and\ninference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes\nwith labels unseen from the training set. Since a lot of networks are\nautomatically constructed by programs, real-world graphs are often noisy and\nmay contain nodes from unknown distributions. In this work, we define the\nproblem of graph learning with out-of-distribution nodes. Specifically, we aim\nto accomplish two tasks: 1) detect nodes which do not belong to the known\ndistribution and 2) classify the remaining nodes to be one of the known\nclasses. We demonstrate that the connection patterns in graphs are informative\nfor outlier detection, and propose Out-of-Distribution Graph Attention Network\n(OODGAT), a novel GNN model which explicitly models the interaction between\ndifferent kinds of nodes and separate inliers from outliers during feature\npropagation. Extensive experiments show that OODGAT outperforms existing\noutlier detection methods by a large margin, while being better or comparable\nin terms of in-distribution classification.",
        "Task": "They tackle a semi-supervised node-level classification task where the graph may contain nodes whose labels are not in the training set (i.e., out-of-distribution nodes). Corresponding datasets are provided to evaluate both the accuracy of in-distribution classification and the effectiveness of outlier detection.",
        "Data_type": "They handle homogeneous graph-structured data, where each node is associated with textual or numeric attributes (e.g., citation features or product/user metadata) and edges reflect adjacency relationships. The paper’s datasets include citation graphs, co-purchase networks, and social graphs, with no mention of multi-modal inputs or chemical sequences.",
        "Data_domain": "They use multiple real-world graph datasets, including academic citation networks (Cora, Citeseer, Pubmed, CoauthorCS), e-commerce co-purchase networks (Amazon-Computers, Amazon-Photo), a music social network (LastFMAsia), and wiki pages (Wiki-CS). These datasets span academic, e-commerce, social, and knowledge-based domains.",
        "ML_phase": "They first split the graph into labeled in-distribution nodes and unlabeled nodes (which may include out-of-distribution samples), then feed the entire graph into a transductive GNN-based pipeline. The proposed OODGAT architecture employs a multi-head attention mechanism and an embedded binary classifier for OOD scoring, training end-to-end with cross-entropy plus consistency, entropy, and discrepancy losses in a semi-supervised framework."
    },
    "kdd22/2207.02590v1.json": {
        "title": "MetroGAN: Simulating Urban Morphology with Generative Adversarial  Network",
        "abs": "Simulating urban morphology with location attributes is a challenging task in\nurban science. Recent studies have shown that Generative Adversarial Networks\n(GANs) have the potential to shed light on this task. However, existing\nGAN-based models are limited by the sparsity of urban data and instability in\nmodel training, hampering their applications. Here, we propose a GAN framework\nwith geographical knowledge, namely Metropolitan GAN (MetroGAN), for urban\nmorphology simulation. We incorporate a progressive growing structure to learn\nhierarchical features and design a geographical loss to impose the constraints\nof water areas. Besides, we propose a comprehensive evaluation framework for\nthe complex structure of urban systems. Results show that MetroGAN outperforms\nthe state-of-the-art urban simulation methods by over 20% in all metrics.\nInspiringly, using physical geography features singly, MetroGAN can still\ngenerate shapes of the cities. These results demonstrate that MetroGAN solves\nthe instability problem of previous urban simulation GANs and is generalizable\nto deal with various urban attributes.",
        "Task": "They formulate a conditional image-to-image generation task, using physical geography attributes (e.g., DEM, water) and nighttime lights as inputs to produce urban built-up area maps. A large-scale dataset of over 10,000 global cities is collected for training and evaluating the quality of the generated city morphology.",
        "Data_type": "They use 2D raster images (DEM, water masks, nighttime lights, and binary built-up areas) to represent global cities at various scales. The input data spans multiple years, capturing spatial attributes of terrain and urbanization.",
        "Data_domain": "All data come from a global geospatial domain capturing urban morphological and physical geography features. Specifically, the dataset includes built-up area, digital elevation models, water-area masks, nighttime lights, and population-based measures for worldwide cities.",
        "ML_phase": "They construct a global city dataset (DEM, water area, nighttime lights, built-up area), resize and filter it, and then feed it into a progressive-growing U-Net-based conditional GAN. The model uses L1, adversarial, and geographical constraint losses, trained with a resolution-by-resolution progression to stabilize learning and capture multi-scale urban morphology."
    },
    "kdd22/2206.07247v2.json": {
        "title": "Fair Ranking as Fair Division: Impact-Based Individual Fairness in  Ranking",
        "abs": "Rankings have become the primary interface in two-sided online markets. Many\nhave noted that the rankings not only affect the satisfaction of the users\n(e.g., customers, listeners, employers, travelers), but that the position in\nthe ranking allocates exposure -- and thus economic opportunity -- to the\nranked items (e.g., articles, products, songs, job seekers, restaurants,\nhotels). This has raised questions of fairness to the items, and most existing\nworks have addressed fairness by explicitly linking item exposure to item\nrelevance. However, we argue that any particular choice of such a link function\nmay be difficult to defend, and we show that the resulting rankings can still\nbe unfair. To avoid these shortcomings, we develop a new axiomatic approach\nthat is rooted in principles of fair division. This not only avoids the need to\nchoose a link function, but also more meaningfully quantifies the impact on the\nitems beyond exposure. Our axioms of envy-freeness and dominance over uniform\nranking postulate that for a fair ranking policy every item should prefer their\nown rank allocation over that of any other item, and that no item should be\nactively disadvantaged by the rankings. To compute ranking policies that are\nfair according to these axioms, we propose a new ranking objective related to\nthe Nash Social Welfare. We show that the solution has guarantees regarding its\nenvy-freeness, its dominance over uniform rankings for every item, and its\nPareto optimality. In contrast, we show that conventional exposure-based\nfairness can produce large amounts of envy and have a highly disparate impact\non the items. Beyond these theoretical results, we illustrate empirically how\nour framework controls the trade-off between impact-based individual item\nfairness and user utility.",
        "Task": "They address a ranking task, aiming to learn stochastic ranking policies that satisfy fairness constraints for items while maximizing user utility. Evaluation is conducted on both synthetic data and real-world extreme classification datasets (Delicious, Wiki10-31K).",
        "Data_type": "They focus on ranking tasks with user–item pairs, where relevance is numeric (e.g., synthetic settings) or derived from textual datasets (e.g., Delicious and Wiki10-31K). Specifically, items are treated as labels, and users as data instances, yielding user–item (label) relevance matrices for fair ranking.",
        "Data_domain": "They use synthetic two-sided market data (e.g., hiring and news platforms) with controlled popularity bias and real-world multi-label classification datasets (Delicious and Wiki10-31K) representing user–item interactions. This allows them to evaluate fairness across domains such as booking services, job-search platforms, and academic recommendations.",
        "ML_phase": "They introduce a convex optimization framework that represents rankings via doubly stochastic matrices and maximizes a Nash Social Welfare–inspired objective. Synthetic data are generated with controlled popularity biases, and logistic regression–based relevance predictions are used on real-world datasets, with the ranking policy trained by solving the resulting convex program."
    },
    "kdd22/2205.14228v2.json": {
        "title": "Sparse Conditional Hidden Markov Model for Weakly Supervised Named  Entity Recognition",
        "abs": "Weakly supervised named entity recognition methods train label models to\naggregate the token annotations of multiple noisy labeling functions (LFs)\nwithout seeing any manually annotated labels. To work well, the label model\nneeds to contextually identify and emphasize well-performed LFs while\ndown-weighting the under-performers. However, evaluating the LFs is challenging\ndue to the lack of ground truths. To address this issue, we propose the sparse\nconditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire\nemission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating\nits diagonal elements, which are considered as the reliability scores of the\nLFs. The sparse scores are then expanded to the full-fledged emission matrix\nwith pre-defined expansion functions. We also augment the emission with\nweighted XOR scores, which track the probabilities of an LF observing incorrect\nentities. Sparse-CHMM is optimized through unsupervised learning with a\nthree-stage training pipeline that reduces the training difficulty and prevents\nthe model from falling into local optima. Compared with the baselines in the\nWrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on\nfive comprehensive datasets. Experiments show that each component of\nSparse-CHMM is effective, and the estimated LF reliabilities strongly correlate\nwith true LF F1 scores.",
        "Task": "They tackle a sequence labeling task for named entity recognition (NER) to identify token-level entity labels in text. They evaluate their approach on five NER datasets (CoNLL 2003, NCBI-Disease, BC5CDR, LaptopReview, and OntoNotes 5.0).",
        "Data_type": "They address sequence labeling on purely textual data, focusing on named entity recognition across multiple domains (e.g., news, biomedical, and product reviews). The method processes token-level annotations derived from weak supervision sources rather than handling multimodal or graph-structured inputs.",
        "Data_domain": "The paper’s datasets originate from diverse textual sources, including news articles (Reuters), product reviews (laptop reviews), and biomedical/chemical research literature (PubMed), as well as a large multi-domain corpus (OntoNotes). They thus encompass both general-domain and specialized scientific/academic domains.",
        "ML_phase": "They propose a label model pipeline that first collects weak annotations via multiple labeling functions, then constructs sentence-level transition and emission matrices predicted from BERT embeddings. A three-stage training procedure (initialization, emission addon refinement, and transition fine-tuning) uses an EM algorithm with Dirichlet sampling to aggregate labels without requiring ground-truth supervision."
    },
    "kdd22/2206.01702v2.json": {
        "title": "Scalar is Not Enough: Vectorization-based Unbiased Learning to Rank",
        "abs": "Unbiased learning to rank (ULTR) aims to train an unbiased ranking model from\nbiased user click logs. Most of the current ULTR methods are based on the\nexamination hypothesis (EH), which assumes that the click probability can be\nfactorized into two scalar functions, one related to ranking features and the\nother related to bias factors. Unfortunately, the interactions among features,\nbias factors and clicks are complicated in practice, and usually cannot be\nfactorized in this independent way. Fitting click data with EH could lead to\nmodel misspecification and bring the approximation error.\n  In this paper, we propose a vector-based EH and formulate the click\nprobability as a dot product of two vector functions. This solution is complete\ndue to its universality in fitting arbitrary click functions. Based on it, we\npropose a novel model named Vectorization to adaptively learn the relevance\nembeddings and sort documents by projecting embeddings onto a base vector.\nExtensive experiments show that our method significantly outperforms the\nstate-of-the-art ULTR methods on complex real clicks as well as simple\nsimulated clicks.",
        "Task": "They address an unbiased learning-to-rank task using large-scale user click data. They empirically evaluate the ranking performance on various benchmark datasets (e.g., Yahoo! and Istella-S).",
        "Data_type": "They handle standard numeric feature vectors representing query-document pairs (e.g., up to 700 features) for learning to rank. The approach employs a flexible embedding dimension d (e.g., 1 to 5 in experiments) to capture complex interactions in click data.",
        "Data_domain": "They draw on publicly available web search ranking datasets (Yahoo! LETOR and Istella-S) and real-world query logs (TianGong-ST) originating from a commercial search engine. In all cases, the data domain is large-scale search engine logs and related ranking features reflecting typical web information retrieval scenarios.",
        "ML_phase": "They propose a two-stage training pipeline that first learns a relevance model and an observation model jointly from (semi-)synthetic or real click data, then fits a separate “base model” to estimate observation embedding distributions. During inference, the learned relevance embeddings are projected onto a computed base vector for final ranking."
    },
    "kdd22/2206.08743v1.json": {
        "title": "Learning Fair Representation via Distributional Contrastive  Disentanglement",
        "abs": "Learning fair representation is crucial for achieving fairness or debiasing\nsensitive information. Most existing works rely on adversarial representation\nlearning to inject some invariance into representation. However, adversarial\nlearning methods are known to suffer from relatively unstable training, and\nthis might harm the balance between fairness and predictiveness of\nrepresentation. We propose a new approach, learning FAir Representation via\ndistributional CONtrastive Variational AutoEncoder (FarconVAE), which induces\nthe latent space to be disentangled into sensitive and nonsensitive parts. We\nfirst construct the pair of observations with different sensitive attributes\nbut with the same labels. Then, FarconVAE enforces each non-sensitive latent to\nbe closer, while sensitive latents to be far from each other and also far from\nthe non-sensitive latent by contrasting their distributions. We provide a new\ntype of contrastive loss motivated by Gaussian and Student-t kernels for\ndistributional contrastive learning with theoretical analysis. Besides, we\nadopt a new swap-reconstruction loss to boost the disentanglement further.\nFarconVAE shows superior performance on fairness, pretrained model debiasing,\nand domain generalization tasks from various modalities, including tabular,\nimage, and text.",
        "Task": "They focus on classification tasks to achieve fair predictions, debias pretrained models, and tackle domain generalization. Performance is evaluated on tabular (Adult, German), image (Extended YaleB, cMNIST, Waterbirds), and text (BERT) datasets.",
        "Data_type": "They empirically validate on tabular data with up to 20 attributes (Adult, German), image data spanning 28×28 to larger resolutions (Extended YaleB, cMNIST, Waterbirds), and textual inputs from BERT embeddings. No experiments are provided for graph-structured data or other modalities such as SMILES.",
        "Data_domain": "The paper leverages datasets from tabular economic/financial sources (Adult, German), biometric imagery (Extended YaleB), synthetic imagery (cMNIST), environmental imagery (Waterbirds), and text corpora (BERT). These multi-domain datasets encompass economic, biometric, and language data to assess fairness and out-of-distribution robustness.",
        "ML_phase": "FarconVAE operates in the representation learning phase of the ML pipeline, using an encoder-decoder architecture that splits latent variables into sensitive and non-sensitive components. Its training framework combines a standard variational autoencoder reconstruction objective with a distributional contrastive loss and swap-based regularization, leveraging paired inputs (original vs. counterfactual) to disentangle and exclude unwanted information."
    },
    "kdd22/2112.15089v2.json": {
        "title": "Causal Attention for Interpretable and Generalizable Graph  Classification",
        "abs": "In graph classification, attention and pooling-based graph neural networks\n(GNNs) prevail to extract the critical features from the input graph and\nsupport the prediction. They mostly follow the paradigm of learning to attend,\nwhich maximizes the mutual information between the attended graph and the\nground-truth label. However, this paradigm makes GNN classifiers recklessly\nabsorb all the statistical correlations between input features and labels in\nthe training data, without distinguishing the causal and noncausal effects of\nfeatures. Instead of underscoring the causal features, the attended graphs are\nprone to visit the noncausal features as the shortcut to predictions. Such\nshortcut features might easily change outside the training distribution,\nthereby making the GNN classifiers suffer from poor generalization. In this\nwork, we take a causal look at the GNN modeling for graph classification. With\nour causal assumption, the shortcut feature serves as a confounder between the\ncausal feature and prediction. It tricks the classifier to learn spurious\ncorrelations that facilitate the prediction in in-distribution (ID) test\nevaluation, while causing the performance drop in out-of-distribution (OOD)\ntest data. To endow the classifier with better interpretation and\ngeneralization, we propose the Causal Attention Learning (CAL) strategy, which\ndiscovers the causal patterns and mitigates the confounding effect of\nshortcuts. Specifically, we employ attention modules to estimate the causal and\nshortcut features of the input graph. We then parameterize the backdoor\nadjustment of causal theory -- combine each causal feature with various\nshortcut features. It encourages the stable relationships between the causal\nestimation and prediction, regardless of the changes in shortcut parts and\ndistributions. Extensive experiments on synthetic and real-world datasets\ndemonstrate the effectiveness of CAL.",
        "Task": "They target a graph-level classification task using both synthetic and real-world graph datasets. Specifically, they evaluate their method on datasets such as TUDataset (MUTAG, NCI1, PROTEINS, COLLAB, IMDB-B, IMDB-M) and superpixel-based MNIST/CIFAR-10 to assess classification performance.",
        "Data_type": "They handle homogeneous graph-structured inputs defined by an N×N adjacency matrix and an N×F node-feature matrix, where F can represent numeric or categorical attributes (e.g., pixel intensities in superpixel graphs, molecular descriptors, or social node features). These graphs span domains such as molecules, social networks, and image superpixels, thereby accommodating various numeric node attributes across diverse datasets.",
        "Data_domain": "The datasets span synthetic graphs, biological (molecular) graphs, social network graphs, and superpixel-based image graphs (MNIST/CIFAR-10). These collectively cover chemical molecules, social networks, and image-based domains.",
        "ML_phase": "They generate synthetic datasets with varying biases and then feed graph inputs into a GNN that uses node- and edge-level attention modules to split each graph into causal and trivial subgraphs. The method’s training framework applies supervised and uniform loss terms separately to these subgraphs, and employs a random combination step (“backdoor adjustment”) to enforce invariant predictions against distribution shifts."
    },
    "kdd22/2206.11972v1.json": {
        "title": "Task-Adaptive Few-shot Node Classification",
        "abs": "Node classification is of great importance among various graph mining tasks.\nIn practice, real-world graphs generally follow the long-tail distribution,\nwhere a large number of classes only consist of limited labeled nodes. Although\nGraph Neural Networks (GNNs) have achieved significant improvements in node\nclassification, their performance decreases substantially in such a few-shot\nscenario. The main reason can be attributed to the vast generalization gap\nbetween meta-training and meta-test due to the task variance caused by\ndifferent node/class distributions in meta-tasks (i.e., node-level and\nclass-level variance). Therefore, to effectively alleviate the impact of task\nvariance, we propose a task-adaptive node classification framework under the\nfew-shot learning setting. Specifically, we first accumulate meta-knowledge\nacross classes with abundant labeled nodes. Then we transfer such knowledge to\nthe classes with limited labeled nodes via our proposed task-adaptive modules.\nIn particular, to accommodate the different node/class distributions among\nmeta-tasks, we propose three essential modules to perform \\emph{node-level},\n\\emph{class-level}, and \\emph{task-level} adaptations in each meta-task,\nrespectively. In this way, our framework can conduct adaptations to different\nmeta-tasks and thus advance the model generalization performance on meta-test\ntasks. Extensive experiments on four prevalent node classification datasets\ndemonstrate the superiority of our framework over the state-of-the-art\nbaselines. Our code is provided at https://github.com/SongW-SW/TENT.",
        "Task": "They tackle the few-shot node classification task, aiming to predict labels for nodes in a graph with limited labeled samples per class. Experimental validation is conducted on four real-world graph datasets (Amazon-E, DBLP, Cora-full, OGBN-arxiv).",
        "Data_type": "They handle homogeneous graph-structured data with node attributes that are predominantly textual/numeric feature vectors (e.g., dimensions ranging up to thousands, such as 128, 7,202, 8,669, and 8,710). Their experiments involve large-scale networks (up to hundreds of thousands of nodes) using these node features plus edge connections for few-shot node classification.",
        "Data_domain": "The datasets come from an e-commerce product network (Amazon-E) and academic citation networks (DBLP, Cora-full, OGBN-arxiv). Specifically, Amazon-E covers product-view relationships, while DBLP, Cora-full, and OGBN-arxiv focus on citation links among academic papers.",
        "ML_phase": "They first generate initial node embeddings from an entire graph via a base GNN, then construct class-ego subgraphs and adapt GNN parameters specifically per-class. Next, they integrate a mutual-information-based task-level adaptation and optimize all components in a meta-learning (episodic) training scheme to classify novel classes with limited labeled nodes."
    },
    "kdd22/2206.13980v1.json": {
        "title": "Label-enhanced Prototypical Network with Contrastive Learning for  Multi-label Few-shot Aspect Category Detection",
        "abs": "Multi-label aspect category detection allows a given review sentence to\ncontain multiple aspect categories, which is shown to be more practical in\nsentiment analysis and attracting increasing attention. As annotating large\namounts of data is time-consuming and labor-intensive, data scarcity occurs\nfrequently in real-world scenarios, which motivates multi-label few-shot aspect\ncategory detection. However, research on this problem is still in infancy and\nfew methods are available. In this paper, we propose a novel label-enhanced\nprototypical network (LPN) for multi-label few-shot aspect category detection.\nThe highlights of LPN can be summarized as follows. First, it leverages label\ndescription as auxiliary knowledge to learn more discriminative prototypes,\nwhich can retain aspect-relevant information while eliminating the harmful\neffect caused by irrelevant aspects. Second, it integrates with contrastive\nlearning, which encourages that the sentences with the same aspect label are\npulled together in embedding space while simultaneously pushing apart the\nsentences with different aspect labels. In addition, it introduces an adaptive\nmulti-label inference module to predict the aspect count in the sentence, which\nis simple yet effective. Extensive experimental results on three datasets\ndemonstrate that our proposed model LPN can consistently achieve\nstate-of-the-art performance.",
        "Task": "This paper addresses multi-label few-shot aspect category detection, a multi-label classification task in sentiment analysis. The authors evaluate their approach on three variants of the FewAsp dataset (FewAsp (single), FewAsp (multi), and FewAsp), measuring classification performance.",
        "Data_type": "They work with textual data (review sentences) that may contain multiple aspect categories (multi-label). The paper’s datasets come from user reviews (YelpAspect) and focus on few-shot multi-label classification of these textual samples.",
        "Data_domain": "The data used in this paper is drawn from user-generated Yelp reviews, a large-scale dataset relevant to social/review platforms. It primarily focuses on multi-domain aspect recommendation within restaurant and service contexts.",
        "ML_phase": "They construct episodic tasks for multi-label few-shot learning, where each episode includes an N-way K-shot support set and a query set, then employ BERT-based feature extraction and label text descriptions to form label-enhanced prototypes. During training, they combine cross-entropy on the label predictions with a contrastive loss to refine embeddings and use an adaptive multi-label inference module to predict the number of aspects per sentence."
    },
    "kdd22/2206.13746v1.json": {
        "title": "Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation  and Instance Generation",
        "abs": "We study the problem of few-shot Fine-grained Entity Typing (FET), where only\na few annotated entity mentions with contexts are given for each entity type.\nRecently, prompt-based tuning has demonstrated superior performance to standard\nfine-tuning in few-shot scenarios by formulating the entity type classification\ntask as a ''fill-in-the-blank'' problem. This allows effective utilization of\nthe strong language modeling capability of Pre-trained Language Models (PLMs).\nDespite the success of current prompt-based tuning approaches, two major\nchallenges remain: (1) the verbalizer in prompts is either manually designed or\nconstructed from external knowledge bases, without considering the target\ncorpus and label hierarchy information, and (2) current approaches mainly\nutilize the representation power of PLMs, but have not explored their\ngeneration power acquired through extensive general-domain pre-training. In\nthis work, we propose a novel framework for few-shot FET consisting of two\nmodules: (1) an entity type label interpretation module automatically learns to\nrelate type labels to the vocabulary by jointly leveraging few-shot instances\nand the label hierarchy, and (2) a type-based contextualized instance generator\nproduces new instances based on given instances to enlarge the training set for\nbetter generalization. On three benchmark datasets, our model outperforms\nexisting methods by significant margins. Code can be found at\nhttps://github.com/teapot123/Fine-Grained-Entity-Typing.",
        "Task": "They address a few-shot fine-grained entity typing classification task in text, where each entity mention is assigned a hierarchical label from a predefined set. Performance is evaluated on three benchmark datasets—OntoNotes, BBN, and Few-NERD.",
        "Data_type": "They handle purely textual data consisting of entity mentions embedded in sentences or short contexts. No multimodal, graph-structured, or molecular (SMILES/fingerprint) data are used.",
        "Data_domain": "The data is drawn from textual corpora, primarily news articles (e.g., Wall Street Journal) and Wikipedia. Specifically, the authors use three benchmark fine-grained entity typing datasets derived from OntoNotes (news/various text), BBN (Wall Street Journal), and Few-NERD (Wikipedia).",
        "ML_phase": "They integrate a prompt-based training pipeline with an automatic label interpretation module that learns a correlation matrix to map MLM predictions to hierarchical entity types. They further incorporate a type-based instance generator to produce new labeled examples, refining the model through iterative augmentation under hierarchical constraints."
    },
    "kdd22/2206.02115v1.json": {
        "title": "Learning Binarized Graph Representations with Multi-faceted Quantization  Reinforcement for Top-K Recommendation",
        "abs": "Learning vectorized embeddings is at the core of various recommender systems\nfor user-item matching. To perform efficient online inference, representation\nquantization, aiming to embed the latent features by a compact sequence of\ndiscrete numbers, recently shows the promising potentiality in optimizing both\nmemory and computation overheads. However, existing work merely focuses on\nnumerical quantization whilst ignoring the concomitant information loss issue,\nwhich, consequently, leads to conspicuous performance degradation. In this\npaper, we propose a novel quantization framework to learn Binarized Graph\nRepresentations for Top-K Recommendation (BiGeaR). BiGeaR introduces\nmulti-faceted quantization reinforcement at the pre-, mid-, and post-stage of\nbinarized representation learning, which substantially retains the\nrepresentation informativeness against embedding binarization. In addition to\nsaving the memory footprint, BiGeaR further develops solid online inference\nacceleration with bitwise operations, providing alternative flexibility for the\nrealistic deployment. The empirical results over five large real-world\nbenchmarks show that BiGeaR achieves about 22%~40% performance improvement over\nthe state-of-the-art quantization-based recommender system, and recovers about\n95%~102% of the performance capability of the best full-precision counterpart\nwith over 8x time and space reduction.",
        "Task": "They address a top-K recommendation (link-level) task on user–item datasets MovieLens, Gowalla, Pinterest, Yelp2018, and Amazon-Book. They measure ranking performance using metrics such as Recall@K and NDCG@K.",
        "Data_type": "They focus on user–item bipartite interaction graphs, where nodes represent users or items and edges indicate observed interactions. The data is homogeneous graph-structured (no explicit textual or multi-modal attributes) with embedding-based node representations derived from adjacency relations.",
        "Data_domain": "The data comes from five widely used real-world benchmarks: MovieLens (movie rating), Gowalla (location-based check-ins), Pinterest (image curation), Yelp2018 (local business reviews), and Amazon-Book (e-commerce). These heterogeneous platforms collectively capture large-scale user behavioral interactions in both social and e-commerce contexts.",
        "ML_phase": "They introduce a pipeline that first pre-trains full-precision node embeddings, then applies layer-wise binarization alongside inference distillation and gradient approximation to optimize a binarized recommendation model end-to-end. The final deployment relies on bitwise operations for fast inference."
    },
    "kdd22/2207.14338v1.json": {
        "title": "Self-Supervised Hypergraph Transformer for Recommender Systems",
        "abs": "Graph Neural Networks (GNNs) have been shown as promising solutions for\ncollaborative filtering (CF) with the modeling of user-item interaction graphs.\nThe key idea of existing GNN-based recommender systems is to recursively\nperform the message passing along the user-item interaction edge for refining\nthe encoded embeddings. Despite their effectiveness, however, most of the\ncurrent recommendation models rely on sufficient and high-quality training\ndata, such that the learned representations can well capture accurate user\npreference. User behavior data in many practical recommendation scenarios is\noften noisy and exhibits skewed distribution, which may result in suboptimal\nrepresentation performance in GNN-based models. In this paper, we propose SHT,\na novel Self-Supervised Hypergraph Transformer framework (SHT) which augments\nuser representations by exploring the global collaborative relationships in an\nexplicit way. Specifically, we first empower the graph neural CF paradigm to\nmaintain global collaborative effects among users and items with a hypergraph\ntransformer network. With the distilled global context, a cross-view generative\nself-supervised learning component is proposed for data augmentation over the\nuser-item interaction graph, so as to enhance the robustness of recommender\nsystems. Extensive experiments demonstrate that SHT can significantly improve\nthe performance over various state-of-the-art baselines. Further ablation\nstudies show the superior representation ability of our SHT recommendation\nframework in alleviating the data sparsity and noise issues. The source code\nand evaluation datasets are available at: https://github.com/akaxlh/SHT.",
        "Task": "They tackle a top-N recommendation task on user–item interaction graphs, aiming to predict and rank relevant items for each user. The experiments are conducted on real-world datasets (Yelp, Gowalla, Tmall) with metrics such as Recall and NDCG used to evaluate the ranking performance.",
        "Data_type": "They process user–item interaction data as graph-structured inputs, where users and items are treated as nodes connected by edges denoting interactions. Additionally, they adopt hypergraph representations with latent hyperedges to capture global relations across multiple nodes.",
        "Data_domain": "The authors utilize three real-world user–item interaction datasets spanning distinct domains: Yelp (a business-rating platform), Gowalla (a location-based social network), and Tmall (an e-commerce platform). These datasets collectively cover recommendation scenarios involving user ratings, geographic check-ins, and online shopping behaviors.",
        "ML_phase": "They construct a user–item interaction graph, normalize its adjacency matrix, and apply a two-layer GCN to obtain local topology-aware embeddings. A hypergraph transformer then refines global representations and is jointly trained with a primary pairwise ranking loss plus a self-augmented solidity-ranking objective."
    },
    "kdd22/2205.13066v1.json": {
        "title": "Semi-supervised Drifted Stream Learning with Short Lookback",
        "abs": "In many scenarios, 1) data streams are generated in real time; 2) labeled\ndata are expensive and only limited labels are available in the beginning; 3)\nreal-world data is not always i.i.d. and data drift over time gradually; 4) the\nstorage of historical streams is limited and model updating can only be\nachieved based on a very short lookback window. This learning setting limits\nthe applicability and availability of many Machine Learning (ML) algorithms. We\ngeneralize the learning task under such setting as a semi-supervised drifted\nstream learning with short lookback problem (SDSL). SDSL imposes two\nunder-addressed challenges on existing methods in semi-supervised learning,\ncontinuous learning, and domain adaptation: 1) robust pseudo-labeling under\ngradual shifts and 2) anti-forgetting adaptation with short lookback. To tackle\nthese challenges, we propose a principled and generic generation-replay\nframework to solve SDSL. The framework is able to accomplish: 1) robust\npseudo-labeling in the generation step; 2) anti-forgetting adaption in the\nreplay step. To achieve robust pseudo-labeling, we develop a novel pseudo-label\nclassification model to leverage supervised knowledge of previously labeled\ndata, unsupervised knowledge of new data, and, structure knowledge of invariant\nlabel semantics. To achieve adaptive anti-forgetting model replay, we propose\nto view the anti-forgetting adaptation task as a flat region search problem. We\npropose a novel minimax game-based replay objective function to solve the flat\nregion search problem and develop an effective optimization solver. Finally, we\npresent extensive experiments to demonstrate our framework can effectively\naddress the task of anti-forgetting learning in drifted streams with short\nlookback.",
        "Task": "They address a semi-supervised classification task on drifting data streams, with limited labeled data and short lookback windows. The authors evaluate classification accuracy across various real-world and synthetic datasets to measure performance under evolving distributions.",
        "Data_type": "They handle numeric tabular data ranging from 2-dimensional synthetic streams (e.g., UG_2C_2D) up to 500-dimensional real-world features (e.g., Spambase) for classification. The data arrives in a streaming manner, with gradually drifting distributions over time.",
        "Data_domain": "They employ four synthetic benchmark datasets simulating gradual distribution shifts (UG_2C_2D, UG_2C_3D, UG_2C_5D, MG_2C_2D) and four real-world classification datasets (Optdigits, Spam, Satimage, Twonorm) commonly used in academic research. Thus, the data domain centers on synthetic and academic classification tasks.",
        "ML_phase": "The paper proposes a “generation–replay” pipeline that first generates robust pseudo-labels by leveraging previously labeled data, unlabeled drifted data, and invariant class semantics, then uses these pseudo-labeled samples in a short lookback memory for continual model training. In the replay phase, a minimax-based flat-region approach is employed to adapt the model to new distributions while mitigating forgetting."
    },
    "kdd22/2205.11678v1.json": {
        "title": "Compressing Deep Graph Neural Networks via Adversarial Knowledge  Distillation",
        "abs": "Deep graph neural networks (GNNs) have been shown to be expressive for\nmodeling graph-structured data. Nevertheless, the over-stacked architecture of\ndeep graph models makes it difficult to deploy and rapidly test on mobile or\nembedded systems. To compress over-stacked GNNs, knowledge distillation via a\nteacher-student architecture turns out to be an effective technique, where the\nkey step is to measure the discrepancy between teacher and student networks\nwith predefined distance functions. However, using the same distance for graphs\nof various structures may be unfit, and the optimal distance formulation is\nhard to determine. To tackle these problems, we propose a novel Adversarial\nKnowledge Distillation framework for graph models named GraphAKD, which\nadversarially trains a discriminator and a generator to adaptively detect and\ndecrease the discrepancy. Specifically, noticing that the well-captured\ninter-node and inter-class correlations favor the success of deep GNNs, we\npropose to criticize the inherited knowledge from node-level and class-level\nviews with a trainable discriminator. The discriminator distinguishes between\nteacher knowledge and what the student inherits, while the student GNN works as\na generator and aims to fool the discriminator. To our best knowledge, GraphAKD\nis the first to introduce adversarial training to knowledge distillation in\ngraph domains. Experiments on node-level and graph-level classification\nbenchmarks demonstrate that GraphAKD improves the student performance by a\nlarge margin. The results imply that GraphAKD can precisely transfer knowledge\nfrom a complicated teacher GNN to a compact student GNN.",
        "Task": "They focus on node-level classification (e.g., on citation networks and social networks) and graph-level classification (e.g., on molecular property prediction). These tasks are evaluated using multiple real-world benchmarks ranging from small to large-scale graphs.",
        "Data_type": "They handle graph-structured data with textual node attributes, image-based features, and molecular graphs of varying scales (e.g., citation networks, social platforms, and chemical compounds). Node representations include bag-of-words or skip-gram embeddings for textual/image inputs and bond-based encodings for molecular data, encompassing both homogeneous and heterogeneous graph setups.",
        "Data_domain": "The paper uses graph data from multiple real-world sources, including academic citation networks (e.g., Cora, CiteSeer, PubMed), large-scale social networks (e.g., Flickr, Reddit, Yelp), e-commerce product co-purchasing networks (e.g., Amazon), and molecular property datasets.",
        "ML_phase": "They propose an adversarial teacher-student framework that first pretrains an over-parameterized teacher GNN, then transfers its representations and logits to a compact student via a topology-aware discriminator criticizing local node pairs and global summary representations. The pipeline iteratively updates the student (as a generator) and the discriminator, enabling compressed knowledge transfer for both node-level and graph-level classification."
    },
    "kdd22/2206.12705v1.json": {
        "title": "p-Meta: Towards On-device Deep Model Adaptation",
        "abs": "Data collected by IoT devices are often private and have a large diversity\nacross users. Therefore, learning requires pre-training a model with available\nrepresentative data samples, deploying the pre-trained model on IoT devices,\nand adapting the deployed model on the device with local data. Such an\non-device adaption for deep learning empowered applications demands data and\nmemory efficiency. However, existing gradient-based meta learning schemes fail\nto support memory-efficient adaptation. To this end, we propose p-Meta, a new\nmeta learning method that enforces structure-wise partial parameter updates\nwhile ensuring fast generalization to unseen tasks. Evaluations on few-shot\nimage classification and reinforcement learning tasks show that p-Meta not only\nimproves the accuracy but also substantially reduces the peak dynamic memory by\na factor of 2.5 on average compared to state-of-the-art few-shot adaptation\nmethods.",
        "Task": "They address few-shot classification (image classification) and few-shot reinforcement learning tasks. The corresponding datasets and environments include MiniImageNet, TieredImageNet, CUB for image classification, and 2D navigation plus Half-Cheetah MuJoCo for reinforcement learning.",
        "Data_type": "They focus on few-shot image classification tasks drawn from datasets such as MiniImageNet, TieredImageNet, and CUB, and also address reinforcement learning tasks involving 2D navigation and Half-Cheetah locomotion in MuJoCo. The data include image-based inputs for classification and continuous observational inputs for control settings.",
        "Data_domain": "The paper’s data come mainly from standard academic few-shot image classification benchmarks (MiniImageNet, TieredImageNet, CUB) and simulated reinforcement learning tasks in MuJoCo. These datasets provide diverse visual samples for meta-training and new unseen tasks, as well as control environments for testing policy adaptation.",
        "ML_phase": "The proposed method performs gradient-based meta-training in the cloud on diverse few-shot tasks, learning a backbone initialization and sparse layer/channel update rules. It then adapts this meta-trained model on-device via memory-efficient partial parameter updates (using meta attention, gradient accumulation, and group normalization), delivering fast personalization with minimal resource overhead."
    },
    "kdd22/2108.03348v3.json": {
        "title": "Global Self-Attention as a Replacement for Graph Convolution",
        "abs": "We propose an extension to the transformer neural network architecture for\ngeneral-purpose graph learning by adding a dedicated pathway for pairwise\nstructural information, called edge channels. The resultant framework - which\nwe call Edge-augmented Graph Transformer (EGT) - can directly accept, process\nand output structural information of arbitrary form, which is important for\neffective learning on graph-structured data. Our model exclusively uses global\nself-attention as an aggregation mechanism rather than static localized\nconvolutional aggregation. This allows for unconstrained long-range dynamic\ninteractions between nodes. Moreover, the edge channels allow the structural\ninformation to evolve from layer to layer, and prediction tasks on edges/links\ncan be performed directly from the output embeddings of these channels. We\nverify the performance of EGT in a wide range of graph-learning experiments on\nbenchmark datasets, in which it outperforms Convolutional/Message-Passing Graph\nNeural Networks. EGT sets a new state-of-the-art for the quantum-chemical\nregression task on the OGB-LSC PCQM4Mv2 dataset containing 3.8 million\nmolecular graphs. Our findings indicate that global self-attention based\naggregation can serve as a flexible, adaptive and effective replacement of\ngraph convolution for general-purpose graph learning. Therefore, convolutional\nlocal neighborhood aggregation is not an essential inductive bias.",
        "Task": "They address node classification (PATTERN, CLUSTER), edge classification (TSP), and graph classification/regression (MNIST, CIFAR10, ZINC) tasks, as well as large-scale graph regression (PCQM4M, PCQM4Mv2). They also perform transfer learning on graph-level classification tasks (MolPCBA, MolHIV).",
        "Data_type": "They process graphs with N nodes, each having a d_h-dimensional embedding, and maintain a full set of NxN edge embeddings of dimension d_e (e.g., derived from adjacency or distance matrices, molecular SMILES, etc.). Their experiments span homogeneous graph data such as superpixel-based image graphs and molecular graphs with node/edge features.",
        "Data_domain": "They evaluate and analyze graph-structured data from diverse domains, including molecular graphs (e.g., ZINC, PCQM4M), superpixel image graphs (MNIST, CIFAR10), and synthetic graph benchmarks (PATTERN, CLUSTER, TSP). Beyond these, the paper notes that communication networks, knowledge bases, and social networks also constitute significant sources of graph data.",
        "ML_phase": "They introduce EGT (Edge-augmented Graph Transformer), an architecture extending Transformers with residual edge channels that update both node and edge embeddings through global self-attention layers, optional SVD-based positional encodings, and specialized gating/centrality mechanisms. The training pipeline includes multi-batch masked attention, standard optimizers (with warmup or cosine decay), and optional distance-prediction regularization, supporting diverse graph-level, node-level, and edge-level tasks at various scales."
    },
    "kdd22/2205.11648v3.json": {
        "title": "Deep Representations for Time-varying Brain Datasets",
        "abs": "Finding an appropriate representation of dynamic activities in the brain is\ncrucial for many downstream applications. Due to its highly dynamic nature,\ntemporally averaged fMRI (functional magnetic resonance imaging) can only\nprovide a narrow view of underlying brain activities. Previous works lack the\nability to learn and interpret the latent dynamics in brain architectures. This\npaper builds an efficient graph neural network model that incorporates both\nregion-mapped fMRI sequences and structural connectivities obtained from DWI\n(diffusion-weighted imaging) as inputs. We find good representations of the\nlatent brain dynamics through learning sample-level adaptive adjacency matrices\nand performing a novel multi-resolution inner cluster smoothing. We also\nattribute inputs with integrated gradients, which enables us to infer (1)\nhighly involved brain connections and subnetworks for each task, (2) temporal\nkeyframes of imaging sequences that characterize tasks, and (3) subnetworks\nthat discriminate between individual subjects. This ability to identify\ncritical subnetworks that characterize signal states across heterogeneous tasks\nand individuals is of great importance to neuroscience and other scientific\ndomains. Extensive experiments and ablation studies demonstrate our proposed\nmethod's superiority and efficiency in spatial-temporal graph signal modeling\nwith insightful interpretations of brain dynamics.",
        "Task": "They address a multi-class classification task to distinguish among six cognitive states (rest, VWM, DYN, DOT, MOD, PVT) from the CRASH dataset’s fMRI scans. Performance is evaluated on 1940 sessions by predicting the correct task class for each input time series.",
        "Data_type": "They handle graph-structured neuroimaging data where nodes represent brain regions with time-series (fMRI) signals and edges represent structural connectivity (from DWI) or learned/adaptive functional connections. Each sample’s node features are temporal signals, and adjacency matrices are either fixed (DWI-based) or dynamically inferred.",
        "Data_domain": "This work leverages neuroimaging data (fMRI and DWI) collected from individuals performing multiple cognitive tasks, focusing on brain-wide functional and structural signals. The data domain is thus neuroscience/medical imaging, where each session’s brain activity is represented as time-varying graph signals.",
        "ML_phase": "They transform fMRI and DWI data into graph-based representations by parcellating brain regions and constructing structural adjacency matrices, then standardizing and splitting sessions for training. The ReBraiD model alternates temporal (TCN) and spatial (GCN) layers, incorporates sample-level latent adjacency learning and multi-resolution smoothing, and is trained with cross-entropy using a standard optimizer and grid-searched hyperparameters."
    },
    "kdd22/2206.12811v1.json": {
        "title": "Towards Representation Alignment and Uniformity in Collaborative  Filtering",
        "abs": "Collaborative filtering (CF) plays a critical role in the development of\nrecommender systems. Most CF methods utilize an encoder to embed users and\nitems into the same representation space, and the Bayesian personalized ranking\n(BPR) loss is usually adopted as the objective function to learn informative\nencoders. Existing studies mainly focus on designing more powerful encoders\n(e.g., graph neural network) to learn better representations. However, few\nefforts have been devoted to investigating the desired properties of\nrepresentations in CF, which is important to understand the rationale of\nexisting CF methods and design new learning objectives. In this paper, we\nmeasure the representation quality in CF from the perspective of alignment and\nuniformity on the hypersphere. We first theoretically reveal the connection\nbetween the BPR loss and these two properties. Then, we empirically analyze the\nlearning dynamics of typical CF methods in terms of quantified alignment and\nuniformity, which shows that better alignment or uniformity both contribute to\nhigher recommendation performance. Based on the analyses results, a learning\nobjective that directly optimizes these two properties is proposed, named\nDirectAU. We conduct extensive experiments on three public datasets, and the\nproposed learning framework with a simple matrix factorization model leads to\nsignificant performance improvements compared to state-of-the-art CF methods.\nOur implementations are publicly available at\nhttps://github.com/THUwangcy/DirectAU.",
        "Task": "They address a top-K recommendation task in collaborative filtering, which aims to learn user–item rankings from implicit feedback. The evaluations are conducted on real-world user–item interaction datasets (Beauty, Gowalla, Yelp2018) using standard top-K metrics such as Recall and NDCG.",
        "Data_type": "They handle user–item interaction data represented as a bipartite graph, where users and items are nodes connected by observed interactions. No textual, visual, or other multi-modal features are considered, as the approach focuses solely on ID-based collaborative filtering inputs.",
        "Data_domain": "The paper’s datasets span e-commerce product reviews (Amazon Beauty), location-based social networks (Gowalla), and local business ratings (Yelp2018). They offer user–item interaction data reflecting purchases, check-ins, and service reviews across these domains.",
        "ML_phase": "They preprocess interaction data by removing duplicates, ensuring at least five interactions per entity, and splitting into training/validation/test sets. The framework then encodes user and item IDs into embeddings (optionally with GNN layers), directly optimizes alignment and uniformity objectives without negative sampling, and updates parameters via gradient-based training."
    },
    "kdd22/2206.12689v1.json": {
        "title": "Improving Data-driven Heterogeneous Treatment Effect Estimation Under  Structure Uncertainty",
        "abs": "Estimating how a treatment affects units individually, known as heterogeneous\ntreatment effect (HTE) estimation, is an essential part of decision-making and\npolicy implementation. The accumulation of large amounts of data in many\ndomains, such as healthcare and e-commerce, has led to increased interest in\ndeveloping data-driven algorithms for estimating heterogeneous effects from\nobservational and experimental data. However, these methods often make strong\nassumptions about the observed features and ignore the underlying causal model\nstructure, which can lead to biased HTE estimation. At the same time,\naccounting for the causal structure of real-world data is rarely trivial since\nthe causal mechanisms that gave rise to the data are typically unknown. To\naddress this problem, we develop a feature selection method that considers each\nfeature's value for HTE estimation and learns the relevant parts of the causal\nstructure from data. We provide strong empirical evidence that our method\nimproves existing data-driven HTE estimation methods under arbitrary underlying\ncausal structures. Our results on synthetic, semi-synthetic, and real-world\ndatasets show that our feature selection algorithm leads to lower HTE\nestimation error.",
        "Task": "They primarily address a regression task for estimating heterogeneous treatment effects (HTEs). Performance is evaluated on synthetic, semi-synthetic, and real-world datasets by measuring how accurately the model predicts conditional average treatment effects.",
        "Data_type": "They handle tabular data of dimension N×d with real-valued or discrete features, including textual embeddings (e.g., BERT). They do not address images or explicit graph-structured inputs.",
        "Data_domain": "They analyze synthetic data, semi-synthetic data from an infant health program, and real-world data from a gaming platform (League of Legends) and social media (tweets related to Juul and cannabis). Together, these datasets span healthcare, gaming, and social media domains.",
        "ML_phase": "They first preprocess data by sequentially selecting features via a heuristic HTE metric, then locally learn partial causal structures to exclude mediators and descendants. Subsequently, training is carried out on standard HTE estimators using the pruned set of features to improve effect estimation accuracy."
    },
    "kdd22/2205.10312v2.json": {
        "title": "ClusterEA: Scalable Entity Alignment with Stochastic Training and  Normalized Mini-batch Similarities",
        "abs": "Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted for\nEA. However, the increasing scale of KGs renders it hard for EA models to adopt\nthe normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.",
        "Task": "They tackle a node-level alignment problem, specifically matching equivalent entities across different knowledge graphs. They evaluate performance on benchmark datasets including IDS and DBP1M, which provide cross-lingual EA scenarios at varying scales.",
        "Data_type": "They handle graph-structured data consisting of nodes and edges in knowledge graphs. The approach focuses on purely structural information (i.e., node and edge relations) without using side modalities like textual attributes or images.",
        "Data_domain": "The paper’s datasets come from cross-lingual knowledge graphs extracted primarily from DBpedia. They cover large-scale real-world knowledge bases in multiple languages (English, French, and German).",
        "ML_phase": "They first apply stochastic training with neighborhood sampling to learn entity embeddings from both KGs and then employ a learning-based mini-batch sampling strategy to preserve high overlap in local structure. Subsequently, each mini-batch undergoes Sinkhorn-based normalization before being fused with a partially normalized global similarity to finalize the one-to-one alignment."
    },
    "kdd22/2206.04789v1.json": {
        "title": "Comprehensive Fair Meta-learned Recommender System",
        "abs": "In recommender systems, one common challenge is the cold-start problem, where\ninteractions are very limited for fresh users in the systems. To address this\nchallenge, recently, many works introduce the meta-optimization idea into the\nrecommendation scenarios, i.e. learning to learn the user preference by only a\nfew past interaction items. The core idea is to learn global shared\nmeta-initialization parameters for all users and rapidly adapt them into local\nparameters for each user respectively. They aim at deriving general knowledge\nacross preference learning of various users, so as to rapidly adapt to the\nfuture new user with the learned prior and a small amount of training data.\nHowever, previous works have shown that recommender systems are generally\nvulnerable to bias and unfairness. Despite the success of meta-learning at\nimproving the recommendation performance with cold-start, the fairness issues\nare largely overlooked. In this paper, we propose a comprehensive fair\nmeta-learning framework, named CLOVER, for ensuring the fairness of\nmeta-learned recommendation models. We systematically study three kinds of\nfairness - individual fairness, counterfactual fairness, and group fairness in\nthe recommender systems, and propose to satisfy all three kinds via a\nmulti-task adversarial learning scheme. Our framework offers a generic training\nparadigm that is applicable to different meta-learned recommender systems. We\ndemonstrate the effectiveness of CLOVER on the representative meta-learned user\npreference estimator on three real-world data sets. Empirical results show that\nCLOVER achieves comprehensive fairness without deteriorating the overall\ncold-start recommendation performance.",
        "Task": "They address cold-start recommendation by performing rating prediction within a meta-learning framework. The approach is evaluated on three real-world datasets (ML-1M, BookCrossing, ML-100K) to measure both predictive performance and fairness.",
        "Data_type": "They handle tabular textual and numeric features from user and item profiles (e.g., gender, age, occupation, publication year, genre), along with explicit rating data. No graph-structured inputs, images, or other multi-modal formats are involved.",
        "Data_domain": "The paper’s data is drawn from publicly accessible recommender system datasets—MovieLens (ML-1M, ML-100K) and BookCrossing—comprising user ratings, demographic profiles, and item metadata. This domain centers on consumer-level rating data in an online recommendation context.",
        "ML_phase": "They employ a meta-learning recommendation pipeline with a bi-level training framework, where an inner loop fine-tunes parameters for each user’s limited data, and an outer loop updates a global initialization across users. Adversarial components are incorporated at both the representation and prediction levels to remove sensitive attribute signals, ensuring fairness during user-specific fine-tuning and final inference."
    },
    "kdd22/2210.17292v1.json": {
        "title": "M$^3$Care: Learning with Missing Modalities in Multimodal Healthcare  Data",
        "abs": "Multimodal electronic health record (EHR) data are widely used in clinical\napplications. Conventional methods usually assume that each sample (patient) is\nassociated with the unified observed modalities, and all modalities are\navailable for each sample. However, missing modality caused by various clinical\nand social reasons is a common issue in real-world clinical scenarios. Existing\nmethods mostly rely on solving a generative model that learns a mapping from\nthe latent space to the original input space, which is an unstable ill-posed\ninverse problem. To relieve the underdetermined system, we propose a model\nsolving a direct problem, dubbed learning with Missing Modalities in Multimodal\nhealthcare data (M3Care). M3Care is an end-to-end model compensating the\nmissing information of the patients with missing modalities to perform clinical\nanalysis. Instead of generating raw missing data, M3Care imputes the\ntask-related information of the missing modalities in the latent space by the\nauxiliary information from each patient's similar neighbors, measured by a\ntask-guided modality-adaptive similarity metric, and thence conducts the\nclinical tasks. The task-guided modality-adaptive similarity metric utilizes\nthe uncensored modalities of the patient and the other patients who also have\nthe same uncensored modalities to find similar patients. Experiments on\nreal-world datasets show that M3Care outperforms the state-of-the-art\nbaselines. Moreover, the findings discovered by M3Care are consistent with\nexperts and medical knowledge, demonstrating the capability and the potential\nof providing useful insights and explanations.",
        "Task": "They address two classification tasks on real-world EHR data: a multi-label classification task for ocular disease diagnosis on the ODIR dataset and a binary classification task for abnormal intraocular pressure prediction on the OV dataset. The datasets serve as benchmarks to evaluate model performance under varying levels of missing modalities.",
        "Data_type": "They process multimodal EHR data combining tabular demographic info, textual clinical notes, time-series records, and medical images. They specifically handle incomplete patient records across these diverse data sources (e.g., images and text) in real-world clinical scenarios.",
        "Data_domain": "These datasets originate from real-world medical settings, specifically electronic health records (EHR) in ophthalmic contexts. They include patient demographics, text (e.g., clinical notes), and visual (e.g., fundus image) modalities collected from hospitals.",
        "ML_phase": "They first encode each modality into latent representations using modality-specific encoders. Then they aggregate representations by finding similar patients via deep kernels, impute missing modalities in the latent space, and employ a multimodal attention-based module for final prediction."
    },
    "kdd22/2301.05466v1.json": {
        "title": "A Nearly-Linear Time Algorithm for Minimizing Risk of Conflict in Social  Networks",
        "abs": "Concomitant with the tremendous prevalence of online social media platforms,\nthe interactions among individuals are unprecedentedly enhanced. People are\nfree to interact with acquaintances, express and exchange their own opinions\nthrough commenting, liking, retweeting on online social media, leading to\nresistance, controversy and other important phenomena over controversial social\nissues, which have been the subject of many recent works. In this paper, we\nstudy the problem of minimizing risk of conflict in social networks by\nmodifying the initial opinions of a small number of nodes. We show that the\nobjective function of the combinatorial optimization problem is monotone and\nsupermodular. We then propose a na\\\"{\\i}ve greedy algorithm with a $(1-1/e)$\napproximation ratio that solves the problem in cubic time. To overcome the\ncomputation challenge for large networks, we further integrate several\neffective approximation strategies to provide a nearly linear time algorithm\nwith a $(1-1/e-\\epsilon)$ approximation ratio for any error parameter\n$\\epsilon>0$. Extensive experiments on various real-world datasets demonstrate\nboth the efficiency and effectiveness of our algorithms. In particular, the\nfast one scales to large networks with more than two million nodes, and\nachieves up to $20\\times$ speed-up over the state-of-the-art algorithm.",
        "Task": "They formulate a node-level subset selection problem that aims to minimize conflict measures (controversy/resistance) in social networks by strategically changing a small number of individuals’ opinions. They evaluate performance on real-world graph datasets, measuring how effectively this optimization reduces the targeted conflict metrics.",
        "Data_type": "They use graph-structured data where each node holds a scalar internal opinion, connected by edges modeling social ties. The graphs are homogeneous, with no additional textual or multimodal attributes.",
        "Data_domain": "The paper’s data comes from real-world social networks, including datasets drawn from public repositories such as KONECT and SNAP. These large-scale graphs capture online interactions among individuals in various social media platforms.",
        "ML_phase": "They construct adjacency matrices from social network data, then formulate a supermodular objective (conflict reduction) and solve it via two greedy approaches (naïve and fast approximate) that replace typical training steps in the pipeline. The method includes computing or approximating matrix inverses, updating internal opinions, and iteratively selecting top-impact nodes without standard model “training,” but rather an algorithmic optimization phase to minimize the defined conflict measures."
    },
    "kdd22/2205.09852v2.json": {
        "title": "Deconfounding Actor-Critic Network with Policy Adaptation for Dynamic  Treatment Regimes",
        "abs": "Despite intense efforts in basic and clinical research, an individualized\nventilation strategy for critically ill patients remains a major challenge.\nRecently, dynamic treatment regime (DTR) with reinforcement learning (RL) on\nelectronic health records (EHR) has attracted interest from both the healthcare\nindustry and machine learning research community. However, most learned DTR\npolicies might be biased due to the existence of confounders. Although some\ntreatment actions non-survivors received may be helpful, if confounders cause\nthe mortality, the training of RL models guided by long-term outcomes (e.g.,\n90-day mortality) would punish those treatment actions causing the learned DTR\npolicies to be suboptimal. In this study, we develop a new deconfounding\nactor-critic network (DAC) to learn optimal DTR policies for patients. To\nalleviate confounding issues, we incorporate a patient resampling module and a\nconfounding balance module into our actor-critic framework. To avoid punishing\nthe effective treatment actions non-survivors received, we design a short-term\nreward to capture patients' immediate health state changes. Combining\nshort-term with long-term rewards could further improve the model performance.\nMoreover, we introduce a policy adaptation method to successfully transfer the\nlearned model to new-source small-scale datasets. The experimental results on\none semi-synthetic and two different real-world datasets show the proposed\nmodel outperforms the state-of-the-art models. The proposed model provides\nindividualized treatment decisions for mechanical ventilation that could\nimprove patient outcomes.",
        "Task": "They aim to learn an optimal policy for mechanical ventilation decisions (dynamic treatment regime) using reinforcement learning to reduce mortality in ICU patients. This approach is evaluated on a semi-synthetic dataset and two real-world EHR datasets (MIMIC-III, AmsterdamUMCdb).",
        "Data_type": "They utilize multivariate, time-series EHR data consisting of numeric vital signs, laboratory values, and demographics extracted from ICU stays. The data are drawn from MIMIC-III and AmsterdamUMCdb, encompassing 48 time-varying variables for mechanical ventilation management.",
        "Data_domain": "The paper’s data domain concerns clinical electronic health records (EHR) from ICU patients, including real-world datasets (MIMIC-III, AmsterdamUMCdb) and a semi-synthetic dataset derived from MIMIC-III. These records capture patient demographics, vital signs, lab values, and invasive mechanical ventilation parameters.",
        "ML_phase": "They preprocess data by resampling matched survivor and non-survivor patients based on mortality risks, then compute inverse-probability weights for balanced mini-batches. The training framework features an LSTM-based actor-critic architecture with short- and long-term rewards, plus a policy adaptation module for effective cross-domain transfer."
    },
    "kdd22/2207.13186v1.json": {
        "title": "On Missing Labels, Long-tails and Propensities in Extreme Multi-label  Classification",
        "abs": "The propensity model introduced by Jain et al. 2016 has become a standard\napproach for dealing with missing and long-tail labels in extreme multi-label\nclassification (XMLC). In this paper, we critically revise this approach\nshowing that despite its theoretical soundness, its application in contemporary\nXMLC works is debatable. We exhaustively discuss the flaws of the\npropensity-based approach, and present several recipes, some of them related to\nsolutions used in search engines and recommender systems, that we believe\nconstitute promising alternatives to be followed in XMLC.",
        "Task": "They tackle an extreme multi-label classification task, where each instance can be associated with a subset of labels from a very large label set. Multiple benchmark datasets with high-dimensional label spaces are used to evaluate classification performance.",
        "Data_type": "They primarily handle large-scale textual datasets (e.g., Wikipedia articles, Amazon product descriptions) in extreme multi-label settings, sometimes complemented by user-item rating data (as in Yahoo R3) but without complex graph or multimodal elements. The paper deals with missing labels and long-tailed distributions within these textual domains, focusing on learning and evaluation challenges in standard XMLC benchmarks.",
        "Data_domain": "The paper examines datasets from large-scale text corpora (e.g., Wikipedia), e-commerce platforms (e.g., Amazon), and user feedback systems (e.g., Yahoo R3). Thus, the data domain spans real-world online platforms involving text classification, product recommendations, and user-item rating logs.",
        "ML_phase": "They propose pipeline modifications that include creating or leveraging unbiased or bias-controlled datasets to handle missing labels, alongside reevaluating or redesigning propensity-based methods and metrics during model training. They also incorporate alternative propensity and joint-learning approaches into standard logistic-based training frameworks to more accurately estimate and correct for label noise."
    },
    "kdd22/2201.07391v3.json": {
        "title": "MetaV: A Meta-Verifier Approach to Task-Agnostic Model Fingerprinting",
        "abs": "For model piracy forensics, previous model fingerprinting schemes are\ncommonly based on adversarial examples constructed for the owner's model as the\n\\textit{fingerprint}, and verify whether a suspect model is indeed pirated from\nthe original model by matching the behavioral pattern on the fingerprint\nexamples between one another. However, these methods heavily rely on the\ncharacteristics of classification tasks which inhibits their application to\nmore general scenarios. To address this issue, we present MetaV, the first\ntask-agnostic model fingerprinting framework which enables fingerprinting on a\nmuch wider range of DNNs independent from the downstream learning task, and\nexhibits strong robustness against a variety of ownership obfuscation\ntechniques. Specifically, we generalize previous schemes into two critical\ndesign components in MetaV: the \\textit{adaptive fingerprint} and the\n\\textit{meta-verifier}, which are jointly optimized such that the meta-verifier\nlearns to determine whether a suspect model is stolen based on the concatenated\noutputs of the suspect model on the adaptive fingerprint. As a key of being\ntask-agnostic, the full process makes no assumption on the model internals in\nthe ensemble only if they have the same input and output dimensions. Spanning\nclassification, regression and generative modeling, extensive experimental\nresults validate the substantially improved performance of MetaV over the\nstate-of-the-art fingerprinting schemes and demonstrate the enhanced generality\nof MetaV for providing task-agnostic fingerprinting. For example, on\nfingerprinting ResNet-18 trained for skin cancer diagnosis, MetaV achieves\nsimultaneously $100\\%$ true positives and $100\\%$ true negatives on a diverse\ntest set of $70$ suspect models, achieving an about $220\\%$ relative\nimprovement in ARUC in comparison to the optimal baseline.",
        "Task": "They investigate three tasks: (1) 7-class skin cancer classification using DermaMNIST, (2) warfarin dose regression on the IWPC dataset, and (3) generative modeling of clothing images with FashionMNIST. These datasets each evaluate performance on their respective task (classification, regression, and generation).",
        "Data_type": "They handle three main data types: (1) 3×28×28 dermatoscopic images (upsampled to 3×224×224), (2) 31-dimensional numeric demographic/physiological features, and (3) 28×28 grayscale images for generative tasks. They also use “fingerprint” samples for verification, with no graph-structured or textual data involved.",
        "Data_domain": "They use three datasets originating from distinct domains: dermatoscopic skin lesion images (medical), patient physiological and demographic records for warfarin dosing (clinical), and clothing images (fashion).",
        "ML_phase": "Data preprocessing includes upsampling skin images to 224×224, using 31-dimensional tabular inputs for Warfarin, and training on 28×28 fashion images for generative modeling. The authors then design and train the respective architectures (ResNet-18, MLP, and DCGAN) within PyTorch using standardized hyperparameters."
    },
    "kdd22/2207.09209v4.json": {
        "title": "FLDetector: Defending Federated Learning Against Model Poisoning Attacks  via Detecting Malicious Clients",
        "abs": "Federated learning (FL) is vulnerable to model poisoning attacks, in which\nmalicious clients corrupt the global model via sending manipulated model\nupdates to the server. Existing defenses mainly rely on Byzantine-robust FL\nmethods, which aim to learn an accurate global model even if some clients are\nmalicious. However, they can only resist a small number of malicious clients in\npractice. It is still an open challenge how to defend against model poisoning\nattacks with a large number of malicious clients. Our FLDetector addresses this\nchallenge via detecting malicious clients. FLDetector aims to detect and remove\nthe majority of the malicious clients such that a Byzantine-robust FL method\ncan learn an accurate global model using the remaining clients. Our key\nobservation is that, in model poisoning attacks, the model updates from a\nclient in multiple iterations are inconsistent. Therefore, FLDetector detects\nmalicious clients via checking their model-updates consistency. Roughly\nspeaking, the server predicts a client's model update in each iteration based\non its historical model updates using the Cauchy mean value theorem and L-BFGS,\nand flags a client as malicious if the received model update from the client\nand the predicted model update are inconsistent in multiple iterations. Our\nextensive experiments on three benchmark datasets show that FLDetector can\naccurately detect malicious clients in multiple state-of-the-art model\npoisoning attacks. After removing the detected malicious clients, existing\nByzantine-robust FL methods can learn accurate global models.Our code is\navailable at https://github.com/zaixizhang/FLDetector.",
        "Task": "They perform image classification on MNIST, CIFAR10, and FEMNIST in a federated learning setting. The task is to maintain or restore high classification accuracy under model poisoning attacks.",
        "Data_type": "They focus on image datasets (MNIST, CIFAR10, FEMNIST), all in the 2D image modality under federated settings. No textual, graph-structured, or other multi-modal data are included in their experiments.",
        "Data_domain": "They use publicly available image datasets (MNIST, CIFAR10, and FEMNIST) to evaluate their method. Hence, the data domain is computer vision, focusing on image classification benchmarks.",
        "ML_phase": "They split the datasets (MNIST, CIFAR10, FEMNIST) across multiple clients with varying degrees of non-iid data, and use either a four-layer CNN (for MNIST/FEMNIST) or ResNet20 (for CIFAR10). In federated training, each client locally updates the model and sends gradients back to the server, which aggregates them using FedAvg, Krum, Trimmed-Mean, or Median over up to 2000 iterations with set learning rates."
    },
    "kdd22/2210.08274v1.json": {
        "title": "CLARE: A Semi-supervised Community Detection Algorithm",
        "abs": "Community detection refers to the task of discovering closely related\nsubgraphs to understand the networks. However, traditional community detection\nalgorithms fail to pinpoint a particular kind of community. This limits its\napplicability in real-world networks, e.g., distinguishing fraud groups from\nnormal ones in transaction networks. Recently, semi-supervised community\ndetection emerges as a solution. It aims to seek other similar communities in\nthe network with few labeled communities as training data. Existing works can\nbe regarded as seed-based: locate seed nodes and then develop communities\naround seeds. However, these methods are quite sensitive to the quality of\nselected seeds since communities generated around a mis-detected seed may be\nirrelevant. Besides, they have individual issues, e.g., inflexibility and high\ncomputational overhead. To address these issues, we propose CLARE, which\nconsists of two key components, Community Locator and Community Rewriter. Our\nidea is that we can locate potential communities and then refine them.\nTherefore, the community locator is proposed for quickly locating potential\ncommunities by seeking subgraphs that are similar to training ones in the\nnetwork. To further adjust these located communities, we devise the community\nrewriter. Enhanced by deep reinforcement learning, it suggests intelligent\ndecisions, such as adding or dropping nodes, to refine community structures\nflexibly. Extensive experiments verify both the effectiveness and efficiency of\nour work compared with prior state-of-the-art approaches on multiple real-world\ndatasets.",
        "Task": "They address a semi-supervised community detection task on networks, aiming to find “targeted” communities using a small set of labeled ones as training data. They evaluate performance on multiple real-world and hybrid graphs labeled with ground-truth communities.",
        "Data_type": "They handle homogeneous graph-structured data, where each node can have feature vectors (e.g., degree statistics, optional attributes). The framework supports both non-attributed and attributed graphs, leveraging node-level information to detect targeted communities.",
        "Data_domain": "They use real-world network data from multiple domains, including an e-commerce platform (Amazon), an academic coauthorship network (DBLP), and social networks (LiveJournal, Facebook). These datasets come from SNAP or are constructed by linking different networks together, encompassing social, academic, and commercial domains.",
        "ML_phase": "They propose a two-stage inference pipeline: a Community Locator encodes subgraphs via GNN-based order embeddings for fast matching, followed by a Community Rewriter that refines each located subgraph through a reinforcement learning agent. The model is trained using a margin-based loss for subgraph matching and policy gradient for iterative structural adjustments."
    },
    "kdd22/2206.06561v4.json": {
        "title": "FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks",
        "abs": "Knowledge distillation (KD) has demonstrated its effectiveness to boost the\nperformance of graph neural networks (GNNs), where its goal is to distill\nknowledge from a deeper teacher GNN into a shallower student GNN. However, it\nis actually difficult to train a satisfactory teacher GNN due to the well-known\nover-parametrized and over-smoothing issues, leading to invalid knowledge\ntransfer in practical applications. In this paper, we propose the first\nFree-direction Knowledge Distillation framework via Reinforcement learning for\nGNNs, called FreeKD, which is no longer required to provide a deeper\nwell-optimized teacher GNN. The core idea of our work is to collaboratively\nbuild two shallower GNNs in an effort to exchange knowledge between them via\nreinforcement learning in a hierarchical way. As we observe that one typical\nGNN model often has better and worse performances at different nodes during\ntraining, we devise a dynamic and free-direction knowledge transfer strategy\nthat consists of two levels of actions: 1) node-level action determines the\ndirections of knowledge transfer between the corresponding nodes of two\nnetworks; and then 2) structure-level action determines which of the local\nstructures generated by the node-level actions to be propagated. In essence,\nour FreeKD is a general and principled framework which can be naturally\ncompatible with GNNs of different architectures. Extensive experiments on five\nbenchmark datasets demonstrate our FreeKD outperforms two base GNNs in a large\nmargin, and shows its efficacy to various GNNs. More surprisingly, our FreeKD\nhas comparable or even better performance than traditional KD algorithms that\ndistill knowledge from a deeper and stronger teacher GNN.",
        "Task": "They address node-level classification tasks on five graph datasets (Cora, Citeseer, Chameleon, Texas, and PPI). Their evaluation covers both transductive and inductive settings to demonstrate improved classification performance.",
        "Data_type": "They operate on homogeneous graph-structured data with node features that are primarily textual (e.g., bag-of-words for web/citation pages) or gene-based (for protein-protein interactions). Across five benchmark datasets (Cora, Citeseer, Chameleon, Texas, and PPI), each node is associated with feature vectors and connected by edges defining the underlying graph.",
        "Data_domain": "They use graph-structured data from academic citation networks, web networks, and human protein–protein interaction graphs, as well as social networks. These datasets span tasks in both transductive and inductive settings covering scholarly documents, webpage hyperlinks, and biological protein interactions.",
        "ML_phase": "This work resides in the model training phase, introducing a hierarchical reinforcement learning framework that simultaneously trains two shallow GNNs via free-direction knowledge distillation. It dynamically selects node-level and structure-level distillation directions, integrating these distillation losses with cross-entropy in the GNN training loop."
    },
    "kdd22/2206.13764v1.json": {
        "title": "Detecting Arbitrary Order Beneficial Feature Interactions for  Recommender Systems",
        "abs": "Detecting beneficial feature interactions is essential in recommender\nsystems, and existing approaches achieve this by examining all the possible\nfeature interactions. However, the cost of examining all the possible\nhigher-order feature interactions is prohibitive (exponentially growing with\nthe order increasing). Hence existing approaches only detect limited order\n(e.g., combinations of up to four features) beneficial feature interactions,\nwhich may miss beneficial feature interactions with orders higher than the\nlimitation. In this paper, we propose a hypergraph neural network based model\nnamed HIRS. HIRS is the first work that directly generates beneficial feature\ninteractions of arbitrary orders and makes recommendation predictions\naccordingly. The number of generated feature interactions can be specified to\nbe much smaller than the number of all the possible interactions and hence, our\nmodel admits a much lower running time. To achieve an effective algorithm, we\nexploit three properties of beneficial feature interactions, and propose\ndeep-infomax-based methods to guide the interaction generation. Our\nexperimental results show that HIRS outperforms state-of-the-art algorithms by\nup to 5% in terms of recommendation accuracy.",
        "Task": "They cast recommendation as a hypergraph-level classification task, predicting each user–item instance’s implicit feedback (0 or 1). They validate performance on large-scale real-world datasets (MovieLens 1M, Book-crossing, MovieLens 25M) using ranking-based metrics.",
        "Data_type": "They represent each data sample as a hypergraph with nodes corresponding to tabular or textual features and edges capturing feature interactions. The proposed approach focuses on these node/edge structures for recommendation scenarios rather than multi-modal image-text data or chemical SMILES inputs.",
        "Data_domain": "The data is drawn from user-item rating logs in large-scale recommendation platforms focusing on movies and books (e.g., MovieLens and Book-crossing). They represent consumer feedback in the entertainment and reading domains, capturing user preferences for items such as films and literature.",
        "ML_phase": "Data is represented as a hypergraph with each feature as a node, where a hyperedge prediction module identifies beneficial feature interactions before an interaction-based hypergraph neural network (IHGNN) models them for recommendation. During training, s-Infomax and Infomin objectives, combined with an L₀ activation regularization, guide the generation of these hyperedges and optimize the final prediction."
    },
    "kdd22/2206.02886v2.json": {
        "title": "Graph Rationalization with Environment-based Augmentations",
        "abs": "Rationale is defined as a subset of input features that best explains or\nsupports the prediction by machine learning models. Rationale identification\nhas improved the generalizability and interpretability of neural networks on\nvision and language data. In graph applications such as molecule and polymer\nproperty prediction, identifying representative subgraph structures named as\ngraph rationales plays an essential role in the performance of graph neural\nnetworks. Existing graph pooling and/or distribution intervention methods\nsuffer from lack of examples to learn to identify optimal graph rationales. In\nthis work, we introduce a new augmentation operation called environment\nreplacement that automatically creates virtual data examples to improve\nrationale identification. We propose an efficient framework that performs\nrationale-environment separation and representation learning on the real and\naugmented examples in latent spaces to avoid the high complexity of explicit\ngraph decoding and encoding. Comparing against recent techniques, experiments\non seven molecular and four polymer real datasets demonstrate the effectiveness\nand efficiency of the proposed augmentation-based graph rationalization\nframework.",
        "Task": "They address graph-level classification (molecule data) and graph-level regression (polymer data). Evaluation is conducted on four small polymer regression datasets and seven molecule classification benchmarks.",
        "Data_type": "They handle graph-structured chemical data—both small-molecule and polymer graphs—where atoms are nodes with associated labels/features, and bonds are edges. No additional modalities such as images or text are included; the focus remains on node-edge level graph inputs.",
        "Data_domain": "The paper’s datasets come from chemistry and materials science, specifically including real-world molecule datasets (e.g., OGBG for chemoinformatics) and polymer datasets (e.g., PolyInfo and membrane databases) for polymer informatics. These data domains focus on predicting and analyzing chemical or material properties, such as polymer density and molecule toxicity.",
        "ML_phase": "They first generate latent representations of node masks (rationales vs. environment) using GNN₁+MLP₁, then produce augments by removing or replacing environment parts in the same latent space for training. Subsequently, GNN₂+MLP₂ are trained in an alternating scheme on both real and augmented examples, optimizing combined losses for rationale selection and property prediction."
    },
    "kdd22/2207.00012v4.json": {
        "title": "Reliable Representations Make A Stronger Defender: Unsupervised  Structure Refinement for Robust GNN",
        "abs": "Benefiting from the message passing mechanism, Graph Neural Networks (GNNs)\nhave been successful on flourish tasks over graph data. However, recent studies\nhave shown that attackers can catastrophically degrade the performance of GNNs\nby maliciously modifying the graph structure. A straightforward solution to\nremedy this issue is to model the edge weights by learning a metric function\nbetween pairwise representations of two end nodes, which attempts to assign low\nweights to adversarial edges. The existing methods use either raw features or\nrepresentations learned by supervised GNNs to model the edge weights. However,\nboth strategies are faced with some immediate problems: raw features cannot\nrepresent various properties of nodes (e.g., structure information), and\nrepresentations learned by supervised GNN may suffer from the poor performance\nof the classifier on the poisoned graph. We need representations that carry\nboth feature information and as mush correct structure information as possible\nand are insensitive to structural perturbations. To this end, we propose an\nunsupervised pipeline, named STABLE, to optimize the graph structure. Finally,\nwe input the well-refined graph into a downstream classifier. For this part, we\ndesign an advanced GCN that significantly enhances the robustness of vanilla\nGCN without increasing the time complexity. Extensive experiments on four\nreal-world graph benchmarks demonstrate that STABLE outperforms the\nstate-of-the-art methods and successfully defends against various attacks.",
        "Task": "They study a transductive node classification task on several graph benchmarks (Cora, Citeseer, PubMed, and Polblogs). The goal is to classify nodes in these graphs and evaluate performance under adversarial perturbations.",
        "Data_type": "They focus on homogeneous graph-structured data with node-level features (including textual attributes) under a single relational type. No multi-modal images or SMILES/fingerprint inputs are involved.",
        "Data_domain": "The data are drawn from multiple graph benchmarks, including three academic citation networks (Cora, Citeseer, PubMed) and one political blog network (Polblogs). In other words, it covers both academic/research citation data and social blog data.",
        "ML_phase": "They first prune suspicious edges and create augmented views for unsupervised contrastive representation learning to obtain more robust node embeddings. Next, these embeddings guide graph refinement, and an advanced GCN with a specialized normalization trick is trained on the refined graph for final node classification."
    },
    "kdd22/2206.11130v1.json": {
        "title": "Multi-View Clustering for Open Knowledge Base Canonicalization",
        "abs": "Open information extraction (OIE) methods extract plenty of OIE triples <noun\nphrase, relation phrase, noun phrase> from unstructured text, which compose\nlarge open knowledge bases (OKBs). Noun phrases and relation phrases in such\nOKBs are not canonicalized, which leads to scattered and redundant facts. It is\nfound that two views of knowledge (i.e., a fact view based on the fact triple\nand a context view based on the fact triple's source context) provide\ncomplementary information that is vital to the task of OKB canonicalization,\nwhich clusters synonymous noun phrases and relation phrases into the same group\nand assigns them unique identifiers. However, these two views of knowledge have\nso far been leveraged in isolation by existing works. In this paper, we propose\nCMVC, a novel unsupervised framework that leverages these two views of\nknowledge jointly for canonicalizing OKBs without the need of manually\nannotated labels. To achieve this goal, we propose a multi-view CH K-Means\nclustering algorithm to mutually reinforce the clustering of view-specific\nembeddings learned from each view by considering their different clustering\nqualities. In order to further enhance the canonicalization performance, we\npropose a training data optimization strategy in terms of data quantity and\ndata quality respectively in each particular view to refine the learned\nview-specific embeddings in an iterative manner. Additionally, we propose a\nLog-Jump algorithm to predict the optimal number of clusters in a data-driven\nway without requiring any labels. We demonstrate the superiority of our\nframework through extensive experiments on multiple real-world OKB data sets\nagainst state-of-the-art methods.",
        "Task": "They focus on an unsupervised clustering task for canonicalizing open knowledge bases, where synonymous noun and relation phrases are grouped together. To evaluate performance, they use three real-world OKB datasets (ReVerb45K, NYTimes2018, and OPIEC59K).",
        "Data_type": "They handle unstructured textual data extracted from natural language, specifically open information extraction (OIE) triples (subject noun phrase, relation phrase, object noun phrase) plus their source contexts. Their focus is strictly on textual attributes rather than image, graph, or other data modalities.",
        "Data_domain": "They use large-scale text corpora (e.g., Clueweb09, NYTimes, Wikipedia) to extract OIE triples, forming open knowledge bases. Specifically, ReVerb45K, NYTimes2018, and OPIEC59K are sampled from those textual sources to serve as the paper’s data sets.",
        "ML_phase": "They propose an unsupervised pipeline that first processes OIE triples to form fact embeddings (via a KB embedding model) and context embeddings (via a pre-trained language model), then iteratively refines these embeddings using data augmentation (fact view) or iterative clustering (context view). Finally, both views are combined with a multi-view CH K-Means algorithm (guided by a Log-Jump method for determining cluster count) to complete the end-to-end training framework for canonicalizing open knowledge bases."
    },
    "kdd22/2206.05311v2.json": {
        "title": "Graph-in-Graph Network for Automatic Gene Ontology Description  Generation",
        "abs": "Gene Ontology (GO) is the primary gene function knowledge base that enables\ncomputational tasks in biomedicine. The basic element of GO is a term, which\nincludes a set of genes with the same function. Existing research efforts of GO\nmainly focus on predicting gene term associations. Other tasks, such as\ngenerating descriptions of new terms, are rarely pursued. In this paper, we\npropose a novel task: GO term description generation. This task aims to\nautomatically generate a sentence that describes the function of a GO term\nbelonging to one of the three categories, i.e., molecular function, biological\nprocess, and cellular component. To address this task, we propose a\nGraph-in-Graph network that can efficiently leverage the structural information\nof GO. The proposed network introduces a two-layer graph: the first layer is a\ngraph of GO terms where each node is also a graph (gene graph). Such a\nGraph-in-Graph network can derive the biological functions of GO terms and\ngenerate proper descriptions. To validate the effectiveness of the proposed\nnetwork, we build three large-scale benchmark datasets. By incorporating the\nproposed Graph-in-Graph network, the performances of seven different\nsequence-to-sequence models can be substantially boosted across all evaluation\nmetrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,\nROUGE-L, and METEOR, respectively.",
        "Task": "They propose an automatic text generation task that produces Gene Ontology term descriptions from associated gene information. This task is evaluated on three newly constructed benchmark datasets and measured by standard natural language generation metrics.",
        "Data_type": "They employ hierarchical graph-structured data, where each node represents a Gene Ontology term (including associated gene text) and edges capture biological “is a” relationships. The node attributes are purely textual, forming a homogeneous graph with descriptions drawn from scientific literature and curated gene annotations.",
        "Data_domain": "The data originates from the biomedical domain, specifically focusing on Gene Ontology terms associated with Homo sapiens. It uses information such as gene text from GeneCards and covers aspects like molecular function, biological process, and cellular component.",
        "ML_phase": "They compile three large-scale datasets of GO terms and associated gene text, then perform tokenization, vocabulary filtering, and split them into training/validation/test sets. Next, they augment standard sequence-to-sequence models with a Graph-in-Graph network (gene graph + term graph) and train end-to-end using cross-entropy loss with the Adam optimizer."
    },
    "kdd22/2205.09717v1.json": {
        "title": "Flexible Modeling and Multitask Learning using Differentiable Tree  Ensembles",
        "abs": "Decision tree ensembles are widely used and competitive learning models.\nDespite their success, popular toolkits for learning tree ensembles have\nlimited modeling capabilities. For instance, these toolkits support a limited\nnumber of loss functions and are restricted to single task learning. We propose\na flexible framework for learning tree ensembles, which goes beyond existing\ntoolkits to support arbitrary loss functions, missing responses, and multi-task\nlearning. Our framework builds on differentiable (a.k.a. soft) tree ensembles,\nwhich can be trained using first-order methods. However, unlike classical\ntrees, differentiable trees are difficult to scale. We therefore propose a\nnovel tensor-based formulation of differentiable trees that allows for\nefficient vectorization on GPUs. We perform experiments on a collection of 28\nreal open-source and proprietary datasets, which demonstrate that our framework\ncan lead to 100x more compact and 23% more expressive tree ensembles than those\nby popular toolkits.",
        "Task": "They tackle single-task and multi-task learning for both classification and regression (including distribution-specific variants such as negative binomial and zero-inflated Poisson). They evaluate performance on multiple open-source and proprietary datasets covering fully and partially observed responses.",
        "Data_type": "They focus on tabular, primarily numeric data (including continuous and count responses), enabling single-task or multi-task settings with potentially missing responses. The paper also supports specialized distributions for count-based data (e.g., Poisson, zero-inflated Poisson, negative binomial), but it does not address unstructured modalities such as images or text.",
        "Data_domain": "Datasets come from a range of real-world domains, including social media (e.g., news popularity), finance (e.g., financial losses), public health (e.g., youth risk surveys), and established academic repositories (Mulan, UCI, Delve). Additionally, a proprietary multi-task dataset from a multinational financial services company is included.",
        "ML_phase": "This work focuses on model architecture design and training within the ML pipeline by proposing a differentiable tree ensemble framework that can be trained end-to-end on various differentiable losses, including multi-task extensions. It provides a TensorFlow-based implementation with GPU-acceleration, allowing seamless customization of model parameters, losses, and training procedures."
    },
    "kdd22/2207.03990v1.json": {
        "title": "Predicting Opinion Dynamics via Sociologically-Informed Neural Networks",
        "abs": "Opinion formation and propagation are crucial phenomena in social networks\nand have been extensively studied across several disciplines. Traditionally,\ntheoretical models of opinion dynamics have been proposed to describe the\ninteractions between individuals (i.e., social interaction) and their impact on\nthe evolution of collective opinions. Although these models can incorporate\nsociological and psychological knowledge on the mechanisms of social\ninteraction, they demand extensive calibration with real data to make reliable\npredictions, requiring much time and effort. Recently, the widespread use of\nsocial media platforms provides new paradigms to learn deep learning models\nfrom a large volume of social media data. However, these methods ignore any\nscientific knowledge about the mechanism of social interaction. In this work,\nwe present the first hybrid method called Sociologically-Informed Neural\nNetwork (SINN), which integrates theoretical models and social media data by\ntransporting the concepts of physics-informed neural networks (PINNs) from\nnatural science (i.e., physics) into social science (i.e., sociology and social\npsychology). In particular, we recast theoretical models as ordinary\ndifferential equations (ODEs). Then we train a neural network that\nsimultaneously approximates the data and conforms to the ODEs that represent\nthe social scientific knowledge. In addition, we extend PINNs by integrating\nmatrix factorization and a language model to incorporate rich side information\n(e.g., user profiles) and structural knowledge (e.g., cluster structure of the\nsocial interaction network). Moreover, we develop an end-to-end training\nprocedure for SINN, which involves Gumbel-Softmax approximation to include\nstochastic mechanisms of social interaction. Extensive experiments on\nreal-world and synthetic datasets show SINN outperforms six baseline methods in\npredicting opinion dynamics.",
        "Task": "They propose a multi-class classification task to predict how users’ opinions evolve over time, using labeled opinion data from social media. Performance is assessed on both synthetic datasets (with different opinion evolution patterns) and real-world social network datasets (Twitter, Reddit) containing labeled user posts.",
        "Data_type": "They handle time-stamped, text-based user attributes in social media networks (Twitter, Reddit), where nodes represent users and edges capture social interactions. The textual data includes user posts and profile descriptions, and they also use synthetic datasets simulating opinion evolution.",
        "Data_domain": "The data are primarily sourced from social media platforms (Twitter and Reddit), capturing real-world user opinions. In addition, the study leverages synthetic datasets generated via a stochastic opinion dynamics model for experimentation.",
        "ML_phase": "They construct a data pipeline by collecting large-scale social media posts (and generating synthetic data via the stochastic bounded confidence model), then annotating or inferring user opinions and extracting profile text features with a fine-tuned BERT-based encoder. Next, they design a feedforward neural network that is trained end-to-end (via backpropagation with ODE-based constraints, reparameterization for stochastic interactions, and low-rank factorization for structural knowledge) to integrate both sociological theory and data-driven learning."
    },
    "kdd22/2206.13413v1.json": {
        "title": "RES: A Robust Framework for Guiding Visual Explanation",
        "abs": "Despite the fast progress of explanation techniques in modern Deep Neural\nNetworks (DNNs) where the main focus is handling \"how to generate the\nexplanations\", advanced research questions that examine the quality of the\nexplanation itself (e.g., \"whether the explanations are accurate\") and improve\nthe explanation quality (e.g., \"how to adjust the model to generate more\naccurate explanations when explanations are inaccurate\") are still relatively\nunder-explored. To guide the model toward better explanations, techniques in\nexplanation supervision - which add supervision signals on the model\nexplanation - have started to show promising effects on improving both the\ngeneralizability as and intrinsic interpretability of Deep Neural Networks.\nHowever, the research on supervising explanations, especially in vision-based\napplications represented through saliency maps, is in its early stage due to\nseveral inherent challenges: 1) inaccuracy of the human explanation annotation\nboundary, 2) incompleteness of the human explanation annotation region, and 3)\ninconsistency of the data distribution between human annotation and model\nexplanation maps. To address the challenges, we propose a generic RES framework\nfor guiding visual explanation by developing a novel objective that handles\ninaccurate boundary, incomplete region, and inconsistent distribution of human\nannotations, with a theoretical justification on model generalizability.\nExtensive experiments on two real-world image datasets demonstrate the\neffectiveness of the proposed framework on enhancing both the reasonability of\nthe explanation and the performance of the backbone DNNs model.",
        "Task": "They address an image classification task for two domains: gender classification and scene recognition. Each domain uses its own real-world image dataset split into training, validation, and test sets, and the primary performance metric is classification accuracy.",
        "Data_type": "They focus on supervising explanation in image classification tasks with pixel-level human annotations (i.e., saliency maps), where labels can be noisy or incomplete. Their framework is designed for image data, handling challenges such as inaccurate boundaries or partial regions in the annotated saliency.",
        "Data_domain": "They use real-world image data for two classification tasks: gender classification samples are collected from the Microsoft COCO dataset, while scene recognition images come from the Places365 dataset.",
        "ML_phase": "They construct image datasets by filtering and annotating samples from COCO and Places, then leverage partial human-labeled regions (positive/negative) as noisy supervision signals. They incorporate this supervision into a robust training pipeline by combining a standard prediction loss with a specially designed explanation loss that uses an adaptive threshold, imputation kernels, and selective penalization to address boundary inaccuracy, missing regions, and label inconsistencies."
    },
    "kdd22/2207.02722v1.json": {
        "title": "Variational Flow Graphical Model",
        "abs": "This paper introduces a novel approach to embed flow-based models with\nhierarchical structures. The proposed framework is named Variational Flow\nGraphical (VFG) Model. VFGs learn the representation of high dimensional data\nvia a message-passing scheme by integrating flow-based functions through\nvariational inference. By leveraging the expressive power of neural networks,\nVFGs produce a representation of the data using a lower dimension, thus\novercoming the drawbacks of many flow-based models, usually requiring a high\ndimensional latent space involving many trivial variables. Aggregation nodes\nare introduced in the VFG models to integrate forward-backward hierarchical\ninformation via a message passing scheme. Maximizing the evidence lower bound\n(ELBO) of data likelihood aligns the forward and backward messages in each\naggregation node achieving a consistency node state. Algorithms have been\ndeveloped to learn model parameters through gradient updating regarding the\nELBO objective.\n  The consistency of aggregation nodes enable VFGs to be applicable in\ntractable inference on graphical structures. Besides representation learning\nand numerical inference, VFGs provide a new approach for distribution modeling\non datasets with graphical latent structures. Additionally, theoretical study\nshows that VFGs are universal approximators by leveraging the implicitly\ninvertible flow-based structures. With flexible graphical structures and\nsuperior excessive power, VFGs could potentially be used to improve\nprobabilistic inference. In the experiments, VFGs achieves improved evidence\nlower bound (ELBO) and likelihood values on multiple datasets.",
        "Task": "They address missing value imputation and probabilistic density estimation (generative modeling) tasks, as well as node-level inference on graphical structures. Performance is demonstrated on synthetic data, the California Housing dataset, standard image benchmarks (MNIST, Caltech101, Omniglot), and a Gaussian graphical model, evaluated via MSE, ELBO, and negative log-likelihood.",
        "Data_type": "They can process graph-structured information with node/edge dependencies and also handle standard image datasets (e.g., MNIST, Caltech101, Omniglot). In particular, the approach supports both tabular (e.g., housing data) and graphical data, enabling missing-value imputation and generative modeling in these domains.",
        "Data_domain": "They use data from multiple domains, including real estate (California Housing dataset), standard benchmark images (MNIST, Caltech101, Omniglot), and synthetic graphical model samples. These datasets span both numeric and visual modalities.",
        "ML_phase": "VFG operates in the model architecture design phase by uniting hierarchical flow-based transformations and aggregation nodes for tractable inference. It is trained end-to-end via a deterministic ELBO framework using forward-backward message passing, supporting missing value imputation and generative modeling in downstream tasks."
    },
    "kdd22/2205.11264v2.json": {
        "title": "Adaptive Fairness-Aware Online Meta-Learning for Changing Environments",
        "abs": "The fairness-aware online learning framework has arisen as a powerful tool\nfor the continual lifelong learning setting. The goal for the learner is to\nsequentially learn new tasks where they come one after another over time and\nthe learner ensures the statistic parity of the new coming task across\ndifferent protected sub-populations (e.g. race and gender). A major drawback of\nexisting methods is that they make heavy use of the i.i.d assumption for data\nand hence provide static regret analysis for the framework. However, low static\nregret cannot imply a good performance in changing environments where tasks are\nsampled from heterogeneous distributions. To address the fairness-aware online\nlearning problem in changing environments, in this paper, we first construct a\nnovel regret metric FairSAR by adding long-term fairness constraints onto a\nstrongly adapted loss regret. Furthermore, to determine a good model parameter\nat each round, we propose a novel adaptive fairness-aware online meta-learning\nalgorithm, namely FairSAOML, which is able to adapt to changing environments in\nboth bias control and model precision. The problem is formulated in the form of\na bi-level convex-concave optimization with respect to the model's primal and\ndual parameters that are associated with the model's accuracy and fairness,\nrespectively. The theoretic analysis provides sub-linear upper bounds for both\nloss regret and violation of cumulative fairness constraints. Our experimental\nevaluation on different real-world datasets with settings of changing\nenvironments suggests that the proposed FairSAOML significantly outperforms\nalternatives based on the best prior online learning approaches.",
        "Task": "They focus on a classification task, evaluating performance on multiple real-world benchmark datasets. The paper provides both theoretical analysis and experimental results to validate their classification approach.",
        "Data_type": "The authors do not explicitly mention specific data modalities or structures within the provided content. No clear references to node/edge-based graph data, multi-modal images, or SMILES/fingerprint are given.",
        "Data_domain": "No specific data domain is explicitly mentioned in the provided text. The paper does not reveal whether the data originates from economic, academic, or social network sources.",
        "ML_phase": "Sections 3 and 4 develop the core methodology, outlining the theoretical foundation and its corresponding training framework. In Section 6, the authors detail data preparation and experimental protocols, followed by performance analyses in Section 7."
    },
    "kdd22/2206.07746v3.json": {
        "title": "Condensing Graphs via One-Step Gradient Matching",
        "abs": "As training deep learning models on large dataset takes a lot of time and\nresources, it is desired to construct a small synthetic dataset with which we\ncan train deep learning models sufficiently. There are recent works that have\nexplored solutions on condensing image datasets through complex bi-level\noptimization. For instance, dataset condensation (DC) matches network gradients\nw.r.t. large-real data and small-synthetic data, where the network weights are\noptimized for multiple steps at each outer iteration. However, existing\napproaches have their inherent limitations: (1) they are not directly\napplicable to graphs where the data is discrete; and (2) the condensation\nprocess is computationally expensive due to the involved nested optimization.\nTo bridge the gap, we investigate efficient dataset condensation tailored for\ngraph datasets where we model the discrete graph structure as a probabilistic\nmodel. We further propose a one-step gradient matching scheme, which performs\ngradient matching for only one single step without training the network\nweights. Our theoretical analysis shows this strategy can generate synthetic\ngraphs that lead to lower classification loss on real graphs. Extensive\nexperiments on various graph datasets demonstrate the effectiveness and\nefficiency of the proposed method. In particular, we are able to reduce the\ndataset size by 90% while approximating up to 98% of the original performance\nand our method is significantly faster than multi-step gradient matching (e.g.\n15x in CIFAR10 for synthesizing 500 graphs). Code is available at\n\\url{https://github.com/amazon-research/DosCond}.",
        "Task": "They focus on graph-level classification tasks using multiple benchmarks, such as molecular property prediction (OGB), superpixel classification (CIFAR10), and an e-commerce dataset. They also extend to node-level classification on standard node benchmarks (e.g., Cora, Citeseer, Pubmed).",
        "Data_type": "They focus on graph-structured data characterized by discrete adjacency matrices and node features (e.g., molecular graphs, superpixels, or e-commerce transaction networks). Their method handles homogeneous graphs with continuous or discrete node attributes for graph- or node-level classification tasks.",
        "Data_domain": "The paper focuses on graph-structured data from molecular (OGB, TU datasets), superpixel-based (CIFAR10), and e-commerce transaction domains. These datasets span diverse real-world scenarios, including molecule classification, image-based graph classification, and product-centered subgraphs in a knowledge graph.",
        "ML_phase": "They construct discrete graph structures by parameterizing adjacency matrices with a Bernoulli distribution and apply a reparameterization trick for differentiability. Then, they perform one-step gradient matching on randomly initialized GNN parameters to efficiently learn a condensed set of synthetic graphs, replacing expensive multi-step bi-level optimization in the training framework."
    },
    "kdd22/2206.07743v1.json": {
        "title": "Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective",
        "abs": "Recent years have witnessed remarkable success achieved by graph neural\nnetworks (GNNs) in many real-world applications such as recommendation and drug\ndiscovery. Despite the success, oversmoothing has been identified as one of the\nkey issues which limit the performance of deep GNNs. It indicates that the\nlearned node representations are highly indistinguishable due to the stacked\naggregators. In this paper, we propose a new perspective to look at the\nperformance degradation of deep GNNs, i.e., feature overcorrelation. Through\nempirical and theoretical study on this matter, we demonstrate the existence of\nfeature overcorrelation in deeper GNNs and reveal potential reasons leading to\nthis issue. To reduce the feature correlation, we propose a general framework\nDeCorr which can encourage GNNs to encode less redundant information. Extensive\nexperiments have demonstrated that DeCorr can help enable deeper GNNs and is\ncomplementary to existing techniques tackling the oversmoothing issue.",
        "Task": "They focus on node-level classification, using standard graph benchmarks (e.g., Cora, Citeseer, Pubmed, CoauthorCS) to evaluate performance. The goal is to learn effective node embeddings that improve classification accuracy.",
        "Data_type": "They handle graph-structured data where each node is associated with textual attributes (e.g., bag-of-words features) and edges represent pairwise relations. All experiments are conducted on homogeneous citation networks, with an adjacency matrix and a node feature matrix capturing the text-based attributes.",
        "Data_domain": "They primarily use well-known graph benchmarks from academic citation networks (Cora, Citeseer, Pubmed) and co-author networks (CoauthorCS), as well as web graphs (Chameleon, Texas, Cornell, Wisconsin) and an actor co-occurrence network (Actor). Overall, the data domain spans academic and web-based graph datasets.",
        "ML_phase": "They integrate their proposed DeCorr framework into the GNN training process by adding an explicit decorrelation loss (computed via a Monte Carlo sampling of node representations) and a mutual information maximization loss to enrich encoded features. This framework is layered on top of standard GNN architectures (e.g., GCN, GAT, ChebyNet) so that, during training, the model’s parameters are optimized jointly with these additional regularization components to mitigate overcorrelation."
    },
    "kdd22/2205.15444v2.json": {
        "title": "Integrity Authentication in Tree Models",
        "abs": "Tree models are very widely used in practice of machine learning and data\nmining. In this paper, we study the problem of model integrity authentication\nin tree models. In general, the task of model integrity authentication is the\ndesign \\& implementation of mechanisms for checking/detecting whether the model\ndeployed for the end-users has been tampered with or compromised, e.g.,\nmalicious modifications on the model. We propose an authentication framework\nthat enables the model builders/distributors to embed a signature to the tree\nmodel and authenticate the existence of the signature by only making a small\nnumber of black-box queries to the model. To the best of our knowledge, this is\nthe first study of signature embedding on tree models. Our proposed method\nsimply locates a collection of leaves and modifies their prediction values,\nwhich does not require any training/testing data nor any re-training. The\nexperiments on a large number of public classification datasets confirm that\nthe proposed signature embedding process has a high success rate while only\nintroducing a minimal prediction accuracy loss.",
        "Task": "They address multi-class classification tasks using boosted tree models. The paper evaluates performance on 20 public datasets covering various multi-class scenarios.",
        "Data_type": "They handle standard classification datasets with numeric or textual feature representations (up to 62,061 dimensions) and image-based inputs (up to 3,072 dimensions in CIFAR10, MNIST, SVHN). No graph-structured or SMILES/fingerprint data types are involved.",
        "Data_domain": "The data used in the paper originates from various widely used public academic benchmarks (e.g., CIFAR10, MNIST, SVHN) covering image, text, and tabular classification tasks. These datasets are drawn primarily from open repositories and represent standard benchmarks in the academic machine learning community.",
        "ML_phase": "After training a boosted tree model, the framework locates specific terminal nodes in the ensemble and embeds fragile signatures without retraining. This post-training manipulation in the model pipeline minimally alters predictions while enabling black-box authentication."
    },
    "kdd22/2206.08555v1.json": {
        "title": "SOS: Score-based Oversampling for Tabular Data",
        "abs": "Score-based generative models (SGMs) are a recent breakthrough in generating\nfake images. SGMs are known to surpass other generative models, e.g.,\ngenerative adversarial networks (GANs) and variational autoencoders (VAEs).\nBeing inspired by their big success, in this work, we fully customize them for\ngenerating fake tabular data. In particular, we are interested in oversampling\nminor classes since imbalanced classes frequently lead to sub-optimal training\noutcomes. To our knowledge, we are the first presenting a score-based tabular\ndata oversampling method. Firstly, we re-design our own score network since we\nhave to process tabular data. Secondly, we propose two options for our\ngeneration method: the former is equivalent to a style transfer for tabular\ndata and the latter uses the standard generative policy of SGMs. Lastly, we\ndefine a fine-tuning method, which further enhances the oversampling quality.\nIn our experiments with 6 datasets and 10 baselines, our method outperforms\nother oversampling methods in all cases.",
        "Task": "They address an imbalanced classification task on tabular datasets, where the goal is to accurately classify both majority and minority classes. They evaluate performance on multiple real-world benchmark datasets for binary and multi-class classification.",
        "Data_type": "They focus on tabular data, where each instance is a row with a fixed number of feature columns (both continuous and discrete). The input dimensionality is determined by the number of columns in the table.",
        "Data_domain": "They use a variety of real-world tabular datasets spanning financial credit, e-commerce, surgical patient records, meteorological observations, animal adoption, and remote sensing domains. All of these datasets are structured tables with mixed types of numerical and categorical columns.",
        "ML_phase": "In the ML pipeline, this method appears in the data augmentation phase, where separate score networks (one per class) are trained using forward–reverse SDE processes, followed by an optional fine-tuning step to refine minority class boundaries. It generates synthetic minority samples either by perturbing major-class examples (style transfer) or by sampling from noise, thus balancing the dataset for downstream model training."
    },
    "kdd22/2112.10973v2.json": {
        "title": "Minimizing Congestion for Balanced Dominators",
        "abs": "A primary challenge in metagenomics is reconstructing individual microbial\ngenomes from the mixture of short fragments created by sequencing. Recent work\nleverages the sparsity of the assembly graph to find $r$-dominating sets which\nenable rapid approximate queries through a dominator-centric graph partition.\nIn this paper, we consider two problems related to reducing uncertainty and\nimproving scalability in this setting.\n  First, we observe that nodes with multiple closest dominators necessitate\narbitrary tie-breaking in the existing pipeline. As such, we propose finding\n$\\textit{sparse}$ dominating sets which minimize this effect via a new\n$\\textit{congestion}$ parameter. We prove minimizing congestion is NP-hard, and\ngive an $\\mathcal{O}(\\sqrt{\\Delta^r})$ approximation algorithm, where $\\Delta$\nis the max degree.\n  To improve scalability, the graph should be partitioned into uniformly sized\npieces, subject to placing vertices with a closest dominator. This leads to\n$\\textit{balanced neighborhood partitioning}$: given an $r$-dominating set,\nfind a partition into connected subgraphs with optimal uniformity so that each\nvertex is co-assigned with some closest dominator. Using variance of piece\nsizes to measure uniformity, we show this problem is NP-hard iff $r$ is greater\nthan $1$. We design and analyze several algorithms, including a polynomial-time\napproach which is exact when $r=1$ (and heuristic otherwise).\n  We complement our theoretical results with computational experiments on a\ncorpus of real-world networks showing sparse dominating sets lead to more\nbalanced neighborhood partitionings. Further, on the metagenome\n$\\textsf{HuSB1}$, our approach maintains high query containment and similarity\nwhile reducing piece size variance.",
        "Task": "They address a node-level graph partitioning and covering task, seeking r-dominating sets that minimize “congestion” and produce balanced connected subgraphs. Performance is evaluated on real metagenome assembly graphs and additional network datasets, measuring how well these sets and partitions satisfy coverage and balance objectives.",
        "Data_type": "They operate on large-scale graph-structured data, specifically compact De Bruijn graphs (cDBGs) in which nodes represent DNA subsequences (k-mers) and edges encode possible overlaps. The methods are designed for millions of vertices in such metagenome assembly graphs, focusing on node/edge relationships for efficient partitioning and query operations.",
        "Data_domain": "The paper’s data come predominantly from biological metagenome assembly graphs representing microbial communities (e.g., gut, marine) derived from shotgun sequencing. Additional datasets include real-world networks from repositories (e.g., DIMACS10, Network Data Repository) for comparative evaluation.",
        "ML_phase": "They present a graph partitioning procedure during the data preprocessing stage, producing sparse dominating sets and balanced neighborhoods before downstream tasks. This hierarchical approach reduces data complexity and accelerates subsequent analyses or queries in the ML pipeline."
    },
    "kdd22/2111.00898v2.json": {
        "title": "Availability Attacks Create Shortcuts",
        "abs": "Availability attacks, which poison the training data with imperceptible\nperturbations, can make the data \\emph{not exploitable} by machine learning\nalgorithms so as to prevent unauthorized use of data. In this work, we\ninvestigate why these perturbations work in principle. We are the first to\nunveil an important population property of the perturbations of these attacks:\nthey are almost \\textbf{linearly separable} when assigned with the target\nlabels of the corresponding samples, which hence can work as \\emph{shortcuts}\nfor the learning objective. We further verify that linear separability is\nindeed the workhorse for availability attacks. We synthesize linearly-separable\nperturbations as attacks and show that they are as powerful as the deliberately\ncrafted attacks. Moreover, such synthetic perturbations are much easier to\ngenerate. For example, previous attacks need dozens of hours to generate\nperturbations for ImageNet while our algorithm only needs several seconds. Our\nfinding also suggests that the \\emph{shortcut learning} is more widely present\nthan previously believed as deep models would rely on shortcuts even if they\nare of an imperceptible scale and mixed together with the normal features. Our\nsource code is published at\n\\url{https://github.com/dayu11/Availability-Attacks-Create-Shortcuts}.",
        "Task": "They address a supervised image classification task, targeting models on datasets including CIFAR-10, CIFAR-100, SVHN, a subset of ImageNet, and a face recognition set (WebFace). Their goal is to evaluate how these methods degrade classification performance on clean data when training is done on poisoned or “unlearnable” examples.",
        "Data_type": "They handle standard image data drawn from benchmarks such as CIFAR-10, CIFAR-100, SVHN, ImageNet, and WebFace. All perturbations are confined to pixel-level manipulations, with no textual, graph-structured, or multi-modal content addressed.",
        "Data_domain": "The paper’s data come from standard image classification benchmarks (CIFAR-10, CIFAR-100, SVHN, and a subset of ImageNet). It also includes a face dataset (WebFace) for experiments on facial recognition tasks.",
        "ML_phase": "They introduce a data preprocessing step that generates synthetic linearly separable perturbations (via normal distributions and patch-based duplication) and injects them into the training set. The poisoned dataset is then fed into a standard training pipeline (e.g., ResNet architectures with typical SGD) without altering the remaining training framework."
    },
    "kdd22/2205.14358v2.json": {
        "title": "Fair Labeled Clustering",
        "abs": "Numerous algorithms have been produced for the fundamental problem of\nclustering under many different notions of fairness. Perhaps the most common\nfamily of notions currently studied is group fairness, in which proportional\ngroup representation is ensured in every cluster. We extend this direction by\nconsidering the downstream application of clustering and how group fairness\nshould be ensured for such a setting. Specifically, we consider a common\nsetting in which a decision-maker runs a clustering algorithm, inspects the\ncenter of each cluster, and decides an appropriate outcome (label) for its\ncorresponding cluster. In hiring for example, there could be two outcomes,\npositive (hire) or negative (reject), and each cluster would be assigned one of\nthese two outcomes. To ensure group fairness in such a setting, we would desire\nproportional group representation in every label but not necessarily in every\ncluster as is done in group fair clustering. We provide algorithms for such\nproblems and show that in contrast to their NP-hard counterparts in group fair\nclustering, they permit efficient solutions. We also consider a well-motivated\nalternative setting where the decision-maker is free to assign labels to the\nclusters regardless of the centers' positions in the metric space. We show that\nthis setting exhibits interesting transitions from computationally hard to easy\naccording to additional constraints on the problem. Moreover, when the\nconstraint parameters take on natural values we show a randomized algorithm for\nthis setting that always achieves an optimal clustering and satisfies the\nfairness constraints in expectation. Finally, we run experiments on real world\ndatasets that validate the effectiveness of our algorithms.",
        "Task": "They introduce a fair labeled clustering task in an unsupervised learning setting. They evaluate performance on real-world datasets (e.g., UCI Adult and Credit) by measuring clustering cost and fairness metrics.",
        "Data_type": "They focus on data represented as points in a metric space (e.g., numeric features) with a categorical “color” (group membership) attribute. The methods can thus be applied to tabular or any other modality that can be embedded in a metric space and annotated with group identifiers.",
        "Data_domain": "These datasets primarily come from the UCI repository and encompass demographic (e.g., Adult, Census) and financial (e.g., Credit) information. They include attributes such as age, marital status, and financial records for large populations.",
        "ML_phase": "They introduce a post-processing stage in clustering, where after centers are either fixed or found (e.g., via k-means++), the algorithm assigns labels under group-fairness constraints. It uses flow-based or dynamic programming methods to minimize clustering objectives while enforcing proportional demographic bounds."
    },
    "kdd22/2301.09210v1.json": {
        "title": "Debiasing the Cloze Task in Sequential Recommendation with Bidirectional  Transformers",
        "abs": "Bidirectional Transformer architectures are state-of-the-art sequential\nrecommendation models that use a bi-directional representation capacity based\non the Cloze task, a.k.a. Masked Language Modeling. The latter aims to predict\nrandomly masked items within the sequence. Because they assume that the true\ninteracted item is the most relevant one, an exposure bias results, where\nnon-interacted items with low exposure propensities are assumed to be\nirrelevant. The most common approach to mitigating exposure bias in\nrecommendation has been Inverse Propensity Scoring (IPS), which consists of\ndown-weighting the interacted predictions in the loss function in proportion to\ntheir propensities of exposure, yielding a theoretically unbiased learning. In\nthis work, we argue and prove that IPS does not extend to sequential\nrecommendation because it fails to account for the temporal nature of the\nproblem. We then propose a novel propensity scoring mechanism, which can\ntheoretically debias the Cloze task in sequential recommendation. Finally we\nempirically demonstrate the debiasing capabilities of our proposed approach and\nits robustness to the severity of exposure bias.",
        "Task": "No specific machine learning task or dataset is described in this paper. The content focuses on publication formatting guidelines rather than proposing or evaluating any particular ML task.",
        "Data_type": "This paper exclusively focuses on multi-language textual data (e.g., titles, abstracts), without covering structured, multi-modal, or other specialized data formats. It does not discuss handling images, graphs, or fingerprint/SMILES data.",
        "Data_domain": "The paper does not present any distinct dataset and instead focuses on academic writing and formatting guidelines. Hence, its data domain can be classified as academic.",
        "ML_phase": "The paper does not describe any processes or techniques related to data collection, feature engineering, or algorithmic training. Consequently, it provides no insight into the model development or training phases of the ML pipeline."
    },
    "kdd23/2308.00279v1.json": {
        "title": "Robust Positive-Unlabeled Learning via Noise Negative Sample  Self-correction",
        "abs": "Learning from positive and unlabeled data is known as positive-unlabeled (PU)\nlearning in literature and has attracted much attention in recent years. One\ncommon approach in PU learning is to sample a set of pseudo-negatives from the\nunlabeled data using ad-hoc thresholds so that conventional supervised methods\ncan be applied with both positive and negative samples. Owing to the label\nuncertainty among the unlabeled data, errors of misclassifying unlabeled\npositive samples as negative samples inevitably appear and may even accumulate\nduring the training processes. Those errors often lead to performance\ndegradation and model instability. To mitigate the impact of label uncertainty\nand improve the robustness of learning with positive and unlabeled data, we\npropose a new robust PU learning method with a training strategy motivated by\nthe nature of human learning: easy cases should be learned first. Similar\nintuition has been utilized in curriculum learning to only use easier cases in\nthe early stage of training before introducing more complex cases.\nSpecifically, we utilize a novel ``hardness'' measure to distinguish unlabeled\nsamples with a high chance of being negative from unlabeled samples with large\nlabel noise. An iterative training strategy is then implemented to fine-tune\nthe selection of negative samples during the training process in an iterative\nmanner to include more ``easy'' samples in the early stage of training.\nExtensive experimental validations over a wide range of learning tasks show\nthat this approach can effectively improve the accuracy and stability of\nlearning with positive and unlabeled data. Our code is available at\nhttps://github.com/woriazzc/Robust-PU",
        "Task": "They propose a binary classification task under a positive-unlabeled learning framework. Performance is evaluated on multiple real-world datasets where only partial positive labels and a large pool of unlabeled instances are provided.",
        "Data_type": "They primarily handle single-modality data in the form of 2D images (e.g., CIFAR-10, STL-10, MNIST, F-MNIST, Alzheimer) and tabular inputs (mushroom, shuttle, spambase). The paper focuses on classification settings with these real-valued or image-based datasets and does not extend to graph-structured or text-based data.",
        "Data_domain": "The paper utilizes image-based datasets (CIFAR-10, STL-10, MNIST, F-MNIST, Alzheimer) and tabular datasets from the UCI repository (mushroom, shuttle, spambase). These collections span common image recognition tasks and tabular classification scenarios.",
        "ML_phase": "They propose a three-stage iterative training framework that first measures sample hardness, then updates sample weights, and finally conducts weighted supervised training. This framework reinterprets PU learning as a repeated supervised process by dynamically adjusting unlabeled data’s influence at each iteration."
    },
    "kdd23/2306.14126v1.json": {
        "title": "Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic  Adversarial Training",
        "abs": "Machine learning-based forecasting models are commonly used in Intelligent\nTransportation Systems (ITS) to predict traffic patterns and provide city-wide\nservices. However, most of the existing models are susceptible to adversarial\nattacks, which can lead to inaccurate predictions and negative consequences\nsuch as congestion and delays. Therefore, improving the adversarial robustness\nof these models is crucial for ITS. In this paper, we propose a novel framework\nfor incorporating adversarial training into spatiotemporal traffic forecasting\ntasks. We demonstrate that traditional adversarial training methods designated\nfor static domains cannot be directly applied to traffic forecasting tasks, as\nthey fail to effectively defend against dynamic adversarial attacks. Then, we\npropose a reinforcement learning-based method to learn the optimal node\nselection strategy for adversarial examples, which simultaneously strengthens\nthe dynamic attack defense capability and reduces the model overfitting.\nAdditionally, we introduce a self-knowledge distillation regularization module\nto overcome the \"forgetting issue\" caused by continuously changing adversarial\nnodes during training. We evaluate our approach on two real-world traffic\ndatasets and demonstrate its superiority over other baselines. Our method\neffectively enhances the adversarial robustness of spatiotemporal traffic\nforecasting models. The source code for our framework is available at\nhttps://github.com/usail-hkust/RDAT.",
        "Task": "They tackle a spatiotemporal traffic forecasting problem framed as a multi-step time-series regression task. They evaluate performance on real-world traffic speed and flow datasets (PEMS-BAY, PEMS-D4).",
        "Data_type": "They use graph-structured spatiotemporal traffic data, where nodes represent geo-distributed sensors with time-varying attributes (e.g., speed and volume) and edges encode road connectivity. All features are node-level signals in a homogeneous traffic network graph.",
        "Data_domain": "The datasets come from real-world spatiotemporal traffic measurements (e.g., PEMS-BAY and PEMS-D4 in California). They encompass sensor-based data (such as speed and flow) collected over time for traffic forecasting.",
        "ML_phase": "They introduce a training framework that integrates dynamically generated adversarial nodes (selected via a reinforcement learning-based policy network) into a spatiotemporal forecasting model’s training loop. This process includes data preprocessing for node features, a spatiotemporal encoder-decoder architecture for node embeddings, and a self-distillation regularization step to stabilize adversarial training."
    },
    "kdd23/2307.09858v1.json": {
        "title": "Towards Reliable Rare Category Analysis on Graphs via Individual  Calibration",
        "abs": "Rare categories abound in a number of real-world networks and play a pivotal\nrole in a variety of high-stakes applications, including financial fraud\ndetection, network intrusion detection, and rare disease diagnosis. Rare\ncategory analysis (RCA) refers to the task of detecting, characterizing, and\ncomprehending the behaviors of minority classes in a highly-imbalanced data\ndistribution. While the vast majority of existing work on RCA has focused on\nimproving the prediction performance, a few fundamental research questions\nheretofore have received little attention and are less explored: How confident\nor uncertain is a prediction model in rare category analysis? How can we\nquantify the uncertainty in the learning process and enable reliable rare\ncategory analysis?\n  To answer these questions, we start by investigating miscalibration in\nexisting RCA methods. Empirical results reveal that state-of-the-art RCA\nmethods are mainly over-confident in predicting minority classes and\nunder-confident in predicting majority classes. Motivated by the observation,\nwe propose a novel individual calibration framework, named CALIRARE, for\nalleviating the unique challenges of RCA, thus enabling reliable rare category\nanalysis. In particular, to quantify the uncertainties in RCA, we develop a\nnode-level uncertainty quantification algorithm to model the overlapping\nsupport regions with high uncertainty; to handle the rarity of minority classes\nin miscalibration calculation, we generalize the distribution-based calibration\nmetric to the instance level and propose the first individual calibration\nmeasurement on graphs named Expected Individual Calibration Error (EICE). We\nperform extensive experimental evaluations on real-world datasets, including\nrare category characterization and model calibration tasks, which demonstrate\nthe significance of our proposed framework.",
        "Task": "They propose a node-level classification task to identify rare categories in graph-structured data. Their evaluation uses five real-world graph benchmarks (Cora, CiteSeer, PubMed, DBLP, and FaceBook) to assess classification performance.",
        "Data_type": "They focus on graph-structured data with node-level textual attributes in homogeneous networks (e.g., citation and social graphs). Each node is described by textual features, and edges capture relationships among these nodes.",
        "Data_domain": "They focus on real-world graph-structured data drawn from diverse sources, notably academic citation networks (Cora, CiteSeer, PubMed, DBLP) and a social network dataset (Facebook). Additionally, they motivate their work using examples from financial transaction networks, network security, and healthcare (patient-symptom networks).",
        "ML_phase": "They introduce an end-to-end GCN-based framework with a novel instance-level calibration term integrated into the training objective (combining cross-entropy loss and EICE) to jointly characterize rare categories and calibrate confidence. The pipeline includes cost-sensitive label weighting, instance-level uncertainty estimation via influence functions (instead of costly leave-one-out retraining), and final prediction with calibrated probabilities."
    },
    "kdd23/2306.07919v1.json": {
        "title": "Skill Disentanglement for Imitation Learning from Suboptimal  Demonstrations",
        "abs": "Imitation learning has achieved great success in many sequential\ndecision-making tasks, in which a neural agent is learned by imitating\ncollected human demonstrations. However, existing algorithms typically require\na large number of high-quality demonstrations that are difficult and expensive\nto collect. Usually, a trade-off needs to be made between demonstration quality\nand quantity in practice. Targeting this problem, in this work we consider the\nimitation of sub-optimal demonstrations, with both a small clean demonstration\nset and a large noisy set. Some pioneering works have been proposed, but they\nsuffer from many limitations, e.g., assuming a demonstration to be of the same\noptimality throughout time steps and failing to provide any interpretation\nw.r.t knowledge learned from the noisy set. Addressing these problems, we\npropose {\\method} by evaluating and imitating at the sub-demonstration level,\nencoding action primitives of varying quality into different skills.\nConcretely, {\\method} consists of a high-level controller to discover skills\nand a skill-conditioned module to capture action-taking policies, and is\ntrained following a two-phase pipeline by first discovering skills with all\ndemonstrations and then adapting the controller to only the clean set. A\nmutual-information-based regularization and a dynamic sub-demonstration\noptimality estimator are designed to promote disentanglement in the skill\nspace. Extensive experiments are conducted over two gym environments and a\nreal-world healthcare dataset to demonstrate the superiority of {\\method} in\nlearning from sub-optimal demonstrations and its improved interpretability by\nexamining learned skills.",
        "Task": "They address an imitation learning task of learning a policy from both expert and sub-optimal demonstrations. Performance is evaluated on synthetic (FourRoom, DoorKey) and real-world (Mimic-IV) demonstration datasets.",
        "Data_type": "They process sequential demonstration data consisting of state-action pairs, where states can be grid positions or patient physiological measurements (numeric or discrete), and actions are discrete (e.g., directional moves or medication prescriptions). Each demonstration is a trajectory of transitions (state-action pairs) that capture noisy or expert behavior over time.",
        "Data_domain": "The paper’s datasets include synthetic navigation tasks (FourRoom, DoorKey) and real-world electronic health records from MIMIC-IV. These navigation tasks revolve around agent movement in maze-like environments, whereas MIMIC-IV provides patient treatment trajectories from a clinical domain.",
        "ML_phase": "They introduce a two-phase hierarchical training pipeline that first discovers latent skills from both small clean and large noisy demonstrations using mutual-information constraints, deep embedding clustering, and positive-unlabeled learning. In the second phase, these discovered skills are adapted by focusing on the clean demonstrations to refine an expert-level policy."
    },
    "kdd23/2306.14403v1.json": {
        "title": "Anomaly Detection with Score Distribution Discrimination",
        "abs": "Recent studies give more attention to the anomaly detection (AD) methods that\ncan leverage a handful of labeled anomalies along with abundant unlabeled data.\nThese existing anomaly-informed AD methods rely on manually predefined score\ntarget(s), e.g., prior constant or margin hyperparameter(s), to realize\ndiscrimination in anomaly scores between normal and abnormal data. However,\nsuch methods would be vulnerable to the existence of anomaly contamination in\nthe unlabeled data, and also lack adaptation to different data scenarios. In\nthis paper, we propose to optimize the anomaly scoring function from the view\nof score distribution, thus better retaining the diversity and more\nfine-grained information of input data, especially when the unlabeled data\ncontains anomaly noises in more practical AD scenarios. We design a novel loss\nfunction called Overlap loss that minimizes the overlap area between the score\ndistributions of normal and abnormal samples, which no longer depends on prior\nanomaly score targets and thus acquires adaptability to various datasets.\nOverlap loss consists of Score Distribution Estimator and Overlap Area\nCalculation, which are introduced to overcome challenges when estimating\narbitrary score distributions, and to ensure the boundness of training loss. As\na general loss component, Overlap loss can be effectively integrated into\nmultiple network architectures for constructing AD models. Extensive\nexperimental results indicate that Overlap loss based AD models significantly\noutperform their state-of-the-art counterparts, and achieve better performance\non different types of anomalies.",
        "Task": "They tackle an anomaly detection task, which involves classifying unusual data points (anomalies) from normal samples. To evaluate performance, they use multiple real-world datasets with partially labeled anomalies.",
        "Data_type": "They primarily evaluate on tabular datasets (e.g., disease diagnosis, speech recognition, and image identification in tabular form) but also include image-based data (e.g., MNIST) and speech features. Their experiments span a total of more than twenty real-world datasets covering these diverse data modalities.",
        "Data_domain": "The paper’s experiments use heterogeneous real-world datasets drawn from multiple domains, including disease diagnosis, speech recognition, and image identification. These sources capture varying anomaly patterns across medical, acoustic, and visual contexts.",
        "ML_phase": "They employ a neural network that outputs anomaly scores for both unlabeled and labeled samples, then estimate these scores’ distributions (via KDE) to calculate and minimize their overlap. This Overlap loss is integrated into an end-to-end training framework (e.g., MLP, AutoEncoder, ResNet, Transformer) to adaptively separate normal and abnormal data without predefined score targets."
    },
    "kdd23/2306.07432v1.json": {
        "title": "FIRE: An Optimization Approach for Fast Interpretable Rule Extraction",
        "abs": "We present FIRE, Fast Interpretable Rule Extraction, an optimization-based\nframework to extract a small but useful collection of decision rules from tree\nensembles. FIRE selects sparse representative subsets of rules from tree\nensembles, that are easy for a practitioner to examine. To further enhance the\ninterpretability of the extracted model, FIRE encourages fusing rules during\nselection, so that many of the selected decision rules share common\nantecedents. The optimization framework utilizes a fusion regularization\npenalty to accomplish this, along with a non-convex sparsity-inducing penalty\nto aggressively select rules. Optimization problems in FIRE pose a challenge to\noff-the-shelf solvers due to problem scale and the non-convexity of the\npenalties. To address this, making use of problem-structure, we develop a\nspecialized solver based on block coordinate descent principles; our solver\nperforms up to 40x faster than existing solvers. We show in our experiments\nthat FIRE outperforms state-of-the-art rule ensemble algorithms at building\nsparse rule sets, and can deliver more interpretable models compared to\nexisting methods.",
        "Task": "They address supervised learning (classification and regression) by extracting interpretable rule ensembles from tree-based models. Their experiments measure performance (e.g., test error or mean squared error) on multiple real-world datasets, including OpenML benchmarks and a Census application.",
        "Data_type": "They handle standard tabular data with N samples × P features (both numeric and categorical) for extracting decision rules from tree ensembles. No mention is made of text, images, or graph-structured inputs, so the framework is not intended for multi-modal or more complex data formats.",
        "Data_domain": "They evaluate their approach on multiple real-world tabular datasets from the OpenML repository, covering domains such as economic indicators (e.g., Stock Price Prediction), housing (e.g., California Housing), and other structured data (e.g., Elevators). Additionally, they showcase a case study using US Census Planning Database records to analyze tract response rates.",
        "ML_phase": "They present a specialized post-training framework that extracts sparse, interpretable rule sets from trained tree ensembles via an optimization-based approach. This involves a novel block coordinate descent solver that assigns and updates rule weights under non-convex and fusion penalties to efficiently produce a final compact model."
    },
    "kdd23/2306.15154v1.json": {
        "title": "Contrastive Meta-Learning for Few-shot Node Classification",
        "abs": "Few-shot node classification, which aims to predict labels for nodes on\ngraphs with only limited labeled nodes as references, is of great significance\nin real-world graph mining tasks. Particularly, in this paper, we refer to the\ntask of classifying nodes in classes with a few labeled nodes as the few-shot\nnode classification problem. To tackle such a label shortage issue, existing\nworks generally leverage the meta-learning framework, which utilizes a number\nof episodes to extract transferable knowledge from classes with abundant\nlabeled nodes and generalizes the knowledge to other classes with limited\nlabeled nodes. In essence, the primary aim of few-shot node classification is\nto learn node embeddings that are generalizable across different classes. To\naccomplish this, the GNN encoder must be able to distinguish node embeddings\nbetween different classes, while also aligning embeddings for nodes in the same\nclass. Thus, in this work, we propose to consider both the intra-class and\ninter-class generalizability of the model. We create a novel contrastive\nmeta-learning framework on graphs, named COSMIC, with two key designs. First,\nwe propose to enhance the intra-class generalizability by involving a\ncontrastive two-step optimization in each episode to explicitly align node\nembeddings in the same classes. Second, we strengthen the inter-class\ngeneralizability by generating hard node classes via a novel\nsimilarity-sensitive mix-up strategy. Extensive experiments on few-shot node\nclassification datasets verify the superiority of our framework over\nstate-of-the-art baselines. Our code is provided at\nhttps://github.com/SongW-SW/COSMIC.",
        "Task": "They study few-shot node classification, where the goal is to assign labels to graph nodes with only limited labeled examples per class. Performance is assessed on four benchmark node classification datasets (CoraFull, ogbn-arxiv, Coauthor-CS, and DBLP).",
        "Data_type": "They work on homogeneous, attributed graphs where each node is associated with textual features (e.g., paper abstracts or keywords). Edges typically represent pairwise relationships (such as co-authorship or citations), and the framework is designed for few-shot node classification on these graph-structured data.",
        "Data_domain": "These experiments focus on academic citation and co-authorship networks. Concretely, the datasets (CoraFull, ogbn-arxiv, Coauthor-CS, DBLP) all originate from scholarly publications, citation relations, and author collaborations in the academic domain.",
        "ML_phase": "They first extract subgraphs for each node via Personalized PageRank to incorporate local structure, then conduct episodic training using a two-step optimization (contrastive learning on support nodes followed by cross-entropy on query nodes), with a similarity-based mix-up generating additional classes. Finally, a simple classifier is fine-tuned on the learned node embeddings in meta-test tasks to perform label prediction."
    },
    "kdd23/2306.13102v1.json": {
        "title": "MBrain: A Multi-channel Self-Supervised Learning Framework for Brain  Signals",
        "abs": "Brain signals are important quantitative data for understanding physiological\nactivities and diseases of human brain. Most existing studies pay attention to\nsupervised learning methods, which, however, require high-cost clinical labels.\nIn addition, the huge difference in the clinical patterns of brain signals\nmeasured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to\nthe lack of a unified method. To handle the above issues, we propose to study\nthe self-supervised learning (SSL) framework for brain signals that can be\napplied to pre-train either SEEG or EEG data. Intuitively, brain signals,\ngenerated by the firing of neurons, are transmitted among different connecting\nstructures in human brain. Inspired by this, we propose MBrain to learn\nimplicit spatial and temporal correlations between different channels (i.e.,\ncontacts of the electrode, corresponding to different brain areas) as the\ncornerstone for uniformly modeling different types of brain signals.\nSpecifically, we represent the spatial correlation by a graph structure, which\nis built with proposed multi-channel CPC. We theoretically prove that\noptimizing the goal of multi-channel CPC can lead to a better predictive\nrepresentation and apply the instantaneou-time-shift prediction task based on\nit. Then we capture the temporal correlation by designing the\ndelayed-time-shift prediction task. Finally, replace-discriminative-learning\ntask is proposed to preserve the characteristics of each channel. Extensive\nexperiments of seizure detection on both EEG and SEEG large-scale real-world\ndatasets demonstrate that our model outperforms several state-of-the-art time\nseries SSL and unsupervised models, and has the ability to be deployed to\nclinical practice.",
        "Task": "They address a binary classification task of seizure detection from multi-channel brain signals (EEG and SEEG). The evaluation is performed at the channel-level on large-scale real-world datasets.",
        "Data_type": "The paper deals with multi-channel time-series data of EEG and SEEG signals, where each channel corresponds to a contact recording brain activity over time. They handle correlation graphs derived from these channels to exploit inter-channel relationships in the raw signals.",
        "Data_domain": "These datasets originate from clinical neuroscience, specifically large-scale EEG and SEEG recordings collected in hospital settings and from a public seizure database. They contain patient brain signals for seizure detection and emotion recognition tasks.",
        "ML_phase": "They design a multi-channel CPC-based self-supervised framework with a learnable correlation graph among channels, augmented by three auxiliary tasks (instantaneous time shift, delayed time shift, and replace discriminative). After pre-training on unlabeled segments, the model is fine-tuned on labeled data with a low learning rate for the final classification objective."
    },
    "kdd23/2306.11264v1.json": {
        "title": "GraphGLOW: Universal and Generalizable Structure Learning for Graph  Neural Networks",
        "abs": "Graph structure learning is a well-established problem that aims at\noptimizing graph structures adaptive to specific graph datasets to help message\npassing neural networks (i.e., GNNs) to yield effective and robust node\nembeddings. However, the common limitation of existing models lies in the\nunderlying \\textit{closed-world assumption}: the testing graph is the same as\nthe training graph. This premise requires independently training the structure\nlearning model from scratch for each graph dataset, which leads to prohibitive\ncomputation costs and potential risks for serious over-fitting. To mitigate\nthese issues, this paper explores a new direction that moves forward to learn a\nuniversal structure learning model that can generalize across graph datasets in\nan open world. We first introduce the mathematical definition of this novel\nproblem setting, and describe the model formulation from a probabilistic\ndata-generative aspect. Then we devise a general framework that coordinates a\nsingle graph-shared structure learner and multiple graph-specific GNNs to\ncapture the generalizable patterns of optimal message-passing topology across\ndatasets. The well-trained structure learner can directly produce adaptive\nstructures for unseen target graphs without any fine-tuning. Across diverse\ndatasets and various challenging cross-graph generalization protocols, our\nexperiments show that even without training on target graphs, the proposed\nmodel i) significantly outperforms expressive GNNs trained on input\n(non-optimized) topology, and ii) surprisingly performs on par with\nstate-of-the-art models that independently optimize adaptive structures for\nspecific target graphs, with notably orders-of-magnitude acceleration for\ntraining on the target graph.",
        "Task": "They tackle a node-level classification task, where the goal is to accurately predict node labels while learning adaptive graph structures that can generalize across multiple source graphs to unseen target graphs. They validate performance on real-world citation and social network datasets under various distribution shifts.",
        "Data_type": "They handle node-level data on homogeneous graph-structured datasets (e.g., citation and social networks) with textual or numeric node attributes. The paper specifically focuses on adjacency matrices describing edges and node feature matrices for classification tasks across multiple graphs under distribution shifts.",
        "Data_domain": "The data come from real-world graph datasets in two main domains: academic citation networks (Cora, CiteSeer, PubMed) and social networks (Facebook-100). These graphs present distinct distribution characteristics, including varying sizes and homophily ratios.",
        "ML_phase": "They devise a two-stage process: first, a shared structure learner and multiple GNNs are jointly trained on source graphs; then, for any new target graph, the fixed structure learner infers latent edges while only the GNN is trained. The pipeline involves iterative updates of node embeddings and a pivot-based bipartite mechanism for scalability, combined in a variational objective that seamlessly integrates predictive loss and structural regularization."
    },
    "kdd23/2307.00553v2.json": {
        "title": "Partial-label Learning with Mixed Closed-set and Open-set  Out-of-candidate Examples",
        "abs": "Partial-label learning (PLL) relies on a key assumption that the true label\nof each training example must be in the candidate label set. This restrictive\nassumption may be violated in complex real-world scenarios, and thus the true\nlabel of some collected examples could be unexpectedly outside the assigned\ncandidate label set. In this paper, we term the examples whose true label is\noutside the candidate label set OOC (out-of-candidate) examples, and pioneer a\nnew PLL study to learn with OOC examples. We consider two types of OOC examples\nin reality, i.e., the closed-set/open-set OOC examples whose true label is\ninside/outside the known label space. To solve this new PLL problem, we first\ncalculate the wooden cross-entropy loss from candidate and non-candidate labels\nrespectively, and dynamically differentiate the two types of OOC examples based\non specially designed criteria. Then, for closed-set OOC examples, we conduct\nreversed label disambiguation in the non-candidate label set; for open-set OOC\nexamples, we leverage them for training by utilizing an effective\nregularization strategy that dynamically assigns random candidate labels from\nthe candidate label set. In this way, the two types of OOC examples can be\ndifferentiated and further leveraged for model training. Extensive experiments\ndemonstrate that our proposed method outperforms state-of-the-art PLL methods.",
        "Task": "They tackle a multi-class image classification task involving both in-distribution and out-of-distribution (OOC) samples. They evaluate their method on CIFAR-10 and SVHN datasets to measure classification performance.",
        "Data_type": "They primarily use and evaluate on standard image datasets (e.g., CIFAR-10 and SVHN). The paper focuses on image-based classification settings with artificially generated noisy or partially labeled data.",
        "Data_domain": "They primarily employ image-classification benchmarks (e.g., CIFAR-10, SVHN) and akin artificially generated datasets. Thus, the data domain is computer-vision/image-centric.",
        "ML_phase": "They employ an iterative selection pipeline to identify normal, closed-set OOC, and open-set OOC examples, leveraging reversed label disambiguation and random candidate generation to refine label confidence. This is integrated with a standard cross-entropy-based training framework, incorporating a hyper-parameter tuning scheme for real-world usage."
    },
    "kdd23/2307.02946v1.json": {
        "title": "Finding Favourite Tuples on Data Streams with Provably Few Comparisons",
        "abs": "One of the most fundamental tasks in data science is to assist a user with\nunknown preferences in finding high-utility tuples within a large database. To\naccurately elicit the unknown user preferences, a widely-adopted way is by\nasking the user to compare pairs of tuples. In this paper, we study the problem\nof identifying one or more high-utility tuples by adaptively receiving user\ninput on a minimum number of pairwise comparisons. We devise a single-pass\nstreaming algorithm, which processes each tuple in the stream at most once,\nwhile ensuring that the memory size and the number of requested comparisons are\nin the worst case logarithmic in $n$, where $n$ is the number of all tuples. An\nimportant variant of the problem, which can help to reduce human error in\ncomparisons, is to allow users to declare ties when confronted with pairs of\ntuples of nearly equal utility. We show that the theoretical guarantees of our\nmethod can be maintained for this important problem variant. In addition, we\nshow how to enhance existing pruning techniques in the literature by leveraging\npowerful tools from mathematical programming. Finally, we systematically\nevaluate all proposed algorithms over both synthetic and real-life datasets,\nexamine their scalability, and demonstrate their superior performance over\nexisting methods.",
        "Task": "They address an interactive ranking task, where the goal is to identify a high-utility tuple (near the top) from a large dataset using pairwise comparisons. The approach minimizes the number of comparisons (queries) required to find such a tuple and is evaluated on both synthetic and real-world datasets.",
        "Data_type": "They handle tabular data represented as d-dimensional real-valued vectors. Each tuple is a point in ℝ^d with continuous numerical attributes.",
        "Data_domain": "Data is drawn from large relational tables spanning diverse real-life domains, including sports (player), real estate (house), automotive (car), gaming (game), and social media (youtube). These datasets provide numerical features for each tuple in contexts such as performance statistics, property attributes, or video metadata.",
        "ML_phase": "They introduce a single-pass streaming framework in which tuples arrive in random order, and the algorithm adaptively queries users for pairwise comparisons to prune suboptimal candidates. This process integrates into a preference-learning pipeline by maintaining minimal memory, updating a shortlist of potential best tuples, and ensuring logarithmic query complexity in the worst case."
    },
    "kdd23/2307.02912v1.json": {
        "title": "LEA: Improving Sentence Similarity Robustness to Typos Using Lexical  Attention Bias",
        "abs": "Textual noise, such as typos or abbreviations, is a well-known issue that\npenalizes vanilla Transformers for most downstream tasks. We show that this is\nalso the case for sentence similarity, a fundamental task in multiple domains,\ne.g. matching, retrieval or paraphrasing. Sentence similarity can be approached\nusing cross-encoders, where the two sentences are concatenated in the input\nallowing the model to exploit the inter-relations between them. Previous works\naddressing the noise issue mainly rely on data augmentation strategies, showing\nimproved robustness when dealing with corrupted samples that are similar to the\nones used for training. However, all these methods still suffer from the token\ndistribution shift induced by typos. In this work, we propose to tackle textual\nnoise by equipping cross-encoders with a novel LExical-aware Attention module\n(LEA) that incorporates lexical similarities between words in both sentences.\nBy using raw text similarities, our approach avoids the tokenization shift\nproblem obtaining improved robustness. We demonstrate that the attention bias\nintroduced by LEA helps cross-encoders to tackle complex scenarios with textual\nnoise, specially in domains with short-text descriptions and limited context.\nExperiments using three popular Transformer encoders in five e-commerce\ndatasets for product matching show that LEA consistently boosts performance\nunder the presence of noise, while remaining competitive on the original\n(clean) splits. We also evaluate our approach in two datasets for textual\nentailment and paraphrasing showing that LEA is robust to typos in domains with\nlonger sentences and more natural context. Additionally, we thoroughly analyze\nseveral design choices in our approach, providing insights about the impact of\nthe decisions made and fostering future research in cross-encoders dealing with\ntypos.",
        "Task": "They tackle a sentence-level binary classification task for textual similarity, primarily in e-commerce product matching. They also evaluate this classification approach on textual entailment (RTE) and paraphrasing (MRPC) datasets.",
        "Data_type": "This work focuses on short textual data, particularly product descriptions and other sentence-pair tasks, with a strong emphasis on handling noise such as typos and abbreviations. It also evaluates performance on additional textual entailment and paraphrasing datasets, highlighting applicability to a range of noisy text matching scenarios.",
        "Data_domain": "The paper’s primary datasets come from e-commerce product matching (Abt-Buy, Amazon-Google, WDC-Computers), containing short product descriptions. It also includes textual entailment (RTE) and paraphrasing (MRPC) benchmarks from the general NLP domain.",
        "ML_phase": "They insert a lexical-attention bias into the cross-encoder’s self-attention layers and apply synthetic data augmentation during training to handle textual noise. The models are then fine-tuned with standard optimization pipelines, focusing on late-layer integration of the bias for improved robustness in sentence similarity tasks."
    },
    "kdd23/2406.19244v1.json": {
        "title": "Improving the Expressiveness of $K$-hop Message-Passing GNNs by  Injecting Contextualized Substructure Information",
        "abs": "Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.",
        "Task": "They conduct graph-level classification on datasets such as MUTAG, PROTEINS, and IMDB, and perform graph-level regression on synthetic (graph property/substructure counting) and QM9 molecular datasets. Their experiments demonstrate improved performance in both classification and regression tasks.",
        "Data_type": "They work with graph-structured data possessing node attributes in homogeneous graphs. Datasets include molecules, social networks, and other domains where nodes carry continuous or discrete features for downstream graph-level tasks.",
        "Data_domain": "They primarily utilize graph-structured data drawn from social networks, drug discovery datasets, recommendation systems, and bioinformatics. Real-world benchmarks include molecule graphs and network datasets from biological and social domains.",
        "ML_phase": "They first compute substructure features via multi-step random walks as a preprocessing step, then incorporate these features into a K-hop message-passing GNN architecture. The entire network (SEK-GNN) is trained end-to-end within a standard GNN training framework using message-passing and update functions for each hop."
    },
    "kdd23/2307.11004v1.json": {
        "title": "Efficient and Joint Hyperparameter and Architecture Search for  Collaborative Filtering",
        "abs": "Automated Machine Learning (AutoML) techniques have recently been introduced\nto design Collaborative Filtering (CF) models in a data-specific manner.\nHowever, existing works either search architectures or hyperparameters while\nignoring the fact they are intrinsically related and should be considered\ntogether. This motivates us to consider a joint hyperparameter and architecture\nsearch method to design CF models. However, this is not easy because of the\nlarge search space and high evaluation cost. To solve these challenges, we\nreduce the space by screening out usefulness yperparameter choices through a\ncomprehensive understanding of individual hyperparameters. Next, we propose a\ntwo-stage search algorithm to find proper configurations from the reduced\nspace. In the first stage, we leverage knowledge from subsampled datasets to\nreduce evaluation costs; in the second stage, we efficiently fine-tune top\ncandidate models on the whole dataset. Extensive experiments on real-world\ndatasets show better performance can be achieved compared with both\nhand-designed and previous searched models. Besides, ablation and case studies\ndemonstrate the effectiveness of our search framework.",
        "Task": "They aim to solve a top-K item recommendation task based on collaborative filtering. The performance is evaluated on several real-world user–item interaction datasets, including MovieLens-100K, MovieLens-1M, Yelp, and Amazon-Book.",
        "Data_type": "They operate on user-item interaction data, represented as bipartite graphs where users and items act as nodes and interactions serve as edges. No additional modalities (e.g., textual, image, SMILES) are involved; they focus on ID or interaction-based embeddings within these homogeneous graphs.",
        "Data_domain": "The paper focuses on user–item rating logs from three real-world recommendation scenarios: MovieLens (movies), Yelp (local businesses), and Amazon-Book (e-commerce). These datasets represent typical collaborative filtering contexts, capturing user feedback across distinct domains.",
        "ML_phase": "They propose a two-stage pipeline that first trims the hyperparameter search space based on ranking analysis and employs frequency-based sampling on the interaction matrix for faster intermediate evaluations. A surrogate-based framework is then used to jointly optimize architectures and hyperparameters, finalizing the model training on the full dataset."
    },
    "kdd23/2308.01737v1.json": {
        "title": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate  Prediction",
        "abs": "With the widespread application of personalized online services,\nclick-through rate (CTR) prediction has received more and more attention and\nresearch. The most prominent features of CTR prediction are its multi-field\ncategorical data format, and vast and daily-growing data volume. The large\ncapacity of neural models helps digest such massive amounts of data under the\nsupervised learning paradigm, yet they fail to utilize the substantial data to\nits full potential, since the 1-bit click signal is not sufficient to guide the\nmodel to learn capable representations of features and instances. The\nself-supervised learning paradigm provides a more promising pretrain-finetune\nsolution to better exploit the large amount of user click logs, and learn more\ngeneralized and effective representations. However, self-supervised learning\nfor CTR prediction is still an open question, since current works on this line\nare only preliminary and rudimentary. To this end, we propose a Model-agnostic\npretraining (MAP) framework that applies feature corruption and recovery on\nmulti-field categorical data, and more specifically, we derive two practical\nalgorithms: masked feature prediction (MFP) and replaced feature detection\n(RFD). MFP digs into feature interactions within each instance through masking\nand predicting a small portion of input features, and introduces noise\ncontrastive estimation (NCE) to handle large feature spaces. RFD further turns\nMFP into a binary classification mode through replacing and detecting changes\nin input features, making it even simpler and more effective for CTR\npretraining. Our extensive experiments on two real-world large-scale datasets\n(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on\nseveral strong backbones (e.g., DCNv2, DeepFM), and achieve new\nstate-of-the-art performance in terms of both effectiveness and efficiency for\nCTR prediction.",
        "Task": "They tackle a binary classification task for Click-Through Rate (CTR) prediction on multi-field categorical data. They evaluate their methods on large-scale Avazu and Criteo datasets to measure predictive performance.",
        "Data_type": "They focus on multi-field categorical data for CTR tasks, where each instance is represented as a sparse one-hot vector with F fields drawn from a large global feature space of M categories. They do not handle graph-structured information, multi-modal images, or chemical fingerprints.",
        "Data_domain": "The paper leverages large-scale click-through rate datasets (Avazu, Criteo) from the online advertising domain. Each dataset comprises user interaction logs with multi-field categorical features typical of real-world ad platforms.",
        "ML_phase": "They adopt a two-stage pretrain-finetune pipeline, where any neural CTR model is first pretrained on corrupted multi-field categorical data (using masked or replaced feature recovery) and then finetuned with click labels. In practice, the approach only modifies the input corruption and prediction head for pretraining, without altering the core model architecture or its standard supervised training procedure."
    },
    "kdd23/2306.07812v2.json": {
        "title": "Automated 3D Pre-Training for Molecular Property Prediction",
        "abs": "Molecular property prediction is an important problem in drug discovery and\nmaterials science. As geometric structures have been demonstrated necessary for\nmolecular property prediction, 3D information has been combined with various\ngraph learning methods to boost prediction performance. However, obtaining the\ngeometric structure of molecules is not feasible in many real-world\napplications due to the high computational cost. In this work, we propose a\nnovel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D\nmolecular graphs, and then fine-tunes it on molecular graphs without 3D\nstructures. Based on fact that bond length, bond angle, and dihedral angle are\nthree basic geometric descriptors corresponding to a complete molecular 3D\nconformer, we first develop a multi-task generative pre-train framework based\non these three attributes. Next, to automatically fuse these three generative\ntasks, we design a surrogate metric using the \\textit{total energy} to search\nfor weight distribution of the three pretext task since total energy\ncorresponding to the quality of 3D conformer.Extensive experiments on 2D\nmolecular graphs are conducted to demonstrate the accuracy, efficiency and\ngeneralization ability of the proposed 3D PGT compared to various pre-training\nbaselines.",
        "Task": "They address graph-level molecular property prediction tasks, encompassing both regression (e.g., quantum chemistry properties) and classification (e.g., pharmacological assays). Their experiments span diverse datasets, including QM9 for quantum regression, MoleculeNet for binary classification, and additional regression tasks such as drug–target affinity prediction.",
        "Data_type": "They work with molecular graphs, where nodes are atoms, edges are bonds, and 3D conformers provide spatial coordinates. Data include both purely 2D molecular graphs (for downstream tasks) and full 3D structures in the pre-training stage.",
        "Data_domain": "The paper’s datasets are from the molecular and quantum chemistry domain, encompassing large-scale repositories of 3D and 2D molecular structures. They primarily involve quantum-computed properties (e.g., from DFT) and chemically relevant collections (QM9, GEOM-Drugs, PCQM4Mv2) used in drug discovery and materials science.",
        "ML_phase": "They first pre-train a Graph Transformer (GPS) on 3D molecular graphs by predicting bond length, bond angle, and dihedral angle, using total energy as a surrogate metric to adaptively fuse these generative tasks. Then, they fine-tune the resulting encoder on purely 2D molecular inputs, optionally searching the hybrid backbone (message passing plus global attention) architecture to optimize performance across different datasets."
    },
    "kdd23/2306.10490v1.json": {
        "title": "Rapid Image Labeling via Neuro-Symbolic Learning",
        "abs": "The success of Computer Vision (CV) relies heavily on manually annotated\ndata. However, it is prohibitively expensive to annotate images in key domains\nsuch as healthcare, where data labeling requires significant domain expertise\nand cannot be easily delegated to crowd workers. To address this challenge, we\npropose a neuro-symbolic approach called Rapid, which infers image labeling\nrules from a small amount of labeled data provided by domain experts and\nautomatically labels unannotated data using the rules. Specifically, Rapid\ncombines pre-trained CV models and inductive logic learning to infer the\nlogic-based labeling rules. Rapid achieves a labeling accuracy of 83.33% to\n88.33% on four image labeling tasks with only 12 to 39 labeled samples. In\nparticular, Rapid significantly outperforms finetuned CV models in two highly\nspecialized tasks. These results demonstrate the effectiveness of Rapid in\nlearning from small data and its capability to generalize among different\ntasks. Code and our dataset are publicly available at\nhttps://github.com/Neural-Symbolic-Image-Labeling/",
        "Task": "They address multiple image labeling tasks in specialized and common domains. The paper evaluates classification performance using corresponding datasets.",
        "Data_type": "They focus on image data from specialized and common domains for labeling tasks. The datasets consist of standard color images, without additional multi-modal or graph-structured components.",
        "Data_domain": "The data domain comprises image labeling tasks from both highly specialized fields and commonly encountered scenarios. These images are associated with domain-specific rules as well as mainstream labeling practices.",
        "ML_phase": "This paper addresses the data labeling stage in the ML pipeline, focusing on specialized and common domains with defined labeling rules. It outlines how labeling guidelines are established and suggests a framework for consistent label generation across diverse datasets."
    },
    "kdd23/2308.04637v1.json": {
        "title": "Sparse Binary Transformers for Multivariate Time Series Modeling",
        "abs": "Compressed Neural Networks have the potential to enable deep learning across\nnew applications and smaller computational environments. However, understanding\nthe range of learning tasks in which such models can succeed is not well\nstudied. In this work, we apply sparse and binary-weighted Transformers to\nmultivariate time series problems, showing that the lightweight models achieve\naccuracy comparable to that of dense floating-point Transformers of the same\nstructure. Our model achieves favorable results across three time series\nlearning tasks: classification, anomaly detection, and single-step forecasting.\nAdditionally, to reduce the computational complexity of the attention\nmechanism, we apply two modifications, which show little to no decline in model\nperformance: 1) in the classification task, we apply a fixed mask to the query,\nkey, and value activations, and 2) for forecasting and anomaly detection, which\nrely on predicting outputs at a single point in time, we propose an attention\nmask to allow computation only at the current time step. Together, each\ncompression technique and attention modification substantially reduces the\nnumber of non-zero operations necessary in the Transformer. We measure the\ncomputational savings of our approach over a range of metrics including\nparameter count, bit size, and floating point operation (FLOPs) count, showing\nup to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.",
        "Task": "They address three supervised and unsupervised time series tasks: classification, anomaly detection, and single-step forecasting. These tasks are evaluated on various multivariate time series datasets from UCR (e.g., Insect Wingbeats, Spoken Arabic Digits), NASA telemetry (MSL, SMAP), SMD, ECL, Weather, and ETTm1.",
        "Data_type": "They handle multivariate time series data ranging from 7 to 321 features per time step (e.g., sensor measurements, telemetry channels). Tasks span classification, anomaly detection, and forecasting, all using numeric sequences rather than text or image modalities.",
        "Data_domain": "The paper utilizes multivariate time series data from various real-world domains, including NASA telemetry (SMAP, MSL), server machine data (SMD), and electricity/climate datasets (ECL, Weather, ETTm1). It also covers time series classification from the UCR repository (e.g., Insect Wingbeats, Spoken Arabic Digits) to encompass diverse sensor and monitoring applications.",
        "ML_phase": "They preprocess the data by normalizing and, for some tasks, applying a sliding window or padding, then feed it into a Transformer encoder modified for weight pruning and binarization. During training, they use standard backpropagation (with the biprop algorithm) to identify sparse subnetworks, and evaluate these compressed models alongside dense baselines for classification, anomaly detection, and forecasting."
    },
    "kdd23/2306.08487v2.json": {
        "title": "Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph  Propagation",
        "abs": "Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen\nobjects, is a promising learning paradigm to understand new real-world\nknowledge for machines continuously. Recently, the Knowledge Graph (KG) has\nbeen proven as an effective scheme for handling the zero-shot task with\nlarge-scale and non-attribute data. Prior studies always embed relationships of\nseen and unseen objects into visual information from existing knowledge graphs\nto promote the cognitive ability of the unseen data. Actually, real-world\nknowledge is naturally formed by multimodal facts. Compared with ordinary\nstructural knowledge from a graph perspective, multimodal KG can provide\ncognitive systems with fine-grained knowledge. For example, the text\ndescription and visual content can depict more critical details of a fact than\nonly depending on knowledge triplets. Unfortunately, this multimodal\nfine-grained knowledge is largely unexploited due to the bottleneck of feature\nalignment between different modalities. To that end, we propose a multimodal\nintensive ZSL framework that matches regions of images with corresponding\nsemantic embeddings via a designed dense attention module and self-calibration\nloss. It makes the semantic transfer process of our ZSL framework learns more\ndifferentiated knowledge between entities. Our model also gets rid of the\nperformance limitation of only using rough global features. We conduct\nextensive experiments and evaluate our model on large-scale real-world data.\nThe experimental results clearly demonstrate the effectiveness of the proposed\nmodel in standard zero-shot classification tasks.",
        "Task": "They tackle a zero-shot classification task, aiming to recognize unseen categories without training examples for those categories. They evaluate performance using large-scale image classification benchmarks (ImageNet and AWA2).",
        "Data_type": "They handle graph-structured data with nodes corresponding to classes that include textual descriptions as attributes, alongside images providing the visual modality. The text-image multi-modal information is leveraged for zero-shot recognition within a unified knowledge graph framework.",
        "Data_domain": "They use large-scale real-world image data primarily sourced from the ImageNet dataset (including its 21K expanded classes) and the AWA2 animal dataset. Textual descriptions from WordNet are also leveraged, forming a multimodal domain of images and text.",
        "ML_phase": "They preprocess textual data (tokenization, phrase extraction, K-means clustering) to obtain semantic embeddings, then align these with region-level visual features through a dense attention mechanism. Finally, they employ a multi-GCN model to learn class-specific weights for both seen and unseen categories, integrating two fine-tuning stages to complete the zero-shot recognition pipeline."
    },
    "kdd23/2307.12756v1.json": {
        "title": "Unbiased Delayed Feedback Label Correction for Conversion Rate  Prediction",
        "abs": "Conversion rate prediction is critical to many online applications such as\ndigital display advertising. To capture dynamic data distribution, industrial\nsystems often require retraining models on recent data daily or weekly.\nHowever, the delay of conversion behavior usually leads to incorrect labeling,\nwhich is called delayed feedback problem. Existing work may fail to introduce\nthe correct information about false negative samples due to data sparsity and\ndynamic data distribution. To directly introduce the correct feedback label\ninformation, we propose an Unbiased delayed feedback Label Correction framework\n(ULC), which uses an auxiliary model to correct labels for observed negative\nfeedback samples. Firstly, we theoretically prove that the label-corrected loss\nis an unbiased estimate of the oracle loss using true labels. Then, as there\nare no ready training data for label correction, counterfactual labeling is\nused to construct artificial training data. Furthermore, since counterfactual\nlabeling utilizes only partial training data, we design an embedding-based\nalternative training method to enhance performance. Comparative experiments on\nboth public and private datasets and detailed analyses show that our proposed\napproach effectively alleviates the delayed feedback problem and consistently\noutperforms the previous state-of-the-art methods.",
        "Task": "They tackle a binary classification task to predict whether a user will eventually convert (i.e., complete a target action) after clicking an advertisement. They use both public (Criteo) and private datasets to evaluate performance on this conversion rate prediction problem.",
        "Data_type": "They work with large-scale tabular data consisting of user click and conversion logs, using categorical and continuous features. No graph-structured, image-text, or molecular data is involved, as the focus is solely on tabular features and timestamps for offline training of CVR tasks.",
        "Data_domain": "Data for this study is sourced from online advertising user logs, including a public Criteo dataset and a private gaming advertisement dataset. These datasets contain click and conversion records collected over several days from real-world ad platforms.",
        "ML_phase": "They introduce an offline training framework that first constructs partial training data via a counterfactual deadline to train a label correction (LC) model, then alternately retrains both the LC model and the CVR model. This pipeline relies on embedding transfer between models and daily/weekly retraining to address delayed conversions while ensuring unbiased label correction."
    },
    "kdd23/2307.07542v1.json": {
        "title": "Source-Free Domain Adaptation with Temporal Imputation for Time Series  Data",
        "abs": "Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a\nlabeled source domain to an unlabeled target domain without access to the\nsource domain data, preserving source domain privacy. Despite its prevalence in\nvisual applications, SFDA is largely unexplored in time series applications.\nThe existing SFDA methods that are mainly designed for visual applications may\nfail to handle the temporal dynamics in time series, leading to impaired\nadaptation performance. To address this challenge, this paper presents a simple\nyet effective approach for source-free domain adaptation on time series data,\nnamely MAsk and imPUte (MAPU). First, to capture temporal information of the\nsource domain, our method performs random masking on the time series signals\nwhile leveraging a novel temporal imputer to recover the original signal from a\nmasked version in the embedding space. Second, in the adaptation step, the\nimputer network is leveraged to guide the target model to produce target\nfeatures that are temporally consistent with the source features. To this end,\nour MAPU can explicitly account for temporal dependency during the adaptation\nwhile avoiding the imputation in the noisy input space. Our method is the first\nto handle temporal consistency in SFDA for time series data and can be\nseamlessly equipped with other existing SFDA methods. Extensive experiments\nconducted on three real-world time series datasets demonstrate that our MAPU\nachieves significant performance gain over existing methods. Our code is\navailable at \\url{https://github.com/mohamedr002/MAPU_SFDA_TS}.",
        "Task": "They address time-series classification tasks using unlabeled target data in a source-free domain adaptation setting. The tasks are evaluated on three benchmarks covering human activity recognition, machine fault diagnosis, and sleep stage classification.",
        "Data_type": "They handle univariate and multivariate time series signals (up to nine channels) with varying sequence lengths (ranging from 128 to 5120). In particular, the datasets include sensor-based human activity recognition data, EEG recordings for sleep stage classification, and vibration signals for machine fault diagnosis.",
        "Data_domain": "They utilize time series data from three real-world applications: machine fault diagnosis (vibration signals), human activity recognition (accelerometer/gyroscope inputs), and sleep stage classification (EEG readings). Each domain poses distinct distribution shifts due to differing sensor configurations, sequence lengths, and label spaces.",
        "ML_phase": "During pretraining, the method trains a temporal imputer on masked source signals at the feature level alongside the standard classification pipeline, capturing essential temporal dependencies. In adaptation, the pretrained imputer guides the target encoder to produce features consistent with the source domain without requiring source data, thus enabling source-free domain alignment."
    },
    "kdd23/2306.11190v1.json": {
        "title": "Using Motif Transitions for Temporal Graph Generation",
        "abs": "Graph generative models are highly important for sharing surrogate data and\nbenchmarking purposes. Real-world complex systems often exhibit dynamic nature,\nwhere the interactions among nodes change over time in the form of a temporal\nnetwork. Most temporal network generation models extend the static graph\ngeneration models by incorporating temporality in the generation process. More\nrecently, temporal motifs are used to generate temporal networks with better\nsuccess. However, existing models are often restricted to a small set of\npredefined motif patterns due to the high computational cost of counting\ntemporal motifs. In this work, we develop a practical temporal graph generator,\nMotif Transition Model (MTM), to generate synthetic temporal networks with\nrealistic global and local features. Our key idea is modeling the arrival of\nnew events as temporal motif transition processes. We first calculate the\ntransition properties from the input graph and then simulate the motif\ntransition processes based on the transition probabilities and transition\nrates. We demonstrate that our model consistently outperforms the baselines\nwith respect to preserving various global and local temporal graph statistics\nand runtime performance.",
        "Task": "They tackle a temporal graph generation task, producing synthetic networks that preserve structural and temporal properties of real-world graphs. The authors evaluate performance on multiple real-world temporal datasets, measuring how closely the generated graphs match the original data’s characteristics.",
        "Data_type": "They address homogeneous graph-structured data where each edge is a timestamped event between nodes, forming temporal networks. The datasets span various real-world domains (e.g., email, messaging, social interactions), all represented as directed edges with precise time information, without additional textual or multimodal attributes.",
        "Data_domain": "They utilize real-world temporal networks encompassing phone messages, email interactions, social media wall posts, and Q&A forums. Overall, these datasets represent social and communication domains with timestamps capturing user-to-user interactions.",
        "ML_phase": "They propose a generative approach that first extracts motif transition properties (transition probabilities, rates, distributions of “cold” events) from an input temporal network and then simulates a Markov-based process to iteratively add events. This pipeline integrates a Poisson-based mechanism for event timing and a probabilistic edge-creation procedure, producing realistic synthetic temporal graphs without explicit motif counting."
    },
    "kdd23/2306.11216v1.json": {
        "title": "CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical  Systems",
        "abs": "Multi-agent dynamical systems refer to scenarios where multiple units\ninteract with each other and evolve collectively over time. To make informed\ndecisions in multi-agent dynamical systems, such as determining the optimal\nvaccine distribution plan, it is essential for decision-makers to estimate the\ncontinuous-time counterfactual outcomes. However, existing studies of causal\ninference over time rely on the assumption that units are mutually independent,\nwhich is not valid for multi-agent dynamical systems. In this paper, we aim to\nbridge this gap and study how to estimate counterfactual outcomes in\nmulti-agent dynamical systems. Causal inference in a multi-agent dynamical\nsystem has unique challenges: 1) Confounders are time-varying and are present\nin both individual unit covariates and those of other units; 2) Units are\naffected by not only their own but also others' treatments; 3) The treatments\nare naturally dynamic, such as receiving vaccines and boosters in a seasonal\nmanner. We model a multi-agent dynamical system as a graph and propose\nCounterFactual GraphODE (CF-GODE), a causal model that estimates\ncontinuous-time counterfactual outcomes in the presence of inter-dependencies\nbetween units. To facilitate continuous-time estimation, we propose\nTreatment-Induced GraphODE, a novel ordinary differential equation based on\nGNN, which incorporates dynamical treatments as additional inputs to predict\npotential outcomes over time. To remove confounding bias, we propose two domain\nadversarial learning based objectives that learn balanced continuous\nrepresentation trajectories, which are not predictive of treatments and\ninterference. We further provide theoretical justification to prove their\neffectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE\noutperforms baselines on counterfactual estimation. We also provide extensive\nanalyses to understand how our model works.",
        "Task": "They propose a continuous-time regression task focused on estimating counterfactual outcomes in multi-agent dynamical systems. Performance is evaluated via mean squared error on semi-synthetic graph-based datasets.",
        "Data_type": "They handle dynamic graph-structured data where nodes represent units (e.g., individuals) with both time-varying and static attributes, and edges encode the interactions among these units. This setup yields a homogeneous graph with continuous node-level covariates evolving over time.",
        "Data_domain": "The data are derived from real-world social network platforms (Flickr and BlogCatalog). Semi-synthetic observational data are constructed on top of these social graphs for the experiments.",
        "ML_phase": "They simulate observational data using real graph structures combined with a PK-PD-based process for time-varying treatments and potential outcomes. Then, they train a GraphODE model with adversarial objectives to learn continuous latent trajectories and balance representations across treatments and interference."
    },
    "kdd23/2307.01217v2.json": {
        "title": "FedCP: Separating Feature Information for Personalized Federated  Learning via Conditional Policy",
        "abs": "Recently, personalized federated learning (pFL) has attracted increasing\nattention in privacy protection, collaborative learning, and tackling\nstatistical heterogeneity among clients, e.g., hospitals, mobile smartphones,\netc. Most existing pFL methods focus on exploiting the global information and\npersonalized information in the client-level model parameters while neglecting\nthat data is the source of these two kinds of information. To address this, we\npropose the Federated Conditional Policy (FedCP) method, which generates a\nconditional policy for each sample to separate the global information and\npersonalized information in its features and then processes them by a global\nhead and a personalized head, respectively. FedCP is more fine-grained to\nconsider personalization in a sample-specific manner than existing pFL methods.\nExtensive experiments in computer vision and natural language processing\ndomains show that FedCP outperforms eleven state-of-the-art methods by up to\n6.69%. Furthermore, FedCP maintains its superiority when some clients\naccidentally drop out, which frequently happens in mobile settings. Our code is\npublic at https://github.com/TsingZ0/FedCP.",
        "Task": "They focus on supervised classification tasks using image (MNIST, CIFAR-10, CIFAR-100, Tiny-ImageNet) and text (AG News) datasets. Their evaluation measures how well personalized federated learning methods perform on these classification benchmarks.",
        "Data_type": "They evaluate their method on image classification datasets (MNIST, Cifar10, Cifar100, Tiny-ImageNet) and text classification tasks (AG News). Thus, the approach can handle both visual (image) and textual data in a federated learning context.",
        "Data_domain": "They use various image datasets (MNIST, Cifar10, Cifar100, Tiny-ImageNet) for computer vision tasks and the AG News dataset for natural language processing. The data thus comes from both CV and NLP domains for classification.",
        "ML_phase": "They introduce a federated training framework that splits each client’s model into a frozen global feature extractor, a personalized feature extractor, a frozen global head, and a personalized head, combined with a conditional policy to separate global and personalized feature information. At each iteration, clients locally update their personalized components (feature extractor, head, and conditional policy) using MMD alignment to the global extractor, then upload aggregated parameters to the server for global averaging."
    },
    "kdd23/2306.12139v1.json": {
        "title": "Spatial Heterophily Aware Graph Neural Networks",
        "abs": "Graph Neural Networks (GNNs) have been broadly applied in many urban\napplications upon formulating a city as an urban graph whose nodes are urban\nobjects like regions or points of interest. Recently, a few enhanced GNN\narchitectures have been developed to tackle heterophily graphs where connected\nnodes are dissimilar. However, urban graphs usually can be observed to possess\na unique spatial heterophily property; that is, the dissimilarity of neighbors\nat different spatial distances can exhibit great diversity. This property has\nnot been explored, while it often exists. To this end, in this paper, we\npropose a metric, named Spatial Diversity Score, to quantitatively measure the\nspatial heterophily and show how it can influence the performance of GNNs.\nIndeed, our experimental investigation clearly shows that existing heterophilic\nGNNs are still deficient in handling the urban graph with high spatial\ndiversity score. This, in turn, may degrade their effectiveness in urban\napplications. Along this line, we propose a Spatial Heterophily Aware Graph\nNeural Network (SHGNN), to tackle the spatial diversity of heterophily of urban\ngraphs. Based on the key observation that spatially close neighbors on the\nurban graph present a more similar mode of difference to the central node, we\nfirst design a rotation-scaling spatial aggregation module, whose core idea is\nto properly group the spatially close neighbors and separately process each\ngroup with less diversity inside. Then, a heterophily-sensitive spatial\ninteraction module is designed to adaptively capture the commonality and\ndiverse dissimilarity in different spatial groups. Extensive experiments on\nthree real-world urban datasets demonstrate the superiority of our SHGNN over\nseveral its competitors.",
        "Task": "They address node-level classification and regression on urban graphs, focusing on three real-world tasks: commercial activeness prediction (regression), crime prediction (regression), and dangerous road section detection (classification). Corresponding datasets are collected to evaluate and compare performance in each task.",
        "Data_type": "They utilize urban graphs with nodes representing regions or road sections, each having numeric attributes (e.g., POI distributions, structural information) and edges denoting physical or social relations (e.g., human mobility, road connectivity). This homogeneous graph-structured data underpins both classification and regression tasks with primarily numeric node features.",
        "Data_domain": "The datasets are drawn from real-world urban environments encompassing mobility graphs (regions, human flows, crime records) and road networks (sections, accident records). They include data such as POI attributes, traffic trips, and satellite imagery spanning cities like Shenzhen, New York City, and Los Angeles.",
        "ML_phase": "They construct urban graphs (e.g., mobility networks, road networks) by assigning node features from urban data, then partition neighbors into spatial groups according to direction and distance. On top of this, they propose a multi-head GNN architecture and train end-to-end (via regression or classification loss) to model both common and disparate information in each spatial group."
    },
    "kdd23/2308.06480v1.json": {
        "title": "Context-aware Event Forecasting via Graph Disentanglement",
        "abs": "Event forecasting has been a demanding and challenging task throughout the\nentire human history. It plays a pivotal role in crisis alarming and disaster\nprevention in various aspects of the whole society. The task of event\nforecasting aims to model the relational and temporal patterns based on\nhistorical events and makes forecasting to what will happen in the future. Most\nexisting studies on event forecasting formulate it as a problem of link\nprediction on temporal event graphs. However, such pure structured formulation\nsuffers from two main limitations: 1) most events fall into general and\nhigh-level types in the event ontology, and therefore they tend to be\ncoarse-grained and offers little utility which inevitably harms the forecasting\naccuracy; and 2) the events defined by a fixed ontology are unable to retain\nthe out-of-ontology contextual information. To address these limitations, we\npropose a novel task of context-aware event forecasting which incorporates\nauxiliary contextual information. First, the categorical context provides\nsupplementary fine-grained information to the coarse-grained events. Second and\nmore importantly, the context provides additional information towards specific\nsituation and condition, which is crucial or even determinant to what will\nhappen next. However, it is challenging to properly integrate context into the\nevent forecasting framework, considering the complex patterns in the\nmulti-context scenario. Towards this end, we design a novel framework named\nSeparation and Collaboration Graph Disentanglement (short as SeCoGD) for\ncontext-aware event forecasting. Since there is no available dataset for this\nnovel task, we construct three large-scale datasets based on GDELT.\nExperimental results demonstrate that our model outperforms a list of SOTA\nmethods.",
        "Task": "They formulate a link prediction task on temporal knowledge graphs, predicting the missing object in a query (s, r, t+1) based on historical event graphs. They construct three new GDELT-based datasets to evaluate the performance of this context-aware event forecasting.",
        "Data_type": "They handle a temporal, multi-relational event graph where each event is enriched by a categorical textual context derived from news articles. The graph nodes represent entities, edges represent event relations, and each event is formulated as a quintuple (subject, relation, object, timestamp, context).",
        "Data_domain": "These datasets are derived from GDELT, which gathers global political and societal events from news articles. They specifically focus on events in Egypt, Iran, and Israel, encompassing political and international domains.",
        "ML_phase": "They construct context-labeled data from GDELT by filtering low-quality sources, assigning contexts via topic modeling, and collapsing timestamps to daily intervals. Then they introduce a two-stage framework (SeCoGD) that disentangles event graphs per context with RGCN-based relational-temporal encoding and employs hypergraph-based collaboration for cross-context knowledge transfer, culminating in a ConvTransE decoder trained end-to-end with cross-entropy loss."
    },
    "kdd23/2306.09862v3.json": {
        "title": "DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock  Trend Forecasting",
        "abs": "Stock trend forecasting is a fundamental task of quantitative investment\nwhere precise predictions of price trends are indispensable. As an online\nservice, stock data continuously arrive over time. It is practical and\nefficient to incrementally update the forecast model with the latest data which\nmay reveal some new patterns recurring in the future stock market. However,\nincremental learning for stock trend forecasting still remains under-explored\ndue to the challenge of distribution shifts (a.k.a. concept drifts). With the\nstock market dynamically evolving, the distribution of future data can slightly\nor significantly differ from incremental data, hindering the effectiveness of\nincremental updates. To address this challenge, we propose DoubleAdapt, an\nend-to-end framework with two adapters, which can effectively adapt the data\nand the model to mitigate the effects of distribution shifts. Our key insight\nis to automatically learn how to adapt stock data into a locally stationary\ndistribution in favor of profitable updates. Complemented by data adaptation,\nwe can confidently adapt the model parameters under mitigated distribution\nshifts. We cast each incremental learning task as a meta-learning task and\nautomatically optimize the adapters for desirable data adaptation and parameter\ninitialization. Experiments on real-world stock datasets demonstrate that\nDoubleAdapt achieves state-of-the-art predictive performance and shows\nconsiderable efficiency.",
        "Task": "They aim to perform a regression task for predicting future stock price trends (i.e., next-day price change rates). They evaluate this on real-world stock datasets (CSI 300 and CSI 500) in the China A-share market.",
        "Data_type": "They handle numeric time-series data of daily stock indicators, where each sample is represented as a 360-dimensional feature vector (60 days × 6 indicators per day). The label is a single scalar reflecting the next-day price trend.",
        "Data_domain": "The data in this work comes from the financial domain, specifically the Chinese stock market (e.g., CSI 300 and CSI 500). It comprises historical stock features and corresponding price trends spanning multiple years in the China A-share market.",
        "ML_phase": "They implement a two-phase training procedure (offline and online) where each incremental task first adapts data via multi-head transformations, then fine-tunes a forecast model initialized by a meta-learner. In each task, the model is trained on adapted incremental data in a lower-level step, and the meta-learners (data adapter and model adapter) are updated in an upper-level optimization using the test error."
    },
    "kdd23/2306.15489v3.json": {
        "title": "Precursor-of-Anomaly Detection for Irregular Time Series",
        "abs": "Anomaly detection is an important field that aims to identify unexpected\npatterns or data points, and it is closely related to many real-world problems,\nparticularly to applications in finance, manufacturing, cyber security, and so\non. While anomaly detection has been studied extensively in various fields,\ndetecting future anomalies before they occur remains an unexplored territory.\nIn this paper, we present a novel type of anomaly detection, called\nPrecursor-of-Anomaly (PoA) detection. Unlike conventional anomaly detection,\nwhich focuses on determining whether a given time series observation is an\nanomaly or not, PoA detection aims to detect future anomalies before they\nhappen. To solve both problems at the same time, we present a neural controlled\ndifferential equation-based neural network and its multi-task learning\nalgorithm. We conduct experiments using 17 baselines and 3 datasets, including\nregular and irregular time series, and demonstrate that our presented method\noutperforms the baselines in almost all cases. Our ablation studies also\nindicate that the multitasking training method significantly enhances the\noverall performance for both anomaly and PoA detection.",
        "Task": "They address two classification tasks on multivariate time series: one for detecting if the current window is anomalous (anomaly detection) and another for predicting if the subsequent window will be anomalous (precursor-of-anomaly detection). They evaluate these tasks on real-world benchmarks (MSL, SWaT, WADI), each providing labeled time series data for performance assessment.",
        "Data_type": "They focus on multivariate time-series data (both regular and irregular) where each observation can have varying time intervals and dimensions. Their framework handles missing or unevenly sampled time steps in real-world telemetry or sensor data.",
        "Data_domain": "These datasets originate from NASA’s spacecraft telemetry and from operational industrial control systems in water treatment and distribution plants. They comprise multivariate time-series signals reflecting physical and cyber processes in real-world settings.",
        "ML_phase": "They augment normal training data by resampling to synthesize artificial anomalies, then segment time-series windows for both anomaly and precursor-of-anomaly detection. The model architecture comprises dual co-evolving NCDEs with partial parameter sharing, trained via multi-task learning, knowledge distillation, and an adjoint solver for memory-efficient gradient computation."
    },
    "kdd23/2306.16526v1.json": {
        "title": "Shilling Black-box Review-based Recommender Systems through Fake Review  Generation",
        "abs": "Review-Based Recommender Systems (RBRS) have attracted increasing research\ninterest due to their ability to alleviate well-known cold-start problems. RBRS\nutilizes reviews to construct the user and items representations. However, in\nthis paper, we argue that such a reliance on reviews may instead expose systems\nto the risk of being shilled. To explore this possibility, in this paper, we\npropose the first generation-based model for shilling attacks against RBRSs.\nSpecifically, we learn a fake review generator through reinforcement learning,\nwhich maliciously promotes items by forcing prediction shifts after adding\ngenerated reviews to the system. By introducing the auxiliary rewards to\nincrease text fluency and diversity with the aid of pre-trained language models\nand aspect predictors, the generated reviews can be effective for shilling with\nhigh fidelity. Experimental results demonstrate that the proposed framework can\nsuccessfully attack three different kinds of RBRSs on the Amazon corpus with\nthree domains and Yelp corpus. Furthermore, human studies also show that the\ngenerated reviews are fluent and informative. Finally, equipped with Attack\nReview Generators (ARGs), RBRSs with adversarial training are much more robust\nto malicious reviews.",
        "Task": "They address a rating prediction (regression) problem in review-based recommender systems by generating fake textual reviews that manipulate the system’s predicted scores. They evaluate performance on multiple Amazon product category datasets (Musical Instruments, Video Games, Office Products) and Yelp.",
        "Data_type": "They use textual reviews from e-commerce platforms (e.g., Amazon and Yelp) with associated user and item IDs, each review containing a star rating (1–5) and descriptive text. The data focuses on written feedback regarding products, capturing fine-grained aspects and user sentiments in a single-modality textual form.",
        "Data_domain": "The paper’s datasets come from e-commerce and local business review domains, specifically Amazon’s 5-star review data (covering categories such as Musical Instruments, Video Games, and Office Products) and Yelp. These corpora provide user-generated textual reviews for items or services.",
        "ML_phase": "They first pre-train a transformer-based encoder-decoder on reviews with a leave-one-out objective, then fine-tune it via reinforcement learning (rewarded by rating shift, fluency, and relevance) to generate attack reviews. Finally, they retrain the recommendation model with these generated reviews (adversarial training) to enhance robustness."
    },
    "kdd23/2306.14375v1.json": {
        "title": "Interpretable Sparsification of Brain Graphs: Better Practices and  Effective Designs for Graph Neural Networks",
        "abs": "Brain graphs, which model the structural and functional relationships between\nbrain regions, are crucial in neuroscientific and clinical applications\ninvolving graph classification. However, dense brain graphs pose computational\nchallenges including high runtime and memory usage and limited\ninterpretability. In this paper, we investigate effective designs in Graph\nNeural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges.\nWhile prior works remove noisy edges based on explainability or task-irrelevant\nproperties, their effectiveness in enhancing performance with sparsified graphs\nis not guaranteed. Moreover, existing approaches often overlook collective edge\nremoval across multiple graphs.\n  To address these issues, we introduce an iterative framework to analyze\ndifferent sparsification models. Our findings are as follows: (i) methods\nprioritizing interpretability may not be suitable for graph sparsification as\nthey can degrade GNNs' performance in graph classification tasks; (ii)\nsimultaneously learning edge selection with GNN training is more beneficial\nthan post-training; (iii) a shared edge selection across graphs outperforms\nseparate selection for each graph; and (iv) task-relevant gradient information\naids in edge selection. Based on these insights, we propose a new model,\nInterpretable Graph Sparsification (IGS), which enhances graph classification\nperformance by up to 5.1% with 55.0% fewer edges. The retained edges identified\nby IGS provide neuroscientific interpretations and are supported by\nwell-established literature.",
        "Task": "They perform a graph-level classification task to predict cognitive scores from resting-state fMRI-based brain networks. The datasets used are derived from the WU-Minn Human Connectome Project, and performance is evaluated on multiple cognitive assessment labels.",
        "Data_type": "They use fully connected, weighted brain graphs derived from resting-state fMRI, each with 100 nodes (brain regions) and a 100×100 adjacency matrix of correlation-based edge weights. Node features (dimension d) are also included, making this a homogeneous graph-structured dataset.",
        "Data_domain": "They use resting-state functional MRI data from the WU-Minn Human Connectome Project, where each subject’s brain activity is represented as a fully connected functional network. This neuroscience dataset captures pairwise correlations of time-series signals across 100 brain regions, tied to cognitive performance labels.",
        "ML_phase": "The paper constructs fully connected brain graphs from resting-state fMRI by computing pairwise correlations across brain regions and then uses an iterative GNN-based training approach that simultaneously learns a shared edge-importance mask. The final model is selected via validation loss with early stopping, using standard train/validation/test splits to refine both the GNN parameters and the learned mask."
    },
    "kdd23/2310.13892v1.json": {
        "title": "Specify Robust Causal Representation from Mixed Observations",
        "abs": "Learning representations purely from observations concerns the problem of\nlearning a low-dimensional, compact representation which is beneficial to\nprediction models. Under the hypothesis that the intrinsic latent factors\nfollow some casual generative models, we argue that by learning a causal\nrepresentation, which is the minimal sufficient causes of the whole system, we\ncan improve the robustness and generalization performance of machine learning\nmodels. In this paper, we develop a learning method to learn such\nrepresentation from observational data by regularizing the learning procedure\nwith mutual information measures, according to the hypothetical factored causal\ngraph. We theoretically and empirically show that the models trained with the\nlearned causal representations are more robust under adversarial attacks and\ndistribution shifts compared with baselines. The supplementary materials are\navailable at https://github.com/ymy $4323460 / \\mathrm{CaRI} /$.",
        "Task": "They address a supervised classification task where the goal is to predict labels (e.g., user feedback or attributes) from high-dimensional observations. Experimental evaluations use synthetic data and several real-world benchmarks (e.g., Yahoo! R3, Coat Shopping, and CelebA) to measure classification performance (AUC, accuracy) under both i.i.d. and OOD conditions.",
        "Data_type": "They employ tabular user–item rating data (e.g., with 14-dimensional user features and 33-dimensional item features in the Coat dataset) and image attributes from CelebA (nine annotated attributes for each image). They also include a synthetic dataset with 15-dimensional latent factors (5 for each causal component) to evaluate representation learning under different distribution shifts.",
        "Data_domain": "Data is drawn from both synthetic simulations and real-world recommendation settings (e.g., Yahoo! R3, Coat Shopping, PCIC) involving user-item ratings, as well as an annotated face attribute dataset (CelebA-anno). In essence, it spans e-commerce recommendation data and image-based attribute data.",
        "ML_phase": "They generate observational data from either a synthetic causal simulator or real recommendation datasets, then feed it into a neural representation encoder. The model architecture is trained via an information-theoretic min-max objective (augmented with intervention for robustness) to learn minimal sufficient causal information."
    },
    "kdd23/2307.09296v1.json": {
        "title": "Rumor Detection with Diverse Counterfactual Evidence",
        "abs": "The growth in social media has exacerbated the threat of fake news to\nindividuals and communities. This draws increasing attention to developing\nefficient and timely rumor detection methods. The prevailing approaches resort\nto graph neural networks (GNNs) to exploit the post-propagation patterns of the\nrumor-spreading process. However, these methods lack inherent interpretation of\nrumor detection due to the black-box nature of GNNs. Moreover, these methods\nsuffer from less robust results as they employ all the propagation patterns for\nrumor detection. In this paper, we address the above issues with the proposed\nDiverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our\nintuition is to exploit the diverse counterfactual evidence of an event graph\nto serve as multi-view interpretations, which are further aggregated for robust\nrumor detection results. Specifically, our method first designs a subgraph\ngeneration strategy to efficiently generate different subgraphs of the event\ngraph. We constrain the removal of these subgraphs to cause the change in rumor\ndetection results. Thus, these subgraphs naturally serve as counterfactual\nevidence for rumor detection. To achieve multi-view interpretation, we design a\ndiversity loss inspired by Determinantal Point Processes (DPP) to encourage\ndiversity among the counterfactual evidence. A GNN-based rumor detection model\nfurther aggregates the diverse counterfactual evidence discovered by the\nproposed DCE-RD to achieve interpretable and robust rumor detection results.\nExtensive experiments on two real-world datasets show the superior performance\nof our method. Our code is available at https://github.com/Vicinity111/DCE-RD.",
        "Task": "They tackle the binary classification of social media posts into “true rumor” vs. “false rumor.” Performance is evaluated on two real-world datasets, Weibo and MC-Fake.",
        "Data_type": "They handle graph-structured data where each node is a social media post with textual attributes, and edges capture reply/retweet relationships (i.e., a homogeneous text-based propagation network). The approach focuses on node-level text features and topological connections for rumor detection.",
        "Data_domain": "They utilize social media datasets from Weibo and Twitter, focusing on user posts and their propagation structures in these networks.",
        "ML_phase": "They first initialize node embeddings from textual features using LSTM, then apply a GAT-based aggregator and generate label-relevant subgraphs via top-K node sampling (with Gumbel-max) before removing these nodes to obtain counterfactual subgraphs. Finally, they train the model with a combined objective (cross-entropy, counterfactual, and diversity losses) to achieve robust rumor detection results."
    },
    "kdd23/2307.01504v2.json": {
        "title": "All in One: Multi-task Prompting for Graph Neural Networks",
        "abs": "Recently, ''pre-training and fine-tuning'' has been adopted as a standard\nworkflow for many graph tasks since it can take general graph knowledge to\nrelieve the lack of graph annotations from each application. However, graph\ntasks with node level, edge level, and graph level are far diversified, making\nthe pre-training pretext often incompatible with these multiple tasks. This gap\nmay even cause a ''negative transfer'' to the specific application, leading to\npoor results. Inspired by the prompt learning in natural language processing\n(NLP), which has presented significant effectiveness in leveraging prior\nknowledge for various NLP tasks, we study the prompting topic for graphs with\nthe motivation of filling the gap between pre-trained models and various graph\ntasks. In this paper, we propose a novel multi-task prompting method for graph\nmodels. Specifically, we first unify the format of graph prompts and language\nprompts with the prompt token, token structure, and inserting pattern. In this\nway, the prompting idea from NLP can be seamlessly introduced to the graph\narea. Then, to further narrow the gap between various graph tasks and\nstate-of-the-art pre-training strategies, we further study the task space of\nvarious graph applications and reformulate downstream problems to the\ngraph-level task. Afterward, we introduce meta-learning to efficiently learn a\nbetter initialization for the multi-task prompt of graphs so that our prompting\nframework can be more reliable and general for different tasks. We conduct\nextensive experiments, results from which demonstrate the superiority of our\nmethod.",
        "Task": "They address multi-task learning on graphs, covering node-level, edge-level, and graph-level classification with induced subgraphs under few-shot conditions. They additionally evaluate link prediction and graph/edge-level regression tasks.",
        "Data_type": "They handle homogeneous graph-structured data with textual or numeric node attributes for node-level, edge-level, and graph-level tasks. The node attributes are typically compressed to 100-dimensional vectors (via SVD) before downstream classification or regression.",
        "Data_domain": "These datasets span multiple graph domains, including academic citation networks (Cora, CiteSeer, PubMed), social networks (Reddit, Facebook, PersonalityCafe), e-commerce (Amazon), and biological or molecular graphs (ENZYMES, ProteinsFull, QM9), as well as user-movie rating data (MovieLens). They are used to evaluate node-level, edge-level, or graph-level tasks in various few-shot learning scenarios.",
        "ML_phase": "They reformulate node- and edge-level tasks into unified graph-level tasks by constructing induced subgraphs, then insert a learnable “prompt graph” (tokens, structures, and inserting patterns) into these subgraphs as input to a frozen pre-trained backbone. A meta-learning framework optimizes the prompt parameters across multiple tasks, thereby adapting the final model without re-training the main GNN."
    },
    "kdd23/2306.14393v1.json": {
        "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient  Transformer Inference",
        "abs": "Deploying pre-trained transformer models like BERT on downstream tasks in\nresource-constrained scenarios is challenging due to their high inference cost,\nwhich grows rapidly with input sequence length. In this work, we propose a\nconstraint-aware and ranking-distilled token pruning method ToP, which\nselectively removes unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while preserving accuracy.\nToP overcomes the limitation of inaccurate token importance ranking in the\nconventional self-attention mechanism through a ranking-distilled token\ndistillation technique, which distills effective token rankings from the final\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\na coarse-to-fine pruning approach that automatically selects the optimal subset\nof transformer layers and optimizes token pruning decisions within these layers\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\nand model compression methods with improved accuracy and speedups. ToP reduces\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\nand provides a real latency speedup of up to 7.4x on an Intel CPU.",
        "Task": "They address text classification (e.g., CoLA, RTE, MNLI, SST-2, QNLI, 20News), regression (STS-B), and extractive question answering (SQuAD v2.0). These tasks collectively form a natural language understanding benchmark, covering classification, regression, and QA settings.",
        "Data_type": "They handle textual data with sequence lengths ranging from short (e.g., 64 tokens) to long (e.g., 512 tokens), covering classification, regression, and extractive QA tasks. Experiments involve benchmarks such as GLUE, SQuAD-v2.0, and 20News, focusing solely on text input.",
        "Data_domain": "The paper’s experiments use textual data from academic benchmarks in natural language processing (NLP), including GLUE, SQuAD, and 20News. These datasets encompass various classification, regression, and question answering tasks on diverse text inputs.",
        "ML_phase": "They introduce a token pruning method that is applied during model fine-tuning, where masks (gate and ranking) are jointly learned alongside model parameters using L0 regularization. The training pipeline also integrates ranking-aware distillation to improve the early-layer attention value–based importance scoring, ensuring constraint-aware compression without sacrificing accuracy."
    },
    "kdd23/2308.05353v1.json": {
        "title": "Preemptive Detection of Fake Accounts on Social Networks via Multi-Class  Preferential Attachment Classifiers",
        "abs": "In this paper, we describe a new algorithm called Preferential Attachment\nk-class Classifier (PreAttacK) for detecting fake accounts in a social network.\nRecently, several algorithms have obtained high accuracy on this problem.\nHowever, they have done so by relying on information about fake accounts'\nfriendships or the content they share with others--the very things we seek to\nprevent. PreAttacK represents a significant departure from these approaches. We\nprovide some of the first detailed distributional analyses of how new fake (and\nreal) accounts first attempt to request friends after joining a major network\n(Facebook). We show that even before a new account has made friends or shared\ncontent, these initial friend request behaviors evoke a natural multi-class\nextension of the canonical Preferential Attachment model of social network\ngrowth. We use this model to derive a new algorithm, PreAttacK. We prove that\nin relevant problem instances, PreAttacK near-optimally approximates the\nposterior probability that a new account is fake under this multi-class\nPreferential Attachment model of new accounts' (not-yet-answered) friend\nrequests. These are the first provable guarantees for fake account detection\nthat apply to new users, and that do not require strong homophily assumptions.\nThis principled approach also makes PreAttacK the only algorithm with provable\nguarantees that obtains state-of-the-art performance on new users on the global\nFacebook network, where it converges to AUC=0.9 after new users send + receive\na total of just 20 not-yet-answered friend requests. For comparison,\nstate-of-the-art benchmarks do not obtain this AUC even after observing\nadditional data on new users' first 100 friend requests. Thus, unlike\nmainstream algorithms, PreAttacK converges before the median new fake account\nhas made a single friendship (accepted friend request) with a human.",
        "Task": "They address a node-level classification task that predicts whether a new social network account is fake or real. Performance is evaluated on large-scale datasets from Facebook using metrics such as AUC.",
        "Data_type": "They operate on a large-scale directed graph of social network accounts, where nodes (users) carry binary labels (fake vs. real) and edges represent friend requests. All analysis and classification tasks are performed exclusively on this graph-structured data without involving additional modalities (e.g., text or images).",
        "Data_domain": "The data is sourced from the global Facebook social network, focusing on new user account behaviors and friend request activities. It encompasses large-scale user interactions to distinguish real from fake profiles.",
        "ML_phase": "This work situates its method in the post-registration phase, where it processes each new user's small batch of initial friend requests (sent/received) before any friendships are accepted. It leverages a large-scale preexisting log of labeled friendship requests (E₀), computes approximate posterior probabilities in a single pass, and updates fake/real labels without extensive iterative retraining."
    },
    "kdd23/2306.08277v1.json": {
        "title": "FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks",
        "abs": "Modelling spatio-temporal processes on road networks is a task of growing\nimportance. While significant progress has been made on developing\nspatio-temporal graph neural networks (Gnns), existing works are built upon\nthree assumptions that are not practical on real-world road networks. First,\nthey assume sensing on every node of a road network. In reality, due to\nbudget-constraints or sensor failures, all locations (nodes) may not be\nequipped with sensors. Second, they assume that sensing history is available at\nall installed sensors. This is unrealistic as well due to sensor failures, loss\nof packets during communication, etc. Finally, there is an assumption of static\nroad networks. Connectivity within networks change due to road closures,\nconstructions of new roads, etc. In this work, we develop FRIGATE to address\nall these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that\nintegrates positional, topological, and temporal information into rich\ninductive node representations. The joint fusion of this diverse information is\nmade feasible through a novel combination of gated Lipschitz embeddings with\nLstms. We prove that the proposed Gnn architecture is provably more expressive\nthan message-passing Gnns used in state-of-the-art algorithms. The higher\nexpressivity of FRIGATE naturally translates to superior empirical performance\nconducted on real-world network-constrained traffic data. In addition, FRIGATE\nis robust to frugal sensor deployment, changes in road network connectivity,\nand temporal irregularity in sensing.",
        "Task": "They tackle a node-level time-series regression task on road networks, specifically forecasting future traffic volumes at intersections. The approach is evaluated on real-world datasets from multiple cities.",
        "Data_type": "They operate on graph-structured road networks with numeric node-level traffic readings (homogeneous intersections connected by directed edges). The node attributes are time-series sensor values, handling partial or missing measurements without relying on textual or multi-modal inputs.",
        "Data_domain": "The data is drawn from real-world urban road networks, capturing traffic volume from aggregated GPS trajectories of taxis. It specifically represents large-scale transportation and mobility data in cities such as Beijing, Chengdu, and Harbin.",
        "ML_phase": "They formulate a road-network time-series forecasting task, where data processing involves map-matching GPS trajectories to nodes, aggregating sensor counts in five-minute buckets, and handling partial/noisy sensors. The model architecture is an inductive, gated graph neural network with an LSTM-based encoder-decoder trained end-to-end on MAE loss, leveraging Lipschitz embeddings for node positioning and directional message-passing layers for robust spatio-temporal learning."
    },
    "kdd23/2306.08892v1.json": {
        "title": "MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text  Classification",
        "abs": "Prompting methods have shown impressive performance in a variety of text\nmining tasks and applications, especially few-shot ones. Despite the promising\nprospects, the performance of prompting model largely depends on the design of\nprompt template and verbalizer. In this work, we propose MetricPrompt, which\neases verbalizer design difficulty by reformulating few-shot text\nclassification task into text pair relevance estimation task. MetricPrompt\nadopts prompting model as the relevance metric, further bridging the gap\nbetween Pre-trained Language Model's (PLM) pre-training objective and text\nclassification task, making possible PLM's smooth adaption. Taking a training\nsample and a query one simultaneously, MetricPrompt captures cross-sample\nrelevance information for accurate relevance estimation. We conduct experiments\non three widely used text classification datasets across four few-shot\nsettings. Results show that MetricPrompt outperforms manual verbalizer and\nother automatic verbalizer design methods across all few-shot settings,\nachieving new state-of-the-art (SOTA) performance.",
        "Task": "They tackle a few-shot text classification task, aiming to improve performance with minimal labeled data. Evaluation is conducted on standard text classification datasets such as AG’s News, Yahoo Answers, and DBPedia.",
        "Data_type": "They focus on single-modality textual data for few-shot text classification tasks. The experiments involve short texts from news and knowledge-based datasets, without integrating any additional data types (e.g., images or graphs).",
        "Data_domain": "They focus on widely used textual corpora, specifically news articles, knowledge bases (DBPedia), and question-answer forums (Yahoo Answers). Hence, the data domain is general text data collected from different sources, including news and community Q&A.",
        "ML_phase": "They reorganize the training data into text pairs indicating same‐class or not, then use a task‐general “meta verbalizer” so the PLM’s output probabilities map to binary relevance labels. Next, they train the prompting model via a cross‐entropy objective over these pairwise samples, and at inference they aggregate per‐pair relevance scores (using mean, max, or kNN pooling) to classify new queries."
    },
    "kdd23/2308.10808v1.json": {
        "title": "Graph Neural Bandits",
        "abs": "Contextual bandits algorithms aim to choose the optimal arm with the highest\nreward out of a set of candidates based on the contextual information. Various\nbandit algorithms have been applied to real-world applications due to their\nability of tackling the exploitation-exploration dilemma. Motivated by online\nrecommendation scenarios, in this paper, we propose a framework named Graph\nNeural Bandits (GNB) to leverage the collaborative nature among users empowered\nby graph neural networks (GNNs). Instead of estimating rigid user clusters as\nin existing works, we model the \"fine-grained\" collaborative effects through\nestimated user graphs in terms of exploitation and exploration respectively.\nThen, to refine the recommendation strategy, we utilize separate GNN-based\nmodels on estimated user graphs for exploitation and adaptive exploration.\nTheoretical analysis and experimental results on multiple real data sets in\ncomparison with state-of-the-art baselines are provided to demonstrate the\neffectiveness of our proposed framework.",
        "Task": "They propose a contextual bandit framework that models user correlations via graph neural networks to minimize cumulative regret in online recommendation tasks. Their experiments span recommendation scenarios on MovieLens/Yelp and classification tasks on MNIST, Shuttle, Letter, and Pendigits.",
        "Data_type": "They handle graph-structured data where each user is treated as a node with edge weights encoding either “exploitation” or “exploration” correlations. They also work with classification data (e.g., MNIST, Letter) by transforming samples into node-like inputs, thus supporting textual or numeric node attributes in homogeneous graphs.",
        "Data_domain": "They use user review data for recommendation (MovieLens movie ratings, Yelp restaurant ratings) and employ four classification datasets (MNIST, Shuttle, Letter, Pendigits) from academic benchmarks. Hence, the data domain spans real-world user feedback scenarios and standard classification tasks.",
        "ML_phase": "They build user exploitation and exploration graphs at each round by converting contextual inputs into graph-structured data through user-specific networks, then feed these graphs into two GNN models (for reward and uncertainty estimation). Model parameters are updated via gradient-based training on accumulated user-item interactions, and arm selection follows from combining the GNN-predicted reward and exploration signals."
    },
    "kdd23/2307.09048v1.json": {
        "title": "FedDefender: Client-Side Attack-Tolerant Federated Learning",
        "abs": "Federated learning enables learning from decentralized data sources without\ncompromising privacy, which makes it a crucial technique. However, it is\nvulnerable to model poisoning attacks, where malicious clients interfere with\nthe training process. Previous defense mechanisms have focused on the\nserver-side by using careful model aggregation, but this may not be effective\nwhen the data is not identically distributed or when attackers can access the\ninformation of benign clients. In this paper, we propose a new defense\nmechanism that focuses on the client-side, called FedDefender, to help benign\nclients train robust local models and avoid the adverse impact of malicious\nmodel updates from attackers, even when a server-side defense cannot identify\nor remove adversaries. Our method consists of two main components: (1)\nattack-tolerant local meta update and (2) attack-tolerant global knowledge\ndistillation. These components are used to find noise-resilient model\nparameters while accurately extracting knowledge from a potentially corrupted\nglobal model. Our client-side defense strategy has a flexible structure and can\nwork in conjunction with any existing server-side strategies. Evaluations of\nreal-world scenarios across multiple datasets show that the proposed method\nenhances the robustness of federated learning against model poisoning attacks.",
        "Task": "They conduct classification tasks on multiple image datasets (CIFAR-10, CIFAR-100, TinyImageNet, and FEMNIST) in a federated learning setup. Performance is evaluated via classification accuracy under various poisoning attack scenarios.",
        "Data_type": "They use image-based data, specifically standard computer vision benchmarks such as CIFAR-10, CIFAR-100, TinyImageNet, and FEMNIST. No graph-structured, text-based, or other modalities (e.g., SMILES) are handled in the discussed work.",
        "Data_domain": "They use standard image classification datasets (CIFAR-10, CIFAR-100, TinyImageNet) and a handwriting dataset (FEMNIST) in the computer vision domain. These datasets comprise diverse images ranging from natural scenes to handwritten characters.",
        "ML_phase": "They introduce a new client-side training procedure that first perturbs local parameters with synthetic noisy labels (attack-tolerant local meta update) and then distills intermediate-layer global knowledge through an auxiliary classifier (attack-tolerant global knowledge distillation). The method slots into the local training phase of federated learning, complementing existing server-side aggregation defenses to mitigate poisoning attacks without altering the overall aggregation pipeline."
    },
    "kdd23/2306.07106v1.json": {
        "title": "Adversarial Constrained Bidding via Minimax Regret Optimization with  Causality-Aware Reinforcement Learning",
        "abs": "The proliferation of the Internet has led to the emergence of online\nadvertising, driven by the mechanics of online auctions. In these repeated\nauctions, software agents participate on behalf of aggregated advertisers to\noptimize for their long-term utility. To fulfill the diverse demands, bidding\nstrategies are employed to optimize advertising objectives subject to different\nspending constraints. Existing approaches on constrained bidding typically rely\non i.i.d. train and test conditions, which contradicts the adversarial nature\nof online ad markets where different parties possess potentially conflicting\nobjectives. In this regard, we explore the problem of constrained bidding in\nadversarial bidding environments, which assumes no knowledge about the\nadversarial factors. Instead of relying on the i.i.d. assumption, our insight\nis to align the train distribution of environments with the potential test\ndistribution meanwhile minimizing policy regret. Based on this insight, we\npropose a practical Minimax Regret Optimization (MiRO) approach that\ninterleaves between a teacher finding adversarial environments for tutoring and\na learner meta-learning its policy over the given distribution of environments.\nIn addition, we pioneer to incorporate expert demonstrations for learning\nbidding strategies. Through a causality-aware policy design, we improve upon\nMiRO by distilling knowledge from the experts. Extensive experiments on both\nindustrial data and synthetic data show that our method, MiRO with\nCausality-aware reinforcement Learning (MiROCL), outperforms prior methods by\nover 30%.",
        "Task": "They formulate a constrained reinforcement learning task in real-time bidding, where the goal is to learn a policy that maximizes long-term utility under both budget and ROI constraints. The method is evaluated on datasets representing adversarial auction environments and seeks to minimize policy regret.",
        "Data_type": "They use large-scale tabular logs of repeated auction interactions, containing numeric fields such as market prices, bids, and utility feedback. No graph structures, images, or SMILES data are involved; the focus is on sequential numeric/contextual data in real-time bidding environments.",
        "Data_domain": "The data originates from large-scale online advertising logs gathered from a major e-commerce platform, encompassing billions of auction interactions. A synthetic dataset further simulates dynamic, adversarial auction scenarios to complement real-world conditions.",
        "ML_phase": "They construct a two-stage pipeline, first learning a latent representation of adversarial factors by reconstructing the environment’s causal structure, and then adopting a minimax game for policy training. The training procedure iterates between a teacher distributing worst-case environment embeddings and a learner refining policy parameters via gradient-based optimization."
    },
    "kdd23/2306.07994v1.json": {
        "title": "MSSRNet: Manipulating Sequential Style Representation for Unsupervised  Text Style Transfer",
        "abs": "Unsupervised text style transfer task aims to rewrite a text into target\nstyle while preserving its main content. Traditional methods rely on the use of\na fixed-sized vector to regulate text style, which is difficult to accurately\nconvey the style strength for each individual token. In fact, each token of a\ntext contains different style intensity and makes different contribution to the\noverall style. Our proposed method addresses this issue by assigning individual\nstyle vector to each token in a text, allowing for fine-grained control and\nmanipulation of the style strength. Additionally, an adversarial training\nframework integrated with teacher-student learning is introduced to enhance\ntraining stability and reduce the complexity of high-dimensional optimization.\nThe results of our experiments demonstrate the efficacy of our method in terms\nof clearly improved style transfer accuracy and content preservation in both\ntwo-style transfer and multi-style transfer settings.",
        "Task": "They focus on an unsupervised text style transfer task, aiming to generate sentences in a desired style while preserving the original content. Experiments are conducted on Yelp, IMDb, and Sentiment-Formality datasets to quantitatively assess style accuracy and content preservation.",
        "Data_type": "They focus exclusively on textual datasets (e.g., Yelp, IMDb, and multi-style corpora) for unsupervised text style transfer. No other data modalities such as images, graphs, or chemical structures are addressed.",
        "Data_domain": "They use user-generated review datasets from Yelp (restaurants/business) and IMDb (movie reviews). Additionally, they adopt a multi-style corpus encompassing sentiment and formality dimensions for broader evaluation.",
        "ML_phase": "They first train a style classifier to serve as a teacher, then build MSSRNet with a Style Generator, Content Encoder, and Decoder for sequential style control. The model is optimized in an adversarial framework with dual discriminators, a self-reconstruction objective, and teacher-student learning to stabilize training and enable fine-grained style manipulation."
    },
    "kdd23/2307.09943v2.json": {
        "title": "Impatient Bandits: Optimizing Recommendations for the Long-Term Without  Delay",
        "abs": "Recommender systems are a ubiquitous feature of online platforms.\nIncreasingly, they are explicitly tasked with increasing users' long-term\nsatisfaction. In this context, we study a content exploration task, which we\nformalize as a multi-armed bandit problem with delayed rewards. We observe that\nthere is an apparent trade-off in choosing the learning signal: Waiting for the\nfull reward to become available might take several weeks, hurting the rate at\nwhich learning happens, whereas measuring short-term proxy rewards reflects the\nactual long-term goal only imperfectly. We address this challenge in two steps.\nFirst, we develop a predictive model of delayed rewards that incorporates all\ninformation obtained to date. Full observations as well as partial (short or\nmedium-term) outcomes are combined through a Bayesian filter to obtain a\nprobabilistic belief. Second, we devise a bandit algorithm that takes advantage\nof this new predictive model. The algorithm quickly learns to identify content\naligned with long-term success by carefully balancing exploration and\nexploitation. We apply our approach to a podcast recommendation problem, where\nwe seek to identify shows that users engage with repeatedly over two months. We\nempirically validate that our approach results in substantially better\nperformance compared to approaches that either optimize for short-term proxies,\nor wait for the long-term outcome to be fully realized.",
        "Task": "They propose a multi-armed bandit task that aims to quickly identify and exploit high-value items (podcast shows) under delayed reward feedback. The performance is evaluated using real-world user engagement datasets from Spotify.",
        "Data_type": "They use time-series data consisting of daily binary engagement indicators over a 59-day window (per-user, per-podcast show). The approach explicitly models these progressively revealed feedback traces as the primary data type.",
        "Data_domain": "The data comes from an online audio streaming platform (Spotify), capturing user engagement traces on newly published podcast content. This domain involves large-scale behavioral logs of podcast consumption over time.",
        "ML_phase": "They construct a Bayesian filtering reward model by first collecting historical interaction data to learn prior parameters, then apply meta-learning to estimate the covariance structures for progressive feedback. Subsequently, this model is integrated into an online Thompson sampling framework, enabling iterative updates with new partial observations at each round to balance exploration and exploitation."
    },
    "kdd23/2306.07930v1.json": {
        "title": "Reducing Exposure to Harmful Content via Graph Rewiring",
        "abs": "Most media content consumed today is provided by digital platforms that\naggregate input from diverse sources, where access to information is mediated\nby recommendation algorithms. One principal challenge in this context is\ndealing with content that is considered harmful. Striking a balance between\ncompeting stakeholder interests, rather than block harmful content altogether,\none approach is to minimize the exposure to such content that is induced\nspecifically by algorithmic recommendations. Hence, modeling media items and\nrecommendations as a directed graph, we study the problem of reducing the\nexposure to harmful content via edge rewiring. We formalize this problem using\nabsorbing random walks, and prove that it is NP-hard and NP-hard to approximate\nto within an additive error, while under realistic assumptions, the greedy\nmethod yields a (1-1/e)-approximation. Thus, we introduce Gamine, a fast greedy\nalgorithm that can reduce the exposure to harmful content with or without\nquality constraints on recommendations. By performing just 100 rewirings on\nYouTube graphs with several hundred thousand edges, Gamine reduces the initial\nexposure by 50%, while ensuring that its recommendations are at most 5% less\nrelevant than the original recommendations. Through extensive experiments on\nsynthetic data and real-world data from video recommendation and news feed\napplications, we confirm the effectiveness, robustness, and efficiency of\nGamine in practice.",
        "Task": "They address a graph-level optimization task that rewires edges in a recommendation graph to minimize the overall “exposure” to harmful content. Experiments are conducted on both synthetic data and real-world recommendation graphs (YouTube, news feeds) to assess performance under various parameter settings.",
        "Data_type": "They work with directed, homogeneous graphs whose nodes represent content items (videos or news articles) labeled by real-valued or binary “harmfulness” costs, and edges encode recommendation links. The node attributes (e.g., veracity scores, channel categories) generally stem from textual metadata.",
        "Data_domain": "They construct and analyze both synthetic datasets and real-world recommendation graphs. The real-world data come from YouTube video recommendations and NELA-GT news feeds.",
        "ML_phase": "They propose a post-processing step that operates on the already generated recommendation graph (i.e., after initial model training and inference) by rewiring edges to minimize harmful exposure. Specifically, they compute a Markov-chain-based objective and greedily edit edge connections while maintaining relevance constraints, thus adjusting recommendations downstream of any learned ranking model."
    },
    "kdd23/2309.05086v2.json": {
        "title": "Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler",
        "abs": "We propose a neuralized undirected graphical model called Neural-Hidden-CRF\nto solve the weakly-supervised sequence labeling problem. Under the umbrella of\nprobabilistic undirected graph theory, the proposed Neural-Hidden-CRF embedded\nwith a hidden CRF layer models the variables of word sequence, latent ground\ntruth sequence, and weak label sequence with the global perspective that\nundirected graphical models particularly enjoy. In Neural-Hidden-CRF, we can\ncapitalize on the powerful language model BERT or other deep models to provide\nrich contextual semantic knowledge to the latent ground truth sequence, and use\nthe hidden CRF layer to capture the internal label dependencies.\nNeural-Hidden-CRF is conceptually simple and empirically powerful. It obtains\nnew state-of-the-art results on one crowdsourcing benchmark and three\nweak-supervision benchmarks, including outperforming the recent advanced model\nCHMM by 2.80 F1 points and 2.23 F1 points in average generalization and\ninference performance, respectively.",
        "Task": "They study a weakly supervised sequence labeling task addressing token-level classification (e.g., named entity recognition). They evaluate the method on several sequence labeling benchmarks (CoNLL-03, WikiGold, MIT-Restaurant) with crowdsourced or artificially defined annotation sources.",
        "Data_type": "They handle textual sequence data (e.g., named entity recognition) in which each token is labeled among multiple categories. Their approach focuses on sequence labeling tasks in NLP, leveraging contextual semantics for text-based inputs.",
        "Data_domain": "All datasets come from text-based domains, specifically news articles, web text, and restaurant reviews. They encompass both general and domain-specific narratives (e.g., news vs. restaurant-related content).",
        "ML_phase": "They design a hidden CRF layer on top of a deep encoder to jointly model the labeled sequences and weak sources, where training maximizes a globally normalized likelihood via dynamic programming. The resulting pipeline integrates parameter initialization from majority-vote labels, learns the weak source transition parameters, and uses Viterbi decoding for inference."
    },
    "kdd23/2306.09368v1.json": {
        "title": "Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time  Series",
        "abs": "Irregularly sampled multivariate time series are ubiquitous in various\nfields, particularly in healthcare, and exhibit two key characteristics:\nintra-series irregularity and inter-series discrepancy. Intra-series\nirregularity refers to the fact that time-series signals are often recorded at\nirregular intervals, while inter-series discrepancy refers to the significant\nvariability in sampling rates among diverse series. However, recent advances in\nirregular time series have primarily focused on addressing intra-series\nirregularity, overlooking the issue of inter-series discrepancy. To bridge this\ngap, we present Warpformer, a novel approach that fully considers these two\ncharacteristics. In a nutshell, Warpformer has several crucial designs,\nincluding a specific input representation that explicitly characterizes both\nintra-series irregularity and inter-series discrepancy, a warping module that\nadaptively unifies irregular time series in a given scale, and a customized\nattention module for representation learning. Additionally, we stack multiple\nwarping and attention modules to learn at different scales, producing\nmulti-scale representations that balance coarse-grained and fine-grained\nsignals for downstream tasks. We conduct extensive experiments on widely used\ndatasets and a new large-scale benchmark built from clinical databases. The\nresults demonstrate the superiority of Warpformer over existing\nstate-of-the-art approaches.",
        "Task": "They pursue multiple classification objectives, including in-hospital mortality, decompensation, length of stay, next-timepoint measurement, clinical intervention prediction, and human activity recognition. These classification tasks are evaluated on several datasets such as PhysioNet, Human Activity, and newly constructed MIMIC-III-based benchmarks.",
        "Data_type": "They deal with irregularly sampled multivariate clinical time-series data from sources such as PhysioNet and MIMIC-III, encompassing multiple physiological signals and interventions recorded at uneven intervals. This setup focuses on capturing both intra-series irregularities (irregular sampling within a single time series) and inter-series discrepancies (varying sampling rates across multiple signals).",
        "Data_domain": "The data originates from clinical healthcare settings, encompassing multivariate time-series records of patients (e.g., MIMIC-III ICU data) with various physiological measurements and interventions. These datasets capture real-world hospital admission information for tasks such as mortality prediction, decompensation, length of stay estimation, and other clinical interventions.",
        "ML_phase": "Warpformer’s pipeline begins with an input encoder that arranges irregular time-series data (values, time, intervals, types) into a structured representation. Subsequently, multiple Warpformer layers—each containing a warping module for adaptive unification and a doubly self-attention mechanism—generate multi-scale embeddings fed into a final task-specific decoder, with all components trained end-to-end."
    },
    "kdd23/2307.07832v1.json": {
        "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with  Data Augmentation",
        "abs": "Graph Neural Networks (GNNs) have received increasing attention due to their\nability to learn from graph-structured data. However, their predictions are\noften not interpretable. Post-hoc instance-level explanation methods have been\nproposed to understand GNN predictions. These methods seek to discover\nsubstructures that explain the prediction behavior of a trained GNN. In this\npaper, we shed light on the existence of the distribution shifting issue in\nexisting methods, which affects explanation quality, particularly in\napplications on real-life datasets with tight decision boundaries. To address\nthis issue, we introduce a generalized Graph Information Bottleneck (GIB) form\nthat includes a label-independent graph variable, which is equivalent to the\nvanilla GIB. Driven by the generalized GIB, we propose a graph mixup method,\nMixupExplainer, with a theoretical guarantee to resolve the distribution\nshifting issue. We conduct extensive experiments on both synthetic and\nreal-world datasets to validate the effectiveness of our proposed mixup\napproach over existing approaches. We also provide a detailed analysis of how\nour proposed approach alleviates the distribution shifting issue.",
        "Task": "They address both node-level and graph-level classification tasks. Experiments are conducted on synthetic datasets (BA-Shapes, BA-Community, Tree-Cycle, Tree-Grid) for node classification and on BA-2Motifs and MUTAG for graph classification.",
        "Data_type": "They focus on graph-structured inputs, where each instance is represented by nodes with associated features (e.g., node attributes) and edges forming an adjacency matrix. Datasets include diverse synthetic graphs (e.g., BA shapes) and real-world molecular graphs (e.g., MUTAG).",
        "Data_domain": "The paper focuses on graph-structured data drawn from social networks, molecular structures, traffic flows, and knowledge graphs. It also uses synthetic graph datasets and a real-world molecular dataset (MUTAG) for evaluation.",
        "ML_phase": "They introduce a post-hoc explanation pipeline that, after training a GNN, generates augmented graphs by mixing the identified subgraph with a randomly sampled “label-irrelevant” structure. This mixup is incorporated into the explanation step of the ML pipeline, minimally modifying the existing GNN architecture and optimization process."
    },
    "kdd23/2307.04287v1.json": {
        "title": "Generalizing Graph ODE for Learning Complex System Dynamics across  Environments",
        "abs": "Learning multi-agent system dynamics has been extensively studied for various\nreal-world applications, such as molecular dynamics in biology. Most of the\nexisting models are built to learn single system dynamics from observed\nhistorical data and predict the future trajectory. In practice, however, we\nmight observe multiple systems that are generated across different\nenvironments, which differ in latent exogenous factors such as temperature and\ngravity. One simple solution is to learn multiple environment-specific models,\nbut it fails to exploit the potential commonalities among the dynamics across\nenvironments and offers poor prediction results where per-environment data is\nsparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary\nDifferential Equations), a machine learning framework for learning continuous\nmulti-agent system dynamics across environments. Our model learns system\ndynamics using neural ordinary differential equations (ODE) parameterized by\nGraph Neural Networks (GNNs) to capture the continuous interaction among\nagents. We achieve the model generalization by assuming the dynamics across\ndifferent environments are governed by common physics laws that can be captured\nvia learning a shared ODE function. The distinct latent exogenous factors\nlearned for each environment are incorporated into the ODE function to account\nfor their differences. To improve model performance, we additionally design two\nregularization losses to (1) enforce the orthogonality between the learned\ninitial states and exogenous factors via mutual information minimization; and\n(2) reduce the temporal variance of learned exogenous factors within the same\nsystem via contrastive learning. Experiments over various physical simulations\nshow that our model can accurately predict system dynamics, especially in the\nlong range, and can generalize well to new systems with few observations.",
        "Task": "They address a regression task of predicting multi-agent trajectories over continuous time, given partial observations of agents’ states. The effectiveness of this trajectory forecasting is evaluated on physical simulation datasets (Water and Lennard-Jones) with mean squared error.",
        "Data_type": "They handle graph-structured multi-agent trajectory data with nodes representing agents (e.g., particles), and edges dynamically formed via a distance threshold. Node features include continuous-time positional, velocity, and acceleration attributes for capturing complex system interactions across different environments.",
        "Data_domain": "The paper’s datasets come from physical simulations of multi-agent systems, specifically fluid dynamics with water in various container shapes and molecular dynamics under different temperatures. They therefore cover two main domains: fluid simulation and atomic-level molecular simulation.",
        "ML_phase": "They preprocess trajectories by sliding-window splitting into observation and prediction segments, then build temporal graphs for a GNN-based initial state encoder and an environment encoder to extract latent factors. The model couples a shared GNN-ODE for continuous dynamics with environment-specific vectors, and is trained end-to-end via KL divergence on initial states, contrastive regularization for time-invariance, and mutual information minimization for factor disentanglement."
    },
    "kdd23/2307.08232v1.json": {
        "title": "Learning for Counterfactual Fairness from Observational Data",
        "abs": "Fairness-aware machine learning has attracted a surge of attention in many\ndomains, such as online advertising, personalized recommendation, and social\nmedia analysis in web applications. Fairness-aware machine learning aims to\neliminate biases of learning models against certain subgroups described by\ncertain protected (sensitive) attributes such as race, gender, and age. Among\nmany existing fairness notions, counterfactual fairness is a popular notion\ndefined from a causal perspective. It measures the fairness of a predictor by\ncomparing the prediction of each individual in the original world and that in\nthe counterfactual worlds in which the value of the sensitive attribute is\nmodified. A prerequisite for existing methods to achieve counterfactual\nfairness is the prior human knowledge of the causal model for the data.\nHowever, in real-world scenarios, the underlying causal model is often unknown,\nand acquiring such human knowledge could be very difficult. In these scenarios,\nit is risky to directly trust the causal models obtained from information\nsources with unknown reliability and even causal discovery methods, as\nincorrect causal models can consequently bring biases to the predictor and lead\nto unfair predictions. In this work, we address the problem of counterfactually\nfair prediction from observational data without given causal models by\nproposing a novel framework CLAIRE. Specifically, under certain general\nassumptions, CLAIRE effectively mitigates the biases from the sensitive\nattribute with a representation learning framework based on counterfactual data\naugmentation and an invariant penalty. Experiments conducted on both synthetic\nand real-world datasets validate the superiority of CLAIRE in both\ncounterfactual fairness and prediction performance.",
        "Task": "They address both regression (Law School dataset) and classification (Adult dataset) tasks, also validating results on a synthetic dataset. Each setting evaluates the model’s predictive performance under fairness constraints.",
        "Data_type": "They handle tabular data with numerical and categorical variables (e.g., income level, race). The method is evaluated on real-world (Law School, Adult) and synthetic tabular datasets without reliance on graph structures, images, or other multi-modal formats.",
        "Data_domain": "They employ datasets from an academic context (Law School data), an economic context (UCI Adult income data), and additionally generate a synthetic dataset. The paper thus spans both real-world (academic and economic) and artificially constructed data domains.",
        "ML_phase": "They first train a VAE-based module to generate counterfactual versions of the input data, then learn representations that remove sensitive-attribute influences. Next, those representations feed into a predictor trained with an invariance penalty and a counterfactual-fairness constraint."
    },
    "kdd23/2306.10347v2.json": {
        "title": "DCdetector: Dual Attention Contrastive Representation Learning for Time  Series Anomaly Detection",
        "abs": "Time series anomaly detection is critical for a wide range of applications.\nIt aims to identify deviant samples from the normal sample distribution in time\nseries. The most fundamental challenge for this task is to learn a\nrepresentation map that enables effective discrimination of anomalies.\nReconstruction-based methods still dominate, but the representation learning\nwith anomalies might hurt the performance with its large abnormal loss. On the\nother hand, contrastive learning aims to find a representation that can clearly\ndistinguish any instance from the others, which can bring a more natural and\npromising representation for time series anomaly detection. In this paper, we\npropose DCdetector, a multi-scale dual attention contrastive representation\nlearning model. DCdetector utilizes a novel dual attention asymmetric design to\ncreate the permutated environment and pure contrastive loss to guide the\nlearning process, thus learning a permutation invariant representation with\nsuperior discrimination abilities. Extensive experiments show that DCdetector\nachieves state-of-the-art results on multiple time series anomaly detection\nbenchmark datasets. Code is publicly available at\nhttps://github.com/DAMO-DI-ML/KDD2023-DCdetector.",
        "Task": "They address an unsupervised anomaly detection task in time series data. The authors evaluate performance on eight real-world benchmark datasets covering both univariate and multivariate scenarios.",
        "Data_type": "The paper handles univariate and multivariate time-series data (up to 55 sensor channels) collected from real-world industrial and sensor-based sources. It does not involve graph-structured inputs, multi-modal image-text data, or molecular SMILES/fingerprint representations.",
        "Data_domain": "They use multivariate and univariate time series from NASA space sensors, eBay server machines, water treatment infrastructure, solar weather monitoring, and IoT-based water quality systems. Additionally, they include datasets drawn from real-world industrial servers and a univariate repository with diverse natural sources.",
        "ML_phase": "They first normalize and patch the time-series data independently by channel, then feed it into a dual-branch attention-based contrastive framework that learns consistent representations without reconstruction. Model training employs a purely contrastive loss over these dual-view embeddings, and anomalies are identified by thresholding the resulting representation discrepancies."
    },
    "kdd23/2306.09364v4.json": {
        "title": "TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series  Forecasting",
        "abs": "Transformers have gained popularity in time series forecasting for their\nability to capture long-sequence interactions. However, their high memory and\ncomputing requirements pose a critical bottleneck for long-term forecasting. To\naddress this, we propose TSMixer, a lightweight neural architecture exclusively\ncomposed of multi-layer perceptron (MLP) modules for multivariate forecasting\nand representation learning on patched time series. Inspired by MLP-Mixer's\nsuccess in computer vision, we adapt it for time series, addressing challenges\nand introducing validated components for enhanced accuracy. This includes a\nnovel design paradigm of attaching online reconciliation heads to the MLP-Mixer\nbackbone, for explicitly modeling the time-series properties such as hierarchy\nand channel-correlations. We also propose a novel Hybrid channel modeling and\ninfusion of a simple gating approach to effectively handle noisy channel\ninteractions and generalization across diverse datasets. By incorporating these\nlightweight components, we significantly enhance the learning capability of\nsimple MLP structures, outperforming complex Transformer models with minimal\ncomputing usage. Moreover, TSMixer's modular design enables compatibility with\nboth supervised and masked self-supervised learning methods, making it a\npromising building block for time-series Foundation Models. TSMixer outperforms\nstate-of-the-art MLP and Transformer models in forecasting by a considerable\nmargin of 8-60%. It also outperforms the latest strong benchmarks of\nPatch-Transformer models (by 1-2%) with a significant reduction in memory and\nruntime (2-3X). The source code of our model is officially released as\nPatchTSMixer in the HuggingFace. Model:\nhttps://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer\nExamples: https://github.com/ibm/tsfm/#notebooks-links",
        "Task": "They address multivariate time series forecasting, evaluating predictive accuracy on seven public benchmarks (ETT, Electricity, Traffic, and Weather datasets). The paper compares forecasting performance against state-of-the-art baselines using standard error metrics.",
        "Data_type": "This work focuses on continuous-valued multivariate time series data, where each sample contains multiple numeric channels (features) observed over time. It is designed for long-sequence forecasting and can handle varying input sequence lengths and channel dimensions without reliance on text, images, or graph-structured inputs.",
        "Data_domain": "These multivariate time series datasets span sensor-based measurements for weather, traffic, and electricity consumption. They encompass real-world environmental and operational settings, providing diverse sources of temporal data.",
        "ML_phase": "TSMixer standardizes inputs via RevIN, then segments them into patches (with optional masking for self-supervision) before passing them through a channel-independent MLP-Mixer backbone augmented by optional cross-channel and hierarchical reconciliation heads. It supports both supervised and masked self-supervised training workflows, enabling patch-based representation learning and final forecasting via specialized prediction heads."
    },
    "kdd23/2308.00535v1.json": {
        "title": "Graph Contrastive Learning with Generative Adversarial Network",
        "abs": "Graph Neural Networks (GNNs) have demonstrated promising results on\nexploiting node representations for many downstream tasks through supervised\nend-to-end training. To deal with the widespread label scarcity issue in\nreal-world applications, Graph Contrastive Learning (GCL) is leveraged to train\nGNNs with limited or even no labels by maximizing the mutual information\nbetween nodes in its augmented views generated from the original graph.\nHowever, the distribution of graphs remains unconsidered in view generation,\nresulting in the ignorance of unseen edges in most existing literature, which\nis empirically shown to be able to improve GCL's performance in our\nexperiments. To this end, we propose to incorporate graph generative\nadversarial networks (GANs) to learn the distribution of views for GCL, in\norder to i) automatically capture the characteristic of graphs for\naugmentations, and ii) jointly train the graph GAN model and the GCL model.\nSpecifically, we present GACN, a novel Generative Adversarial Contrastive\nlearning Network for graph representation learning. GACN develops a view\ngenerator and a view discriminator to generate augmented views automatically in\nan adversarial style. Then, GACN leverages these views to train a GNN encoder\nwith two carefully designed self-supervised learning losses, including the\ngraph contrastive loss and the Bayesian personalized ranking Loss. Furthermore,\nwe design an optimization framework to train all GACN modules jointly.\nExtensive experiments on seven real-world datasets show that GACN is able to\ngenerate high-quality augmented views for GCL and is superior to twelve\nstate-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly\ndiscovers that the generated views in data augmentation finally conform to the\nwell-known preferential attachment rule in online networks.",
        "Task": "They conduct node classification on the Cora and Citeseer datasets. They also perform link prediction on UCI, Taobao, Amazon, Last.fm, and Kuaishou.",
        "Data_type": "They focus on node-level tasks over homogeneous graphs with primarily textual or numerical node attributes (e.g., bag-of-words features) and standard adjacency relationships. The datasets used range from citation networks (Cora, Citeseer) to user-item interaction graphs (Taobao, Amazon, etc.) for link prediction and classification.",
        "Data_domain": "These datasets span academic citation networks (Cora, Citeseer) and real-world social/e-commerce networks (UCI, Taobao, Amazon, Last.fm, Kuaishou). They cover domains including scholarly publications, online shopping platforms, and user interaction networks.",
        "ML_phase": "They propose a three-module pipeline involving a view generator, a view discriminator, and a graph encoder, which are trained sequentially and iteratively (G-Steps, D-Steps, E-Steps) to produce augmented views for node-level representation learning. The pipeline combines adversarial classification, regularization, and self-supervised losses (contrastive and BPR) in a joint framework to yield robust node embeddings."
    },
    "kdd23/2306.10313v2.json": {
        "title": "Adversaries with Limited Information in the Friedkin--Johnsen Model",
        "abs": "In recent years, online social networks have been the target of adversaries\nwho seek to introduce discord into societies, to undermine democracies and to\ndestabilize communities. Often the goal is not to favor a certain side of a\nconflict but to increase disagreement and polarization. To get a mathematical\nunderstanding of such attacks, researchers use opinion-formation models from\nsociology, such as the Friedkin--Johnsen model, and formally study how much\ndiscord the adversary can produce when altering the opinions for only a small\nset of users. In this line of work, it is commonly assumed that the adversary\nhas full knowledge about the network topology and the opinions of all users.\nHowever, the latter assumption is often unrealistic in practice, where user\nopinions are not available or simply difficult to estimate accurately.\n  To address this concern, we raise the following question: Can an attacker sow\ndiscord in a social network, even when only the network topology is known? We\nanswer this question affirmatively. We present approximation algorithms for\ndetecting a small set of users who are highly influential for the disagreement\nand polarization in the network. We show that when the adversary radicalizes\nthese users and if the initial disagreement/polarization in the network is not\nvery high, then our method gives a constant-factor approximation on the setting\nwhen the user opinions are known. To find the set of influential users, we\nprovide a novel approximation algorithm for a variant of MaxCut in graphs with\npositive and negative edge weights. We experimentally evaluate our methods,\nwhich have access only to the network topology, and we find that they have\nsimilar performance as methods that have access to the network topology and all\nuser opinions. We further present an NP-hardness proof, which was an open\nquestion by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].",
        "Task": "They address a graph-level combinatorial optimization task of selecting a subset of nodes to maximize a global measure of discord (disagreement or polarization) in social networks. The performance is evaluated on real-world social network datasets.",
        "Data_type": "They work with weighted, undirected graph-structured data, where each node has an innate real-valued opinion in [0,1] (sometimes extended to [-1,1]) and edges carry real-valued weights (potentially positive or negative). These weights model interaction strengths in social networks, and the framework considers the entire graph with node-level opinion attributes.",
        "Data_domain": "They work with datasets drawn from social-network platforms (e.g., Twitter, Reddit, Google+), along with synthetic community-based graphs. These networks form the primary source of data for evaluating discord-measurement methods in the paper.",
        "ML_phase": "They situate their work in the data analysis and optimization stage of the ML pipeline, where the social-network graph and user opinions have already been collected. Their method uses matrix‐based computations (such as SDP relaxations or greedy matrix operations) to select a small set of users for opinion manipulation, maximizing network‐level discord without needing full user‐opinion data."
    },
    "kdd23/2308.03035v1.json": {
        "title": "Serverless Federated AUPRC Optimization for Multi-Party Collaborative  Imbalanced Data Mining",
        "abs": "Multi-party collaborative training, such as distributed learning and\nfederated learning, is used to address the big data challenges. However,\ntraditional multi-party collaborative training algorithms were mainly designed\nfor balanced data mining tasks and are intended to optimize accuracy\n(\\emph{e.g.}, cross-entropy). The data distribution in many real-world\napplications is skewed and classifiers, which are trained to improve accuracy,\nperform poorly when applied to imbalanced data tasks since models could be\nsignificantly biased toward the primary class. Therefore, the Area Under\nPrecision-Recall Curve (AUPRC) was introduced as an effective metric. Although\nsingle-machine AUPRC maximization methods have been designed, multi-party\ncollaborative algorithm has never been studied. The change from the\nsingle-machine to the multi-party setting poses critical challenges.\n  To address the above challenge, we study the serverless multi-party\ncollaborative AUPRC maximization problem since serverless multi-party\ncollaborative training can cut down the communications cost by avoiding the\nserver node bottleneck, and reformulate it as a conditional stochastic\noptimization problem in a serverless multi-party collaborative learning setting\nand propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to\ndirectly optimize the AUPRC. After that, we use the variance reduction\ntechnique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based\nvariance reduction (SLATE-M) algorithm to improve the convergence rate, which\nmatches the best theoretical convergence result reached by the single-machine\nonline method. To the best of our knowledge, this is the first work to solve\nthe multi-party collaborative AUPRC maximization problem.",
        "Task": "They tackle imbalanced binary classification by directly optimizing AUPRC on benchmarks (w7a, w8a, MNIST, Fashion-MNIST, CIFAR-10, Tiny-ImageNet). Their goal is to improve classification performance in a multi-party collaborative setting under skewed data distribution.",
        "Data_type": "They handle both tabular data (w7a, w8a) with 300-dimensional numeric features and image data (MNIST/Fashion-MNIST: 1×28×28, CIFAR-10: 3×32×32, Tiny-ImageNet: 3×64×64). Thus, the paper’s methods are applicable to numeric tabular inputs and standard multi-channel image inputs.",
        "Data_domain": "They conduct experiments on standard academic benchmark datasets spanning text (w7a, w8a) and vision (MNIST, Fashion-MNIST, CIFAR-10, Tiny-ImageNet) classification tasks. All datasets are publicly available and commonly used in machine learning research.",
        "ML_phase": "They introduce a serverless multi-party collaborative training framework that replaces conventional single-machine or centralized approaches in the model training phase. The method uses biased stochastic gradients and momentum-based variance reduction to jointly optimize imbalanced data classification metrics (AUPRC)."
    },
    "kdd23/2307.02759v1.json": {
        "title": "Knowledge Graph Self-Supervised Rationalization for Recommendation",
        "abs": "In this paper, we introduce a new self-supervised rationalization method,\ncalled KGRec, for knowledge-aware recommender systems. To effectively identify\ninformative knowledge connections, we propose an attentive knowledge\nrationalization mechanism that generates rational scores for knowledge\ntriplets. With these scores, KGRec integrates generative and contrastive\nself-supervised tasks for recommendation through rational masking. To highlight\nrationales in the knowledge graph, we design a novel generative task in the\nform of masking-reconstructing. By masking important knowledge with high\nrational scores, KGRec is trained to rebuild and highlight useful knowledge\nconnections that serve as rationales. To further rationalize the effect of\ncollaborative interactions on knowledge graph learning, we introduce a\ncontrastive learning task that aligns signals from knowledge and user-item\ninteraction views. To ensure noise-resistant contrasting, potential noisy edges\nin both graphs judged by the rational scores are masked. Extensive experiments\non three real-world datasets demonstrate that KGRec outperforms\nstate-of-the-art methods. We also provide the implementation codes for our\napproach at https://github.com/HKUDS/KGRec.",
        "Task": "They tackle a recommendation (ranking) task, aiming to predict user–item interactions in a top-N recommendation setting. Evaluations are conducted on real-world datasets containing users, items, and associated knowledge graphs for performance benchmarking.",
        "Data_type": "They use user–item interaction data and a heterogeneous knowledge graph, where nodes represent users, items, or entities, and edges denote relations between them. No additional modalities (e.g., images) are involved; the focus is on graph-structured data with relational triplets.\n\n\n",
        "Data_domain": "They use three real-world datasets covering music (Last-FM), news (MIND), and online fashion shopping (Alibaba-iFashion). Thus, the data domain spans consumer music streaming, news content, and e-commerce scenarios.",
        "ML_phase": "They first generate user-item and knowledge-graph inputs, rationalize important knowledge triplets, and then apply masked autoencoding plus contrastive graph augmentations. The final model is trained end-to-end with a BPR-based objective that combines generative reconstruction of masked triplets and cross-view contrastive alignment."
    },
    "kdd23/2306.14009v4.json": {
        "title": "Boosting Multitask Learning on Graphs through Higher-Order Task  Affinities",
        "abs": "Predicting node labels on a given graph is a widely studied problem with many\napplications, including community detection and molecular graph prediction.\nThis paper considers predicting multiple node labeling functions on graphs\nsimultaneously and revisits this problem from a multitask learning perspective.\nFor a concrete example, consider overlapping community detection: each\ncommunity membership is a binary node classification task. Due to complex\noverlapping patterns, we find that negative transfer is prevalent when we apply\nnaive multitask learning to multiple community detection, as task relationships\nare highly nonlinear across different node labeling. To address the challenge,\nwe develop an algorithm to cluster tasks into groups based on a higher-order\ntask affinity measure. We then fit a multitask model on each task group,\nresulting in a boosting procedure on top of the baseline model. We estimate the\nhigher-order task affinity measure between two tasks as the prediction loss of\none task in the presence of another task and a random subset of other tasks.\nThen, we use spectral clustering on the affinity score matrix to identify task\ngrouping. We design several speedup techniques to compute the higher-order\naffinity scores efficiently and show that they can predict negative transfers\nmore accurately than pairwise task affinities. We validate our procedure using\nvarious community detection and molecular graph prediction data sets, showing\nfavorable results compared with existing methods. Lastly, we provide a\ntheoretical analysis to show that under a planted block model of tasks on\ngraphs, our affinity scores can provably separate tasks into groups.",
        "Task": "They address multi-label node classification tasks on graphs for overlapping community detection, where each node can belong to multiple communities simultaneously. They also tackle multi-task regression and classification for molecular property prediction on graph-structured molecular data.",
        "Data_type": "Data_type:  \nThey address multiple node-level prediction tasks on homogeneous graph-structured data, primarily social networks with node embeddings (e.g., VERSE) and molecular graphs with atom-level features. The approach handles node attributes derived from embedding or coordinate-based representations, without incorporating explicit text, images, or SMILES/fingerprints.",
        "Data_domain": "The paper employs large-scale social network data (Amazon, YouTube, DBLP, LiveJournal) for overlapping community detection and molecular graph data (TUDatasets, OGB) for property prediction. Thus, the data domain spans both social networks and molecular graphs.",
        "ML_phase": "They sample random subsets of tasks, train a shared GNN-based multitask model on each subset, and record each task’s performance to build higher-order affinity scores. Spectral clustering on these scores then forms task groups, each of which is trained in its own multitask framework."
    },
    "kdd23/2306.08967v1.json": {
        "title": "Accelerating Dynamic Network Embedding with Billions of Parameter  Updates to Milliseconds",
        "abs": "Network embedding, a graph representation learning method illustrating\nnetwork topology by mapping nodes into lower-dimension vectors, is challenging\nto accommodate the ever-changing dynamic graphs in practice. Existing research\nis mainly based on node-by-node embedding modifications, which falls into the\ndilemma of efficient calculation and accuracy. Observing that the embedding\ndimensions are usually much smaller than the number of nodes, we break this\ndilemma with a novel dynamic network embedding paradigm that rotates and scales\nthe axes of embedding space instead of a node-by-node update. Specifically, we\npropose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which\nachieves an efficient and accurate dynamic network embedding by rotating and\nscaling the coordinate system where the network embedding resides with no more\nthan the number of edge modifications changes of node embeddings. Moreover, a\ndynamic Personalized PageRank is applied to the obtained network embeddings to\nenhance node embeddings and capture higher-order neighbor information\ndynamically. Experiments of node classification, link prediction, and graph\nreconstruction on different-sized dynamic graphs suggest that DAMF advances\ndynamic network embedding. Further, we unprecedentedly expand dynamic network\nembedding experiments to billion-edge graphs, where DAMF updates billion-level\nparameters in less than 10ms.",
        "Task": "They address dynamic network embedding for node classification, link prediction, and graph reconstruction tasks. These tasks are evaluated on six real-world graph datasets of varying scales (Wiki, Cora, Flickr, YouTube, Orkut, and Twitter).",
        "Data_type": "They handle dynamic, adjacency matrix-based graph data, mapping each node to a 128-dimensional embedding. The approach focuses on frequent node or edge changes in large-scale homogeneous graphs without explicit node features.",
        "Data_domain": "The data originates from social networks (Twitter, Orkut, YouTube, Flickr), academic citation networks (Cora), and hyperlinked Wikipedia pages (Wiki). This covers diverse sources spanning both large-scale social platforms and academic domains.",
        "ML_phase": "They propose a dynamic network embedding pipeline that begins by factorizing the adjacency matrix to obtain low-dimensional node representations, then incrementally updates these embeddings via a space projection mechanism whenever nodes or edges change. The approach includes an additional dynamic Personalized PageRank enhancement, and the entire framework operates in an unsupervised manner with truncated SVD plus minimal embedding modifications."
    },
    "kdd23/2306.14003v1.json": {
        "title": "Weakly Supervised Multi-Label Classification of Full-Text Scientific  Papers",
        "abs": "Instead of relying on human-annotated training samples to build a classifier,\nweakly supervised scientific paper classification aims to classify papers only\nusing category descriptions (e.g., category names, category-indicative\nkeywords). Existing studies on weakly supervised paper classification are less\nconcerned with two challenges: (1) Papers should be classified into not only\ncoarse-grained research topics but also fine-grained themes, and potentially\ninto multiple themes, given a large and fine-grained label space; and (2) full\ntext should be utilized to complement the paper title and abstract for\nclassification. Moreover, instead of viewing the entire paper as a long linear\nsequence, one should exploit the structural information such as citation links\nacross papers and the hierarchy of sections and paragraphs in each paper. To\ntackle these challenges, in this study, we propose FUTEX, a framework that uses\nthe cross-paper network structure and the in-paper hierarchy structure to\nclassify full-text scientific papers under weak supervision. A network-aware\ncontrastive fine-tuning module and a hierarchy-aware aggregation module are\ndesigned to leverage the two types of structural signals, respectively.\nExperiments on two benchmark datasets demonstrate that FUTEX significantly\noutperforms competitive baselines and is on par with fully supervised\nclassifiers that use 1,000 to 60,000 ground-truth training samples.",
        "Task": "They address weakly supervised multi-label text classification for large-scale scientific paper labeling. The proposed method is evaluated on two datasets (MAG-CS and PubMed), both containing over 10,000 fine-grained categories per paper.",
        "Data_type": "They handle graph-structured data, where each node is a scientific paper linked by citation edges and endowed with hierarchical text (title, abstract, paragraphs). Labels are also represented in text form (names and descriptions) to enable multi-label classification in a large, fine-grained label space.",
        "Data_domain": "These data come from the academic domain, specifically large-scale scientific paper corpora (MAG-CS and PubMed) derived from S2ORC. They include full-text publications organized into paragraphs, sections, and citation networks, enabling fine-grained multi-label classification.",
        "ML_phase": "They employ a three-stage pipeline: (1) network-aware contrastive fine-tuning of a pre-trained model using citation-based paragraph pairs, (2) hierarchy-aware aggregation of paragraph embeddings, and (3) a self-training phase that uses pseudo-labels to refine the final classifier. Data processing involves extracting and segmenting full-text papers, constructing paragraph-level representations, and leveraging both paper-level hierarchies and cross-paper networks for model optimization."
    },
    "kdd23/2306.09614v1.json": {
        "title": "HomoGCL: Rethinking Homophily in Graph Contrastive Learning",
        "abs": "Contrastive learning (CL) has become the de-facto learning paradigm in\nself-supervised learning on graphs, which generally follows the\n\"augmenting-contrasting\" learning scheme. However, we observe that unlike CL in\ncomputer vision domain, CL in graph domain performs decently even without\naugmentation. We conduct a systematic analysis of this phenomenon and argue\nthat homophily, i.e., the principle that \"like attracts like\", plays a key role\nin the success of graph CL. Inspired to leverage this property explicitly, we\npropose HomoGCL, a model-agnostic framework to expand the positive set using\nneighbor nodes with neighbor-specific significances. Theoretically, HomoGCL\nintroduces a stricter lower bound of the mutual information between raw node\nfeatures and node embeddings in augmented views. Furthermore, HomoGCL can be\ncombined with existing graph CL models in a plug-and-play way with light extra\ncomputational overhead. Extensive experiments demonstrate that HomoGCL yields\nmultiple state-of-the-art results across six public datasets and consistently\nbrings notable performance improvements when applied to various graph CL\nmethods. Code is avilable at https://github.com/wenzhilics/HomoGCL.",
        "Task": "They focus on node-level representation learning aimed at node classification and node clustering tasks. The authors evaluate their approach on six benchmark datasets (Cora, CiteSeer, PubMed, Amazon-Photo, Amazon-Computers, and ogbn-arXiv) to demonstrate performance.",
        "Data_type": "They focus on homogeneous graph-structured data, where nodes have textual or bag-of-words features and edges represent relationships (e.g., citations, co-purchases). The method handles node-level tasks (e.g., classification, clustering) driven by adjacency information in high-homophily settings.",
        "Data_domain": "These experiments primarily use academic citation graphs (Cora, CiteSeer, PubMed, ogbn-arXiv) and e-commerce co-purchase networks (Amazon-Photo, Amazon-Computers). Nodes represent papers or products with text-based features, and edges denote citations or co-purchases.",
        "ML_phase": "This work situates itself in the self-supervised training phase by first applying graph-level data augmentations (e.g., edge dropping, feature masking) and then encoding the augmented graphs using a GNN to learn node embeddings. The training framework employs a contrastive objective, enhanced by a GMM-based mechanism to expand positive samples and incorporate homophily information."
    },
    "kdd23/2306.15098v1.json": {
        "title": "Off-Policy Evaluation of Ranking Policies under Diverse User Behavior",
        "abs": "Ranking interfaces are everywhere in online platforms. There is thus an ever\ngrowing interest in their Off-Policy Evaluation (OPE), aiming towards an\naccurate performance evaluation of ranking policies using logged data. A\nde-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides\nan unbiased and consistent value estimate. However, it becomes extremely\ninaccurate in the ranking setup due to its high variance under large action\nspaces. To deal with this problem, previous studies assume either independent\nor cascade user behavior, resulting in some ranking versions of IPS. While\nthese estimators are somewhat effective in reducing the variance, all existing\nestimators apply a single universal assumption to every user, causing excessive\nbias and variance. Therefore, this work explores a far more general formulation\nwhere user behavior is diverse and can vary depending on the user context. We\nshow that the resulting estimator, which we call Adaptive IPS (AIPS), can be\nunbiased under any complex user behavior. Moreover, AIPS achieves the minimum\nvariance among all unbiased estimators based on IPS. We further develop a\nprocedure to identify the appropriate user behavior model to minimize the mean\nsquared error (MSE) of AIPS in a data-driven fashion. Extensive experiments\ndemonstrate that the empirical accuracy improvement can be significant,\nenabling effective OPE of ranking systems even under diverse user behavior.",
        "Task": "They tackle off-policy evaluation of ranking policies, i.e., estimating the performance of a new ranking policy using only previously logged (bandit) data without online deployment. They evaluate the approach on both synthetic datasets and real-world e-commerce logs containing user interactions with ranked items.",
        "Data_type": "They use tabular context data in ℝ^d and a finite set of discrete actions arranged into rankings, with each position in the ranking receiving a reward. The logged dataset consists of (context, ranking action, reward) tuples where the context dimension is d.",
        "Data_domain": "The paper experiments with both synthetic data and real-world data from an e-commerce platform. The real-world data is sourced from a module-ranking interface that optimizes click-based user interactions.",
        "ML_phase": "This paper introduces a new off-policy evaluation methodology situated in the post-training evaluation phase of a ranking system’s pipeline, where logged data are used to assess a policy’s performance without additional online experimentation. It refines the importance weighting step by adaptively modeling user behavior to address bias-variance tradeoffs in offline ranking evaluations."
    },
    "kdd23/2307.00077v1.json": {
        "title": "DECOR: Degree-Corrected Social Graph Refinement for Fake News Detection",
        "abs": "Recent efforts in fake news detection have witnessed a surge of interest in\nusing graph neural networks (GNNs) to exploit rich social context. Existing\nstudies generally leverage fixed graph structures, assuming that the graphs\naccurately represent the related social engagements. However, edge noise\nremains a critical challenge in real-world graphs, as training on suboptimal\nstructures can severely limit the expressiveness of GNNs. Despite initial\nefforts in graph structure learning (GSL), prior works often leverage node\nfeatures to update edge weights, resulting in heavy computational costs that\nhinder the methods' applicability to large-scale social graphs. In this work,\nwe approach the fake news detection problem with a novel aspect of social graph\nrefinement. We find that the degrees of news article nodes exhibit distinctive\npatterns, which are indicative of news veracity. Guided by this, we propose\nDECOR, a novel application of Degree-Corrected Stochastic Blockmodels to the\nfake news detection problem. Specifically, we encapsulate our empirical\nobservations into a lightweight social graph refinement component that\niteratively updates the edge weights via a learnable degree correction mask,\nwhich allows for joint optimization with a GNN-based detector. Extensive\nexperiments on two real-world benchmarks validate the effectiveness and\nefficiency of DECOR.",
        "Task": "They propose a binary classification task (real vs. fake) at the node level on a social graph, where each node corresponds to a news article. Performance is evaluated on PolitiFact and GossipCop from the FakeNewsNet benchmark.",
        "Data_type": "They handle homogeneous graph-structured data where each node is a news article represented by a 768-dimensional textual embedding. Edges denote the co-engagement counts (number of common readers) between connected news articles.",
        "Data_domain": "The paper’s datasets come from social media platforms (mainly Twitter), capturing user engagements like reposts and retweets with news articles. Specifically, they collect and analyze data from real-world benchmarks such as PolitiFact and GossipCop, which focus on political and celebrity news.",
        "ML_phase": "They construct a news engagement graph from user interactions, then learn a degree-based correction mask to refine edge weights by feeding co-engagement and node degrees into an MLP. Finally, they jointly train a GNN on the corrected adjacency matrix using cross-entropy loss to predict news veracity."
    },
    "kdd23/2307.10171v1.json": {
        "title": "LightPath: Lightweight and Scalable Path Representation Learning",
        "abs": "Movement paths are used widely in intelligent transportation and smart city\napplications. To serve such applications, path representation learning aims to\nprovide compact representations of paths that enable efficient and accurate\noperations when used for different downstream tasks such as path ranking and\ntravel cost estimation. In many cases, it is attractive that the path\nrepresentation learning is lightweight and scalable; in resource-limited\nenvironments and under green computing limitations, it is essential. Yet,\nexisting path representation learning studies focus on accuracy and pay at most\nsecondary attention to resource consumption and scalability.\n  We propose a lightweight and scalable path representation learning framework,\ntermed LightPath, that aims to reduce resource consumption and achieve\nscalability without affecting accuracy, thus enabling broader applicability.\nMore specifically, we first propose a sparse auto-encoder that ensures that the\nframework achieves good scalability with respect to path length. Next, we\npropose a relational reasoning framework to enable faster training of more\nrobust sparse path encoders. We also propose global-local knowledge\ndistillation to further reduce the size and improve the performance of sparse\npath encoders. Finally, we report extensive experiments on two real-world\ndatasets to offer insight into the efficiency, scalability, and effectiveness\nof the proposed framework.",
        "Task": "They address path-level regression tasks to estimate travel time and rank paths. Specifically, they evaluate performance on two real-world datasets (Aalborg and Chengdu) by predicting a continuous travel time value and a continuous ranking score for each path.",
        "Data_type": "They use graph-structured road network data, where each path is treated as a sequence of edges in a homogeneous graph. Edge embeddings (numeric vectors) capture road segments, and the resulting learned representation targets these edge-based sequences.",
        "Data_domain": "The data is sourced from real-world road networks and GPS trajectories in urban environments. It primarily encompasses location-based records and path sequences within the transportation domain.",
        "ML_phase": "They introduce a path representation learning pipeline that first applies random sparsity to reduce path length, then encodes the sparse paths (plus learnable path tokens and position embeddings) using a Transformer-based auto-encoder. The framework then trains via relational reasoning (cross-network and cross-view) and refines a smaller student model through global-local knowledge distillation."
    },
    "kdd23/2307.15198v1.json": {
        "title": "One-shot Joint Extraction, Registration and Segmentation of Neuroimaging  Data",
        "abs": "Brain extraction, registration and segmentation are indispensable\npreprocessing steps in neuroimaging studies. The aim is to extract the brain\nfrom raw imaging scans (i.e., extraction step), align it with a target brain\nimage (i.e., registration step) and label the anatomical brain regions (i.e.,\nsegmentation step). Conventional studies typically focus on developing separate\nmethods for the extraction, registration and segmentation tasks in a supervised\nsetting. The performance of these methods is largely contingent on the quantity\nof training samples and the extent of visual inspections carried out by experts\nfor error correction. Nevertheless, collecting voxel-level labels and\nperforming manual quality control on high-dimensional neuroimages (e.g., 3D\nMRI) are expensive and time-consuming in many medical studies. In this paper,\nwe study the problem of one-shot joint extraction, registration and\nsegmentation in neuroimaging data, which exploits only one labeled template\nimage (a.k.a. atlas) and a few unlabeled raw images for training. We propose a\nunified end-to-end framework, called JERS, to jointly optimize the extraction,\nregistration and segmentation tasks, allowing feedback among them.\nSpecifically, we use a group of extraction, registration and segmentation\nmodules to learn the extraction mask, transformation and segmentation mask,\nwhere modules are interconnected and mutually reinforced by self-supervision.\nEmpirical results on real-world datasets demonstrate that our proposed method\nperforms exceptionally in the extraction, registration and segmentation tasks.\nOur code and data can be found at https://github.com/Anonymous4545/JERS",
        "Task": "They address a unified approach for simultaneously performing brain extraction (i.e., segmenting out non-cerebral tissues), registering the extracted brain to a standard template, and segmenting anatomical structures in 3D MRI scans. Performance on these three tasks is evaluated using public brain MRI datasets that include ground-truth extraction masks, templates for registration, and labeled regions for segmentation.",
        "Data_type": "They employ 3D T1-weighted brain MRI scans, each represented as a volumetric dataset of size W×H×D. These volumes include brain masks and segmentation labels (when available) to facilitate one-shot extraction, registration, and segmentation.",
        "Data_domain": "The paper’s data domain is medical neuroimaging, specifically 3D brain MRI scans from publicly available datasets such as LPBA40, CC359, and IBSR. These datasets contain T1-weighted raw scans along with corresponding brain masks and anatomical labels.",
        "ML_phase": "The authors build a multi-stage end-to-end pipeline comprising three interconnected modules (extraction, registration, segmentation), each implemented with 3D CNN-based architectures. They preprocess the MRI scans by cropping/resizing to uniform dimensions and apply random transformations, then jointly train the modules using a combined similarity and segmentation loss through backpropagation."
    },
    "kdd23/2306.08604v1.json": {
        "title": "A Unified Framework of Graph Information Bottleneck for Robustness and  Membership Privacy",
        "abs": "Graph Neural Networks (GNNs) have achieved great success in modeling\ngraph-structured data. However, recent works show that GNNs are vulnerable to\nadversarial attacks which can fool the GNN model to make desired predictions of\nthe attacker. In addition, training data of GNNs can be leaked under membership\ninference attacks. This largely hinders the adoption of GNNs in high-stake\ndomains such as e-commerce, finance and bioinformatics. Though investigations\nhave been made in conducting robust predictions and protecting membership\nprivacy, they generally fail to simultaneously consider the robustness and\nmembership privacy. Therefore, in this work, we study a novel problem of\ndeveloping robust and membership privacy-preserving GNNs. Our analysis shows\nthat Information Bottleneck (IB) can help filter out noisy information and\nregularize the predictions on labeled samples, which can benefit robustness and\nmembership privacy. However, structural noises and lack of labels in node\nclassification challenge the deployment of IB on graph-structured data. To\nmitigate these issues, we propose a novel graph information bottleneck\nframework that can alleviate structural noises with neighbor bottleneck. Pseudo\nlabels are also incorporated in the optimization to minimize the gap between\nthe predictions on the labeled set and unlabeled set for membership privacy.\nExtensive experiments on real-world datasets demonstrate that our method can\ngive robust predictions and simultaneously preserve membership privacy.",
        "Task": "This work addresses semi-supervised node-level classification on graph-structured data. Evaluation is conducted on real-world benchmark datasets (Cora, Citeseer, Pubmed, and Flickr) for performance assessment.",
        "Data_type": "They focus on homogeneous graph-structured data with textual node attributes (e.g., bag-of-words features) for semi-supervised node classification. Their experiments involve citation networks (Cora, Citeseer, Pubmed) and a Flickr graph, all containing textual attributes and edges representing relationships among nodes.",
        "Data_domain": "The paper utilizes benchmark citation networks (Cora, Citeseer, Pubmed) from the academic domain and a large-scale Flickr graph from the social media domain.",
        "ML_phase": "They propose a pipeline that first processes a small labeled set and generates pseudo labels for unlabeled nodes, then adopts a dual-bottleneck GNN architecture (attribute bottleneck and neighbor bottleneck) to isolate task-relevant information. The training framework integrates these modules via an information bottleneck loss and a self-supervision mechanism for neighbor selection."
    },
    "kdd23/2306.13854v1.json": {
        "title": "Similarity Preserving Adversarial Graph Contrastive Learning",
        "abs": "Recent works demonstrate that GNN models are vulnerable to adversarial\nattacks, which refer to imperceptible perturbation on the graph structure and\nnode features. Among various GNN models, graph contrastive learning (GCL) based\nmethods specifically suffer from adversarial attacks due to their inherent\ndesign that highly depends on the self-supervision signals derived from the\noriginal graph, which however already contains noise when the graph is\nattacked. To achieve adversarial robustness against such attacks, existing\nmethods adopt adversarial training (AT) to the GCL framework, which considers\nthe attacked graph as an augmentation under the GCL framework. However, we find\nthat existing adversarially trained GCL methods achieve robustness at the\nexpense of not being able to preserve the node feature similarity. In this\npaper, we propose a similarity-preserving adversarial graph contrastive\nlearning (SP-AGCL) framework that contrasts the clean graph with two auxiliary\nviews of different properties (i.e., the node similarity-preserving view and\nthe adversarial view). Extensive experiments demonstrate that SP-AGCL achieves\na competitive performance on several downstream tasks, and shows its\neffectiveness in various scenarios, e.g., a network with adversarial attacks,\nnoisy labels, and heterophilous neighbors. Our code is available at\nhttps://github.com/yeonjun-in/torch-SP-AGCL.",
        "Task": "They focus on unsupervised graph representation learning to address node-level classification, link prediction, and node clustering tasks. These tasks are evaluated on multiple benchmark datasets to verify the performance of the learned node embeddings.",
        "Data_type": "They focus on graph-structured data with adjacency information and node attributes (often discrete or binary, e.g., bag-of-words features). The approach applies to homogeneous networks such as citation, co-purchase, and co-authorship graphs.",
        "Data_domain": "They use graph datasets collected from academic citation networks, co-purchase networks, co-authorship networks, and heterophilous networks.",
        "ML_phase": "They construct multiple graph views (two random augmentations, a structure-free similarity-preserving k-NN view, and an adversarially perturbed view), then feed them into a shared GCN-based encoder. The training framework applies a cross-view contrastive objective that aligns node representations across the clean, similarity-preserving, and adversarial views for robust unsupervised learning."
    },
    "kdd23/2307.11079v2.json": {
        "title": "Disentangled Dynamic Intrusion Detection",
        "abs": "Network-based intrusion detection system (NIDS) monitors network traffic for\nmalicious activities, forming the frontline defense against increasing attacks\nover information infrastructures. Although promising, our quantitative analysis\nshows that existing methods perform inconsistently in declaring various\nattacks, and perform poorly in few-shot intrusion detections. We reveal that\nthe underlying cause is entangled distributions of flow features. This\nmotivates us to propose DIDS-MFL, a disentangled intrusion detection method to\nhandle various intrusion detection scenarios. DIDS-MFL involves two key\ncomponents, respectively: a double Disentanglementbased Intrusion Detection\nSystem (DIDS) and a plug-and-play Multi-scale Few-shot Learning-based (MFL)\nintrusion detection module. Specifically, the proposed DIDS first disentangles\ntraffic features by a non-parameterized optimization, automatically\ndifferentiating tens and hundreds of complex features of various attacks. Such\ndifferentiated features will be further disentangled to highlight the\nattack-specific features. Our DIDS additionally uses a novel graph diffusion\nmethod that dynamically fuses the network topology in evolving data streams.\nFurthermore, the proposed MFL involves an alternating optimization framework to\naddress the entangled representations in few-shot traffic threats with rigorous\nderivation. MFL first captures multiscale information in latent space to\ndistinguish attack-specific information and then optimizes the disentanglement\nterm to highlight the attack-specific information. Finally, MFL fuses and\nalternately solves them in an end-to-end way. Experiments show the superiority\nof our proposed DIDS-MFL. Our code is available at\nhttps://github.com/qcydm/DIDS-MFL",
        "Task": "They address a supervised classification task to distinguish network traffic as benign or malicious, including multi-class detection for various attack types. Additionally, they tackle few-shot classification and unknown attack detection using large-scale intrusion detection datasets.",
        "Data_type": "They process network traffic by transforming NetFlow data into node-edge representations on multi-layer dynamic graphs, where each node carries traffic attributes (e.g., IP, layer marks) and each edge encodes flow-based features. The approach specifically handles numeric/statistical attributes of nodes and edges in these graphs for large-scale intrusion detection.",
        "Data_domain": "The datasets originate from network traffic of various IoT/IIoT systems, encompassing flows from devices, routers, and servers. These large-scale real-world NetFlow-based traces capture a wide range of benign and malicious operations in contemporary cyber-infrastructures.",
        "ML_phase": "They transform raw network traffic into dynamic multi-layer graphs, then disentangle features (statistical and representational) before applying a diffusion-based GCN to fuse spatiotemporal information. The final classification is trained through a supervised or few-shot framework, producing alerts for both known and novel attacks."
    },
    "kdd23/2306.08921v1.json": {
        "title": "Multi-Temporal Relationship Inference in Urban Areas",
        "abs": "Finding multiple temporal relationships among locations can benefit a bunch\nof urban applications, such as dynamic offline advertising and smart public\ntransport planning. While some efforts have been made on finding static\nrelationships among locations, little attention is focused on studying\ntime-aware location relationships. Indeed, abundant location-based human\nactivities are time-varying and the availability of these data enables a new\nparadigm for understanding the dynamic relationships in a period among\nconnective locations. To this end, we propose to study a new problem, namely\nmulti-Temporal relationship inference among locations (Trial for short), where\nthe major challenge is how to integrate dynamic and geographical influence\nunder the relationship sparsity constraint. Specifically, we propose a solution\nto Trial with a graph learning scheme, which includes a spatially evolving\ngraph neural network (SEENet) with two collaborative components: spatially\nevolving graph convolution module (SEConv) and spatially evolving\nself-supervised learning strategy (SE-SSL). SEConv performs the intra-time\naggregation and inter-time propagation to capture the multifaceted spatially\nevolving contexts from the view of location message passing. In addition,\nSE-SSL designs time-aware self-supervised learning tasks in a global-local\nmanner with additional evolving constraint to enhance the location\nrepresentation learning and further handle the relationship sparsity. Finally,\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod over several state-of-the-art approaches.",
        "Task": "They address a multi-temporal link-level prediction task on dynamic location graphs, seeking to infer missing time-specific relationships among locations. Their experiments use four real-world citywide datasets (Beijing, Tokyo, New York, Chicago) to evaluate performance on this relationship inference problem.",
        "Data_type": "They handle dynamic graph-structured data where each node is a location (with coordinates) and edges denote time-specific location relationships from user queries or mobility logs. The relationships are decomposed across multiple daily segments, forming a multi-temporal location graph.",
        "Data_domain": "They utilize real-world urban data encompassing user query logs, check-ins, and taxi/bike trajectories from four major cities (Beijing, Tokyo, New York, and Chicago). Specifically, the datasets capture business-based relational behaviors (e.g., competitive and complementary usage patterns) and mobility-based flow data reflecting spatiotemporal travel activities.",
        "ML_phase": "They construct a dynamic location graph across multiple time segments from human mobility data, then apply a spatially evolving GNN architecture that combines intra-time aggregation with inter-time propagation. During training, they incorporate self-supervised objectives (global and local) to enhance embeddings and finally use a DistMult-based link prediction head to infer multi-temporal relationships."
    },
    "kdd23/2308.04708v1.json": {
        "title": "Generative Perturbation Analysis for Probabilistic Black-Box Anomaly  Attribution",
        "abs": "We address the task of probabilistic anomaly attribution in the black-box\nregression setting, where the goal is to compute the probability distribution\nof the attribution score of each input variable, given an observed anomaly. The\ntraining dataset is assumed to be unavailable. This task differs from the\nstandard XAI (explainable AI) scenario, since we wish to explain the anomalous\ndeviation from a black-box prediction rather than the black-box model itself.\n  We begin by showing that mainstream model-agnostic explanation methods, such\nas the Shapley values, are not suitable for this task because of their\n``deviation-agnostic property.'' We then propose a novel framework for\nprobabilistic anomaly attribution that allows us to not only compute\nattribution scores as the predictive mean but also quantify the uncertainty of\nthose scores. This is done by considering a generative process for\nperturbations that counter-factually bring the observed anomalous observation\nback to normalcy. We introduce a variational Bayes algorithm for deriving the\ndistributions of per variable attribution scores. To the best of our knowledge,\nthis is the first probabilistic anomaly attribution framework that is free from\nbeing deviation-agnostic.",
        "Task": "They focus on anomaly attribution in a black-box regression task, where unexpected deviations in continuous outputs must be explained at the input variable level. The method’s performance is evaluated on various regression datasets, including both synthetic (2D sinusoidal) and real-world examples (e.g., Diabetes, Boston Housing, California Housing).",
        "Data_type": "They handle real-valued input vectors of dimension M (x ∈ ℝ^M) with real-valued regression outputs. The approach is designed for numeric data in black-box regression settings.",
        "Data_domain": "They use both synthetic numeric data (“2Dsinusoidal”) and real-world datasets from medical (Diabetes) and housing/economic domains (Boston and California Housing). These datasets cover regression tasks in health and real-estate valuation contexts.",
        "ML_phase": "GPA operates as a post-hoc module in the ML pipeline, applied after any black-box regression model has been trained and without requiring the original training data. Its inference procedure fits seamlessly into the existing workflow by reframing anomaly attribution as a parameter estimation step, producing variable-wise score distributions once predictions are obtained."
    },
    "kdd23/2310.07477v1.json": {
        "title": "GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized  Adaptive Testing",
        "abs": "Computerized Adaptive Testing(CAT) refers to an online system that adaptively\nselects the best-suited question for students with various abilities based on\ntheir historical response records. Most CAT methods only focus on the quality\nobjective of predicting the student ability accurately, but neglect concept\ndiversity or question exposure control, which are important considerations in\nensuring the performance and validity of CAT. Besides, the students' response\nrecords contain valuable relational information between questions and knowledge\nconcepts. The previous methods ignore this relational information, resulting in\nthe selection of sub-optimal test questions. To address these challenges, we\npropose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly,\nthree objectives, namely quality, diversity and novelty, are introduced into\nthe Scalarized Multi-Objective Reinforcement Learning framework of CAT, which\nrespectively correspond to improving the prediction accuracy, increasing the\nconcept diversity and reducing the question exposure. We use an Actor-Critic\nRecommender to select questions and optimize three objectives simultaneously by\nthe scalarization function. Secondly, we utilize the graph neural network to\nlearn relation-aware embeddings of questions and concepts. These embeddings are\nable to aggregate neighborhood information in the relation graphs between\nquestions and concepts. We conduct experiments on three real-world educational\ndatasets, and show that GMOCAT not only outperforms the state-of-the-art\nmethods in the ability prediction, but also achieve superior performance in\nimproving the concept diversity and alleviating the question exposure. Our code\nis available at https://github.com/justarter/GMOCAT.",
        "Task": "They formulate a multi-objective reinforcement learning task for selecting questions in computerized adaptive testing. They evaluate the learned policy on real-world educational datasets to balance prediction accuracy, concept coverage, and question exposure.",
        "Data_type": "They handle graph-structured data where questions and concepts are represented as nodes with edges capturing correlation or prerequisite relations. The nodes can be associated with textual embeddings, and the model aggregates these relational signals to facilitate adaptive testing.",
        "Data_domain": "This paper employs real-world educational datasets generated from online learning platforms, capturing students’ responses to questions and associated concepts. Specifically, the data come from Eedi, ASSIST, and Junyi, providing extensive records of student performance across multiple knowledge concepts.",
        "ML_phase": "They preprocess each dataset (Eedi, ASSIST, Junyi) by removing students with fewer than 40 records, splitting data into training/validation/testing sets, and randomly partitioning each student’s records into a candidate set (for question selection) and a meta set (for evaluating ability estimates). The model then uses a graph-based relation aggregator for question/concept embeddings, a self-attentive state encoder to form states from prior responses, and an actor-critic (PPO) framework to jointly train a policy that optimizes quality, diversity, and novelty objectives in question selection."
    },
    "DeepDTA/1801.10193v2.json": {
        "title": "DeepDTA: Deep Drug-Target Binding Affinity Prediction",
        "abs": "The identification of novel drug-target (DT) interactions is a substantial\npart of the drug discovery process. Most of the computational methods that have\nbeen proposed to predict DT interactions have focused on binary classification,\nwhere the goal is to determine whether a DT pair interacts or not. However,\nprotein-ligand interactions assume a continuum of binding strength values, also\ncalled binding affinity and predicting this value still remains a challenge.\nThe increase in the affinity data available in DT knowledge-bases allows the\nuse of advanced learning techniques such as deep learning architectures in the\nprediction of binding affinities. In this study, we propose a deep-learning\nbased model that uses only sequence information of both targets and drugs to\npredict DT interaction binding affinities. The few studies that focus on DT\nbinding affinity prediction use either 3D structures of protein-ligand\ncomplexes or 2D features of compounds. One novel approach used in this work is\nthe modeling of protein sequences and compound 1D representations with\nconvolutional neural networks (CNNs). The results show that the proposed deep\nlearning based model that uses the 1D representations of targets and drugs is\nan effective approach for drug target binding affinity prediction. The model in\nwhich high-level representations of a drug and a target are constructed via\nCNNs achieved the best Concordance Index (CI) performance in one of our larger\nbenchmark data sets, outperforming the KronRLS algorithm and SimBoost, a\nstate-of-the-art method for DT binding affinity prediction.",
        "Task": "They formulate drug–target affinity prediction as a regression task, using the Davis and KIBA datasets with continuous binding affinity values. Performance is evaluated through metrics such as the Concordance Index and Mean Squared Error.",
        "Data_type": "They handle 1D textual sequence data: amino acid sequences for proteins and SMILES strings for compounds. Each sequence is label-encoded and either truncated or zero-padded to a fixed length before processing.",
        "Data_domain": "The data come from the biomedical domain, specifically involving drug–target binding affinity measurements for protein–ligand interactions. The authors utilize large-scale kinase inhibitor datasets (Davis and KIBA) that provide continuous binding strength values (e.g., Kd or Ki).",
        "ML_phase": "They first convert raw protein and SMILES sequences into fixed-length integer encodings, applying truncation or zero-padding as needed, and then feed these representations into two separate 1D CNN blocks (for protein and compound) whose outputs are concatenated and passed through fully connected layers. The model is trained for binding affinity prediction via MSE loss using the Adam optimizer, with hyperparameters tuned by nested cross-validation on Davis and KIBA datasets."
    },
    "DeepDTA/2004.11424v1.json": {
        "title": "MolTrans: Molecular Interaction Transformer for Drug Target Interaction  Prediction",
        "abs": "Drug target interaction (DTI) prediction is a foundational task for in silico\ndrug discovery, which is costly and time-consuming due to the need of\nexperimental search over large drug compound space. Recent years have witnessed\npromising progress for deep learning in DTI predictions. However, the following\nchallenges are still open: (1) the sole data-driven molecular representation\nlearning approaches ignore the sub-structural nature of DTI, thus produce\nresults that are less accurate and difficult to explain; (2) existing methods\nfocus on limited labeled data while ignoring the value of massive unlabelled\nmolecular data. We propose a Molecular Interaction Transformer (MolTrans) to\naddress these limitations via: (1) knowledge inspired sub-structural pattern\nmining algorithm and interaction modeling module for more accurate and\ninterpretable DTI prediction; (2) an augmented transformer encoder to better\nextract and capture the semantic relations among substructures extracted from\nmassive unlabeled biomedical data. We evaluate MolTrans on real world data and\nshow it improved DTI prediction performance compared to state-of-the-art\nbaselines.",
        "Task": "They address a binary classification task of predicting whether a given drug–protein pair will interact, evaluated on the MINER DTI dataset from the BIOSNAP collection. Performance is measured using ROC-AUC and PR-AUC on both randomly selected and unseen drug/target splits.",
        "Data_type": "They process textual SMILES representations for drugs and textual amino acid sequences for proteins, leveraging frequent sub-structure fingerprints derived from large unlabeled datasets. The data is strictly textual (no images or graph node-edge inputs), focusing on SMILES-based token sequences and protein amino acid token sequences.",
        "Data_domain": "The data is from large-scale biomedical sources (e.g., Uniprot for proteins, ChEMBL for drugs), focusing on drug-target interaction information. It encompasses unlabeled and labeled bio-molecular sequences (SMILES for drugs, amino-acid sequences for proteins) to aid in in silico drug discovery.",
        "ML_phase": "They decompose raw protein and drug sequences into frequent sub-structures using a specialized mining algorithm on large unlabeled data, then feed these sub-structures through transformer encoders for contextual embedding and a CNN-based interaction module for pairwise sub-structural modeling. The end-to-end training framework uses a binary cross-entropy loss to optimize all components jointly for robust DTI classification."
    },
    "DeepDTA/1902.04166v1.json": {
        "title": "WideDTA: prediction of drug-target binding affinity",
        "abs": "Motivation: Prediction of the interaction affinity between proteins and\ncompounds is a major challenge in the drug discovery process. WideDTA is a\ndeep-learning based prediction model that employs chemical and biological\ntextual sequence information to predict binding affinity.\n  Results: WideDTA uses four text-based information sources, namely the protein\nsequence, ligand SMILES, protein domains and motifs, and maximum common\nsubstructure words to predict binding affinity. WideDTA outperformed one of the\nstate of the art deep learning methods for drug-target binding affinity\nprediction, DeepDTA on the KIBA dataset with a statistical significance. This\nindicates that the word-based sequence representation adapted by WideDTA is a\npromising alternative to the character-based sequence representation approach\nin deep learning models for binding affinity prediction, such as the one used\nin DeepDTA. In addition, the results showed that, given the protein sequence\nand ligand SMILES, the inclusion of protein domain and motif information as\nwell as ligand maximum common substructure words do not provide additional\nuseful information for the deep learning model. Interestingly, however, using\nonly domain and motif information to represent proteins achieved similar\nperformance to using the full protein sequence, suggesting that important\nbinding relevant information is contained within the protein motifs and\ndomains.",
        "Task": "They address a regression task by predicting continuous drug-target binding affinity values. Performance is evaluated on the Davis and KIBA datasets, which provide quantitative affinity measurements for training and testing.",
        "Data_type": "They utilize text-based protein sequences (including domains/motifs) and text-based ligand representations (SMILES strings and maximum common substructures). No additional structural or graph-based data is used, relying solely on these textual formats for drug–target interaction modeling.",
        "Data_domain": "The data come from the biological/chemical domain, specifically focused on protein-ligand binding affinity. They use benchmark datasets (Davis and KIBA) containing kinase family proteins and small-molecule compounds for drug-target interaction studies.",
        "ML_phase": "The paper introduces a CNN-based model (WideDTA) that processes text-derived features from protein sequences and ligand SMILES (optionally alongside domain/motif and MCS data), embedding them using a 1D-convolution architecture. These extracted features are concatenated and passed through fully connected layers trained (via MSE and CI metrics) to predict binding affinities."
    },
    "DeepDTA/1911.04738v1.json": {
        "title": "SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug  Discovery",
        "abs": "In drug-discovery-related tasks such as virtual screening, machine learning\nis emerging as a promising way to predict molecular properties. Conventionally,\nmolecular fingerprints (numerical representations of molecules) are calculated\nthrough rule-based algorithms that map molecules to a sparse discrete space.\nHowever, these algorithms perform poorly for shallow prediction models or small\ndatasets. To address this issue, we present SMILES Transformer. Inspired by\nTransformer and pre-trained language models from natural language processing,\nSMILES Transformer learns molecular fingerprints through unsupervised\npre-training of the sequence-to-sequence language model using a huge corpus of\nSMILES, a text representation system for molecules. We performed benchmarks on\n10 datasets against existing fingerprints and graph-based methods and\ndemonstrated the superiority of the proposed algorithms in small-data settings\nwhere pre-training facilitated good generalization. Moreover, we define a novel\nmetric to concurrently measure model accuracy and data efficiency.",
        "Task": "They perform both classification and regression tasks focused on molecular property prediction, using ten benchmark datasets from MoleculeNet. Specifically, ESOL, FreeSolv, and Lipophilicity are treated as regression tasks, while MUV, HIV, BACE, BBBP, Tox21, SIDER, and ClinTox are evaluated as classification tasks.",
        "Data_type": "They use text-based molecular sequences (SMILES) and convert them into a 1024-dimensional fingerprint representation. This approach does not involve graph-based node/edge information, images, or other multimodal inputs.",
        "Data_domain": "The data is sourced from large-scale chemical databases (e.g., ChEMBL24) consisting of molecular SMILES used for drug discovery. It encompasses chemical properties and bioactivity information relevant to tasks such as solubility, lipophilicity, and toxicity prediction.",
        "ML_phase": "They pre-train an encoder-decoder Transformer by randomly enumerating input SMILES, then optimize via cross-entropy with Adam to learn contextual molecular representations. These representations are subsequently fed to simple predictive models, completing the training pipeline and enabling efficient property prediction under small data settings."
    },
    "LinkPrediction/2412.00261v1.json": {
        "title": "Attribute-Enhanced Similarity Ranking for Sparse Link Prediction",
        "abs": "Link prediction is a fundamental problem in graph data. In its most realistic\nsetting, the problem consists of predicting missing or future links between\nrandom pairs of nodes from the set of disconnected pairs. Graph Neural Networks\n(GNNs) have become the predominant framework for link prediction. GNN-based\nmethods treat link prediction as a binary classification problem and handle the\nextreme class imbalance -- real graphs are very sparse -- by sampling\n(uniformly at random) a balanced number of disconnected pairs not only for\ntraining but also for evaluation. However, we show that the reported\nperformance of GNNs for link prediction in the balanced setting does not\ntranslate to the more realistic imbalanced setting and that simpler\ntopology-based approaches are often better at handling sparsity. These findings\nmotivate Gelato, a similarity-based link-prediction method that applies (1)\ngraph learning based on node attributes to enhance a topological heuristic, (2)\na ranking loss for addressing class imbalance, and (3) a negative sampling\nscheme that efficiently selects hard training pairs via graph partitioning.\nExperiments show that Gelato outperforms existing GNN-based alternatives.",
        "Task": "They tackle a link prediction task, treating it as an imbalanced binary classification problem over graph edges. They evaluate performance on multiple real-world graph benchmarks (such as Cora, CiteSeer, PubMed, OGBL-DDI, and OGBL-Collab).",
        "Data_type": "They work with graph-structured data where each node is enriched by textual or numeric attributes in a homogeneous graph setting. No other modalities (e.g., images or SMILES) are considered.",
        "Data_domain": "The paper primarily uses academic citation networks (Cora, CiteSeer, PubMed) together with additional datasets capturing drug-drug interactions (OGBL-DDI) and collaboration patterns (OGBL-collab). Consequently, the data domain is predominantly academic, with a biomedical extension in the drug-interaction dataset.",
        "ML_phase": "They augment the input graph by adding attribute-similarity edges and learn the corresponding weights via an MLP, then apply a topological heuristic (Autocovariance) to generate pairwise scores. Training uses partition-based negative sampling (to include hard negatives) with an N-pair rank-based loss, thus integrating data processing, model design, and end-to-end optimization in one unified framework."
    },
    "LinkPrediction/2411.01410v1.json": {
        "title": "PageRank Bandits for Link Prediction",
        "abs": "Link prediction is a critical problem in graph learning with broad\napplications such as recommender systems and knowledge graph completion.\nNumerous research efforts have been directed at solving this problem, including\napproaches based on similarity metrics and Graph Neural Networks (GNN).\nHowever, most existing solutions are still rooted in conventional supervised\nlearning, which makes it challenging to adapt over time to changing customer\ninterests and to address the inherent dilemma of exploitation versus\nexploration in link prediction. To tackle these challenges, this paper\nreformulates link prediction as a sequential decision-making process, where\neach link prediction interaction occurs sequentially. We propose a novel fusion\nalgorithm, PRB (PageRank Bandits), which is the first to combine contextual\nbandits with PageRank for collaborative exploitation and exploration. We also\nintroduce a new reward formulation and provide a theoretical performance\nguarantee for PRB. Finally, we extensively evaluate PRB in both online and\noffline settings, comparing it with bandit-based and graph-based methods. The\nempirical success of PRB demonstrates the value of the proposed fusion\napproach. Our code is released at https://github.com/jiaruzouu/PRB.",
        "Task": "They tackle link prediction on graphs in both online and offline settings, and also extend it for node classification. They evaluate these tasks using real-world benchmark datasets.",
        "Data_type": "They operate on graph-structured data, where each node is represented by a d-dimensional context vector and edges capture relationships in a homogeneous graph. Additionally, they extend this to node classification by transforming class labels into supernode links, but the core data type remains node-level feature vectors and adjacency information.",
        "Data_domain": "The datasets come from user–item recommendation systems (e.g., MovieLens, Amazon), social networks (Facebook, GR-QC), and academic citation networks (Cora, Citeseer, Pubmed). Each is represented as a graph or set of edges and nodes reflecting domains like online recommendations and scholarly citations.",
        "ML_phase": "They sequentially construct training examples by framing each link prediction instance as a contextual bandit round (or, for node classification, transforming it into link prediction via super-nodes), thereby generating data samples on the fly. The model architecture combines a neural exploitation network, a neural exploration network, and a PageRank component, with gradient-based updates applied after each round in a contextual bandit training framework."
    },
    "LinkPrediction/2208.00850v3.json": {
        "title": "Subgraph Neighboring Relations Infomax for Inductive Link Prediction on  Knowledge Graphs",
        "abs": "Inductive link prediction for knowledge graph aims at predicting missing\nlinks between unseen entities, those not shown in training stage. Most previous\nworks learn entity-specific embeddings of entities, which cannot handle unseen\nentities. Recent several methods utilize enclosing subgraph to obtain inductive\nability. However, all these works only consider the enclosing part of subgraph\nwithout complete neighboring relations, which leads to the issue that partial\nneighboring relations are neglected, and sparse subgraphs are hard to be\nhandled. To address that, we propose Subgraph Neighboring Relations Infomax,\nSNRI, which sufficiently exploits complete neighboring relations from two\naspects: neighboring relational feature for node feature and neighboring\nrelational path for sparse subgraph. To further model neighboring relations in\na global way, we innovatively apply mutual information (MI) maximization for\nknowledge graph. Experiments show that SNRI outperforms existing state-of-art\nmethods by a large margin on inductive link prediction task, and verify the\neffectiveness of exploring complete neighboring relations in a global way to\ncharacterize node features and reason on sparse subgraphs.",
        "Task": "They address inductive link prediction (a link-level classification task) on knowledge graphs. Performance is evaluated on the WN18RR and FB15k-237 datasets.",
        "Data_type": "They operate on multi-relational knowledge graphs, where nodes represent entities and edges denote different relations. The approach explicitly handles unseen nodes over time, focusing on inductive link prediction within these graph-structured data.",
        "Data_domain": "These experiments use knowledge graph datasets WN18RR (derived from WordNet) and FB15k-237 (derived from Freebase). Hence, the data domain is general knowledge and lexical knowledge graphs.",
        "ML_phase": "They first extract enclosing subgraphs around each target triple, preserving complete neighboring relations and initializing node features by combining positional labels with attentively aggregated relational embeddings before feeding them into a GNN. The model is then trained with a margin-based supervised objective and a self-supervised mutual information mechanism to inductively predict missing links."
    },
    "LinkPrediction/1812.04206v2.json": {
        "title": "GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction",
        "abs": "Dynamic link prediction is a research hot in complex networks area,\nespecially for its wide applications in biology, social network, economy and\nindustry. Compared with static link prediction, dynamic one is much more\ndifficult since network structure evolves over time. Currently most researches\nfocus on static link prediction which cannot achieve expected performance in\ndynamic network. Aiming at low AUC, high Error Rate, add/remove link prediction\ndifficulty, we propose GC-LSTM, a Graph Convolution Network (GC) embedded Long\nShort Term Memory network (LTSM), for end-to-end dynamic link prediction. To\nthe best of our knowledge, it is the first time that GCN embedded LSTM is put\nforward for link prediction of dynamic networks. GCN in this new deep model is\ncapable of node structure learning of network snapshot for each time slide,\nwhile LSTM is responsible for temporal feature learning for network snapshot.\nBesides, current dynamic link prediction method can only handle removed links,\nGC-LSTM can predict both added or removed link at the same time. Extensive\nexperiments are carried out to testify its performance in aspects of prediction\naccuracy, Error Rate, add/remove link prediction and key link prediction. The\nresults prove that GC-LSTM outperforms current state-of-art method.",
        "Task": "No machine learning task is explicitly described in the provided information. As a result, it is not clear whether the paper addresses classification, regression, segmentation, or any other specific ML task.",
        "Data_type": "No information regarding specific data types or modalities is provided in the given text. Consequently, it is unclear whether the paper addresses any particular forms of graph, text, image, or other data.",
        "Data_domain": "The data domain is not explicitly stated in the provided text. Consequently, no information is available to determine the specific domain from which the data originate.",
        "ML_phase": "No explicit information is provided regarding data preprocessing, model architecture, or training steps. Therefore, the paper’s ML pipeline phase remains unspecified based on the given text."
    },
    "LinkPrediction/2106.06935v4.json": {
        "title": "Neural Bellman-Ford Networks: A General Graph Neural Network Framework  for Link Prediction",
        "abs": "Link prediction is a very fundamental task on graphs. Inspired by traditional\npath-based methods, in this paper we propose a general and flexible\nrepresentation learning framework based on paths for link prediction.\nSpecifically, we define the representation of a pair of nodes as the\ngeneralized sum of all path representations, with each path representation as\nthe generalized product of the edge representations in the path. Motivated by\nthe Bellman-Ford algorithm for solving the shortest path problem, we show that\nthe proposed path formulation can be efficiently solved by the generalized\nBellman-Ford algorithm. To further improve the capacity of the path\nformulation, we propose the Neural Bellman-Ford Network (NBFNet), a general\ngraph neural network framework that solves the path formulation with learned\noperators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes\nthe generalized Bellman-Ford algorithm with 3 neural components, namely\nINDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary\ncondition, multiplication operator, and summation operator respectively. The\nNBFNet is very general, covers many traditional path-based methods, and can be\napplied to both homogeneous graphs and multi-relational graphs (e.g., knowledge\ngraphs) in both transductive and inductive settings. Experiments on both\nhomogeneous graphs and knowledge graphs show that the proposed NBFNet\noutperforms existing methods by a large margin in both transductive and\ninductive settings, achieving new state-of-the-art results.",
        "Task": "They address a link-level prediction task (link prediction) for both knowledge graphs (e.g., FB15k-237, WN18RR) and homogeneous graphs (e.g., Cora, Citeseer, PubMed). They evaluate the model on transductive and inductive variants of these datasets to measure link prediction performance.",
        "Data_type": "They handle graph-structured data, specifically both homogeneous (single-relational) and multi-relational (knowledge) graphs, where nodes and edges can be represented without relying on node features. No multi-modal (e.g., image-text) or chemical representations (e.g., SMILES/fingerprints) are discussed.",
        "Data_domain": "They use knowledge graph datasets (e.g., FB15k-237 and WN18RR) from sources such as Freebase and WordNet, plus large-scale or biomedical KGs (e.g., ogbl-biokg, WikiKG90M) and academic citation networks (Cora, CiteSeer, PubMed). These domains capture real-world relational structures spanning encyclopedic knowledge bases and scholarly publications.",
        "ML_phase": "They construct a path-based link prediction framework by parameterizing the Bellman-Ford algorithm with neural functions (Indicator, Message, Aggregate) and iteratively passing messages across graph edges. The model is trained end-to-end with negative sampling and augmented edges (e.g., inverses or self-loops), using a small fixed number of layers to propagate source-conditioned representations."
    },
    "LinkPrediction/2411.07482v3.json": {
        "title": "Enhancing Link Prediction with Fuzzy Graph Attention Networks and  Dynamic Negative Sampling",
        "abs": "Link prediction is crucial for understanding complex networks but traditional\nGraph Neural Networks (GNNs) often rely on random negative sampling, leading to\nsuboptimal performance. This paper introduces Fuzzy Graph Attention Networks\n(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative\nsampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)\nsystematically selects high-quality negative edges based on fuzzy similarities,\nimproving training efficiency. FGAT layer incorporates fuzzy rough set\nprinciples, enabling robust and discriminative node representations.\nExperiments on two research collaboration networks demonstrate FGAT's superior\nlink prediction accuracy, outperforming state-of-the-art baselines by\nleveraging the power of fuzzy rough sets for effective negative sampling and\nnode feature learning.",
        "Task": "This paper addresses a link prediction task in directed graphs, evaluated on two real-world research collaboration network datasets. Performance is assessed using metrics such as Precision, Recall, F1, and ROC.",
        "Data_type": "They use directed, homogeneous graph-structured data (research collaboration networks) defined by node-edge adjacency. Each node is represented in a 128-dimensional feature space, with no textual or other multi-modal inputs considered.",
        "Data_domain": "The datasets are research collaboration networks in the academic domain. They consist of directed edges representing co-authorship relationships among researchers.",
        "ML_phase": "They introduce a fuzzy rough sets-based negative sampling mechanism (FNS) that dynamically selects negative edges during training and merge these sampled edges with positive edges to form the training set. Subsequently, the multi-layer FGAT architecture aggregates neighborhood information with attention and normalization, completing the end-to-end training pipeline for link prediction."
    },
    "LinkPrediction/2209.15486v3.json": {
        "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
        "abs": "Many Graph Neural Networks (GNNs) perform poorly compared to simple\nheuristics on Link Prediction (LP) tasks. This is due to limitations in\nexpressive power such as the inability to count triangles (the backbone of most\nLP heuristics) and because they can not distinguish automorphic nodes (those\nhaving identical structural roles). Both expressiveness issues can be\nalleviated by learning link (rather than node) representations and\nincorporating structural features such as triangle counts. Since explicit link\nrepresentations are often prohibitively expensive, recent works resorted to\nsubgraph-based methods, which have achieved state-of-the-art performance for\nLP, but suffer from poor efficiency due to high levels of redundancy between\nsubgraphs. We analyze the components of subgraph GNN (SGNN) methods for link\nprediction. Based on our analysis, we propose a novel full-graph GNN called\nELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as\nmessages to approximate the key components of SGNNs without explicit subgraph\nconstruction. ELPH is provably more expressive than Message Passing GNNs\n(MPNNs). It outperforms existing SGNN models on many standard LP benchmarks\nwhile being orders of magnitude faster. However, it shares the common GNN\nlimitation that it is only efficient when the dataset fits in GPU memory.\nAccordingly, we develop a highly scalable model, called BUDDY, which uses\nfeature precomputation to circumvent this limitation without sacrificing\npredictive performance. Our experiments show that BUDDY also outperforms SGNNs\non standard LP benchmarks while being highly scalable and faster than ELPH.",
        "Task": "They address a link‐prediction task on various graph datasets, seeking to predict whether an edge (link) should exist between two nodes. Performance is measured on standard benchmarks (e.g., citation networks and OGB datasets) using metrics such as hit rate and mean reciprocal rank.",
        "Data_type": "They focus on homogeneous graph-structured data, where each node has d-dimensional features (e.g., numeric or categorical) and edges are undirected. The method applies to large-scale single-relational graphs using node/edge information for link prediction.",
        "Data_domain": "They conduct experiments on multiple graph datasets spanning academic citation networks (Cora, Citeseer, Pubmed, Citation2) and collaboration networks (Collab), as well as biological/biomedical graphs (protein-protein associations and drug-drug interactions). Thus, the data domain primarily includes academic and bio/medical settings.",
        "ML_phase": "They introduce a subgraph-free link prediction pipeline that first preprocesses data by computing node-level sketches (via MinHash and HyperLogLog) and optionally diffusing node features globally. Then, a model (ELPH or BUDDY) uses these preprocessed representations in an MLP-like training framework, eliminating costly subgraph extraction while enabling efficient inference."
    },
    "LinkPrediction/2406.07979v2.json": {
        "title": "Heuristic Learning with Graph Neural Networks: A Unified Framework for  Link Prediction",
        "abs": "Link prediction is a fundamental task in graph learning, inherently shaped by\nthe topology of the graph. While traditional heuristics are grounded in graph\ntopology, they encounter challenges in generalizing across diverse graphs.\nRecent research efforts have aimed to leverage the potential of heuristics, yet\na unified formulation accommodating both local and global heuristics remains\nundiscovered. Drawing insights from the fact that both local and global\nheuristics can be represented by adjacency matrix multiplications, we propose a\nunified matrix formulation to accommodate and generalize various heuristics. We\nfurther propose the Heuristic Learning Graph Neural Network (HL-GNN) to\nefficiently implement the formulation. HL-GNN adopts intra-layer propagation\nand inter-layer connections, allowing it to reach a depth of around 20 layers\nwith lower time complexity than GCN. Extensive experiments on the Planetoid,\nAmazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.\nIt outperforms existing methods by a large margin in prediction performance.\nAdditionally, HL-GNN is several orders of magnitude faster than\nheuristic-inspired methods while requiring only a few trainable parameters. The\ncase study further demonstrates that the generalized heuristics and learned\nweights are highly interpretable.",
        "Task": "They tackle a link prediction task on diverse graph datasets (Planetoid, Amazon, and OGB) to evaluate performance. Their approach integrates local and global topological information for predicting the likelihood of edges forming between nodes.",
        "Data_type": "They focus on homogeneous, undirected graphs with node-level features (often textual or numeric) and can scale to millions of nodes. The method does not address multi-modal data (e.g., image-text) or specialized molecular formats (SMILES/fingerprints) and only handles single-relational edges.",
        "Data_domain": "These datasets encompass academic citation networks (Cora, Citeseer, Pubmed, ogbl-citation2, ogbl-collab), e-commerce co-purchase networks (Photo, Computers), and biological or medical networks (ogbl-ddi, ogbl-ppa). Such variety indicates the paper’s focus on link prediction across scholarly, commercial, and biochemical domains.",
        "ML_phase": "They propose HL-GNN as a global GNN architecture that replaces traditional transformation and activation steps with propagation-centric layers, enabling up to 20-layer deep message passing. Training is performed end-to-end by integrating multi-range topological information (via learnable weights) with an AUC-based loss for link prediction."
    },
    "LinkPrediction/2010.10046v1.json": {
        "title": "Line Graph Neural Networks for Link Prediction",
        "abs": "We consider the graph link prediction task, which is a classic graph\nanalytical problem with many real-world applications. With the advances of deep\nlearning, current link prediction methods commonly compute features from\nsubgraphs centered at two neighboring nodes and use the features to predict the\nlabel of the link between these two nodes. In this formalism, a link prediction\nproblem is converted to a graph classification task. In order to extract\nfixed-size features for classification, graph pooling layers are necessary in\nthe deep learning model, thereby incurring information loss. To overcome this\nkey limitation, we propose to seek a radically different and novel path by\nmaking use of the line graphs in graph theory. In particular, each node in a\nline graph corresponds to a unique edge in the original graph. Therefore, link\nprediction problems in the original graph can be equivalently solved as a node\nclassification problem in its corresponding line graph, instead of a graph\nclassification task. Experimental results on fourteen datasets from different\napplications demonstrate that our proposed method consistently outperforms the\nstate-of-the-art methods, while it has fewer parameters and high training\nefficiency.",
        "Task": "They address a link prediction task, which is a link-level classification problem in graphs. They conduct experiments on 14 real-world datasets from multiple domains to evaluate the performance.",
        "Data_type": "They address single-relational (homogeneous), undirected graphs that may include textual or numeric node attributes. Their method applies to both plain and attributed graph data for link prediction.",
        "Data_domain": "These experiments explore graph-structured data drawn from six diverse real-world domains, encompassing political blogs, biology, transportation, co-authorship, email networks, and power networks. By covering multiple domains, the paper demonstrates the method’s applicability to various real-world link prediction scenarios.",
        "ML_phase": "They extract an h-hop enclosing subgraph around target nodes, apply a labeling function to encode structural roles, and transform the subgraph into a line graph. A GNN-based training framework then learns edge features from this line graph and predicts link existence via a binary classification layer."
    },
    "LinkPrediction/2401.13227v3.json": {
        "title": "LPNL: Scalable Link Prediction with Large Language Models",
        "abs": "Exploring the application of large language models (LLMs) to graph learning\nis a emerging endeavor. However, the vast amount of information inherent in\nlarge graphs poses significant challenges to this process. This work focuses on\nthe link prediction task and introduces $\\textbf{LPNL}$ (Link Prediction via\nNatural Language), a framework based on large language models designed for\nscalable link prediction on large-scale heterogeneous graphs. We design novel\nprompts for link prediction that articulate graph details in natural language.\nWe propose a two-stage sampling pipeline to extract crucial information from\nthe graphs, and a divide-and-conquer strategy to control the input tokens\nwithin predefined limits, addressing the challenge of overwhelming information.\nWe fine-tune a T5 model based on our self-supervised learning designed for link\nprediction. Extensive experimental results demonstrate that LPNL outperforms\nmultiple advanced baselines in link prediction tasks on large-scale graphs.",
        "Task": "They address a link prediction task on large-scale heterogeneous graphs, specifically focusing on author name disambiguation. Experiments are conducted on the Open Academic Graph (OAG) dataset to evaluate performance.",
        "Data_type": "They handle graph-structured data with heterogeneous node and edge types, where each node possesses textual attributes (e.g., papers, authors, fields). Specifically, they address large-scale heterogeneous graphs containing multiple node categories and relationships, leveraging textual node attributes for link prediction.",
        "Data_domain": "The data domain is academic, specifically sourced from the Open Academic Graph (OAG). This large-scale heterogeneous graph contains multiple node types (papers, authors, venues, institutes, and fields) reflecting scholarly relationships.",
        "ML_phase": "They transform graph data into textual prompts using a two-stage sampling (normalized degree-based subgraph sampling and personalized PageRank-based ranking) and then apply a divide-and-conquer strategy for managing large candidate sets. The final pipeline fine-tunes a T5-based LLM in a self-supervised manner to perform link prediction end-to-end."
    },
    "LinkPrediction/2410.04013v1.json": {
        "title": "Improving Temporal Link Prediction via Temporal Walk Matrix Projection",
        "abs": "Temporal link prediction, aiming at predicting future interactions among\nentities based on historical interactions, is crucial for a series of\nreal-world applications. Although previous methods have demonstrated the\nimportance of relative encodings for effective temporal link prediction,\ncomputational efficiency remains a major concern in constructing these\nencodings. Moreover, existing relative encodings are usually constructed based\non structural connectivity, where temporal information is seldom considered. To\naddress the aforementioned issues, we first analyze existing relative encodings\nand unify them as a function of temporal walk matrices. This unification\nestablishes a connection between relative encodings and temporal walk matrices,\nproviding a more principled way for analyzing and designing relative encodings.\nBased on this analysis, we propose a new temporal graph neural network called\nTPNet, which introduces a temporal walk matrix that incorporates the time decay\neffect to simultaneously consider both temporal and structural information.\nMoreover, TPNet designs a random feature propagation mechanism with theoretical\nguarantees to implicitly maintain the temporal walk matrices, which improves\nthe computation and storage efficiency. Experimental results on 13 benchmark\ndatasets verify the effectiveness and efficiency of TPNet, where TPNet\noutperforms other baselines on most datasets and achieves a maximum speedup of\n$33.3 \\times$ compared to the SOTA baseline. Our code can be found at\n\\url{https://github.com/lxd99/TPNet}.",
        "Task": "They aim to perform temporal link prediction, i.e., predicting whether an interaction (link) will occur at a future time, which constitutes a link-level classification task on temporal graphs. The method is validated on 13 benchmark datasets spanning domains such as social networks, communication networks, and transportation systems.",
        "Data_type": "They work with continuous-time dynamic graphs containing node features in ℝ^(d_N) and link features in ℝ^(d_E), where each interaction has a timestamp. The method supports large-scale temporal graph data and can incorporate various numeric or textual attributes on nodes and edges.",
        "Data_domain": "They use 13 benchmark datasets drawn from diverse domains, including social networks (Reddit, Wikipedia, Social Evo, UCI), academic interactions (MOOC), music platforms (LastFM), corporate emails (Enron), political voting or legislative data (Canadian Parliament, US Legislature, UN Vote), economic/trade records (UN Trade), transportation (Flights), and proximity contacts (Contact). These datasets thus span social, economic, academic, political, and other domains.",
        "ML_phase": "They maintain node representations through a random-feature propagation mechanism that incrementally updates embeddings upon each new interaction, thereby encoding temporal-walk information. These embeddings are then combined via a pairwise feature decoder and an MLP-based predictor to train on future link likelihood in a end-to-end framework with negative sampling."
    },
    "PEFT/2205.05638.json": {
        "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning",
        "abs": "Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)$^3$ that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.",
        "Task": "They address textual classification tasks, including both direct class prediction and multiple-choice scenarios, using few-shot datasets (e.g., ANLI, COPA, RAFT) for evaluation. All listed benchmarks are treated as classification tasks with label strings or answer choices.",
        "Data_type": "They handle purely textual data for language-based tasks (e.g., classification and multiple-choice), with no indication of multimodal or graph-structured inputs. There is no mention of handling images, SMILES/fingerprints, or node/edge attributes.",
        "Data_domain": "The data primarily originates from academic NLP benchmarks covering classification, multiple-choice, and text-to-text tasks (e.g., ANLI, WiC, RTE). Additionally, it includes real-world data from the RAFT benchmark, which focuses on “economically valuable” tasks.",
        "ML_phase": "T-Few fine-tunes a T0 model on prompted text-to-text tasks by adding a small number of learned vectors (Infused Adapters) into the transformer’s activations, alongside unlikelihood and length-normalized loss terms. The framework uses a fixed set of hyperparameters (e.g., a 1,000-step Adafactor schedule) and can process mixed-task batches for efficient few-shot learning."
    },
    "PEFT/2407.05000.json": {
        "title": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation",
        "abs": "Fine-tuning large-scale pretrained models is prohibitively expensive in terms\nof computational and memory costs. LoRA, as one of the most popular\nParameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective\nalternative by fine-tuning an auxiliary low-rank model that has significantly\nfewer parameters. Although LoRA reduces the computational and memory\nrequirements significantly at each iteration, extensive empirical evidence\nindicates that it converges at a considerably slower rate compared to full\nfine-tuning, ultimately leading to increased overall compute and often worse\ntest performance. In our paper, we perform an in-depth investigation of the\ninitialization method of LoRA and show that careful initialization (without any\nchange of the architecture and the training algorithm) can significantly\nenhance both efficiency and performance. In particular, we introduce a novel\ninitialization method, LoRA-GA (Low Rank Adaptation with Gradient\nApproximation), which aligns the gradients of low-rank matrix product with\nthose of full fine-tuning at the first step. Our extensive experiments\ndemonstrate that LoRA-GA achieves a convergence rate comparable to that of full\nfine-tuning (hence being significantly faster than vanilla LoRA as well as\nvarious recent improvements) while simultaneously attaining comparable or even\nbetter performance. For example, on the subset of the GLUE dataset with\nT5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as\nLlama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05%\non MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up\nto 2-4 times convergence speed improvement compared to vanilla LoRA, validating\nits effectiveness in accelerating convergence and enhancing model performance.\nCode is available at https://github.com/Outsider565/LoRA-GA.",
        "Task": "They address classification tasks on benchmark datasets (e.g., a subset of GLUE) and generative tasks involving multi-turn dialogue, mathematical reasoning, and code generation. Performance is evaluated across these distinct domains using established evaluation sets.",
        "Data_type": "They focus on textual data, including natural language understanding, dialogue, mathematical reasoning, and code tasks. No other data modalities (e.g., images, graphs, or molecular fingerprints) are discussed.",
        "Data_domain": "They use text-centric datasets spanning multiple natural language domains (e.g., the GLUE benchmark for general language understanding, WizardLM for dialogue, GSM8K for math reasoning, and HumanEval for code). Hence, the data largely come from open-source academic benchmarks, instruction-tuned corpora, and coding/problem-solving repositories.",
        "ML_phase": "They apply their initialization technique at the training stage of the ML pipeline by intercepting the backward pass and performing SVD on sampled gradients to shape LoRA’s adapter weights. This approach minimally changes data processing (standard datasets, typical preprocessing) and does not alter the model architecture, but instead focuses on a parameter-efficient fine-tuning framework that hooks into PyTorch’s gradient flow."
    },
    "PEFT/2406.09044.json": {
        "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning",
        "abs": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.",
        "Task": "They address multiple-choice classification tasks for commonsense and math reasoning (e.g., BoolQ, PIQA, GSM8K, MATH) and evaluate instruction-following performance using benchmarks like AlpacaEval 2.0, FollowBench, and IFEval. They also conduct classification-style evaluations in a multimodal (vision-language) setting on datasets such as VQAv2, GQA, and VizWiz.",
        "Data_type": "They finetune on textual data for commonsense, math, and instruction tasks, as well as on image-text data for visual instruction tasks. They do not handle graph-structured data or SMILES/fingerprints.",
        "Data_domain": "They employ a broad range of NLP and multimodal benchmarks derived from academic and publicly available sources, encompassing commonsense reasoning datasets (e.g., WinoGrande, ARC), math reasoning (e.g., GSM8K, MATH), instruction following corpora (e.g., Ultrafeedback), and visual instruction data (e.g., LLaVA1.5 tasks). This collection spans standard academic test sets and curated large-scale training data for performance evaluation.",
        "ML_phase": "Positioned in the fine-tuning phase of large language models, the proposed method modifies the LoRA framework by initializing and updating only the minor singular components of pretrained weight matrices while freezing the principal components. This approach thus reduces resource demands and preserves core pretrained knowledge during training."
    },
    "PEFT/2402.12354.json": {
        "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
        "abs": "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.",
        "Task": "They evaluate performance on text classification tasks from the GLUE benchmark (MNLI, QQP, QNLI, SST2), multi-choice QA from MMLU, and instruction tuning on flan-v2. Additionally, a toy synthetic dataset is used for a simple regression setting.",
        "Data_type": "They handle large-scale text datasets (e.g., GLUE, MMLU, flan-v2) for language tasks. No multi-modal inputs, graph data, or SMILES/fingerprint formats are addressed.",
        "Data_domain": "The data comes from standard NLP corpora, covering well-known academic benchmarks such as GLUE (MNLI, QQP, QNLI, SST2), flan-v2 for instruction tuning, and MMLU. These datasets encompass various language understanding and generation tasks in academic settings.",
        "ML_phase": "They insert low-rank adapter modules into pretrained networks, freeze the original weights, and only train the newly added matrices. Downstream data is prepared in the usual way (e.g., tokenization), then fed into this modified architecture, which is optimized via standard frameworks (e.g., Adam) with carefully tailored learning-rate scaling for the adapters."
    },
    "PEFT/2301.01821.json": {
        "title": "Parameter-Efficient Fine-Tuning Design Spaces",
        "abs": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
        "Task": "They address parameter-efficient fine-tuning for multiple NLP tasks, including classification (GLUE, SuperGLUE), summarization (XSum), and machine translation (WMT16 en-ro). The paper reports performance using standard metrics (e.g., accuracy, correlation, ROUGE, BLEU) for these respective tasks.",
        "Data_type": "They work exclusively with unstructured textual data (e.g., GLUE classification tasks, XSum summarization, WMT translation), without involving other modalities or graph-structured inputs. No image-text multi-modal or chemical structural data is considered.",
        "Data_domain": "The paper focuses on natural language data from standard NLP tasks, including classification (GLUE, SuperGLUE), summarization (XSum), and machine translation (WMT). These benchmarks represent diverse linguistic domains such as paraphrase detection, textual entailment, question answering, and sentence acceptability.",
        "ML_phase": "They introduce a training framework for parameter-efficient fine-tuning of large pretrained language models, focusing on how to organize layer groups, allocate trainable parameters, decide which layers to tune, and assign fine-tuning strategies during the model adaptation phase. Through comparing randomly sampled designs under progressively refined constraints, they discover design patterns that consistently improve performance across various NLP tasks."
    },
    "HeterogeneousGraph/2312.11947v1.json": {
        "title": "Emotion Rendering for Conversational Speech Synthesis with Heterogeneous\n  Graph-Based Context Modeling",
        "abs": "Conversational Speech Synthesis (CSS) aims to accurately express an utterance\nwith the appropriate prosody and emotional inflection within a conversational\nsetting. While recognising the significance of CSS task, the prior studies have\nnot thoroughly investigated the emotional expressiveness problems due to the\nscarcity of emotional conversational datasets and the difficulty of stateful\nemotion modeling. In this paper, we propose a novel emotional CSS model, termed\nECSS, that includes two main components: 1) to enhance emotion understanding,\nwe introduce a heterogeneous graph-based emotional context modeling mechanism,\nwhich takes the multi-source dialogue history as input to model the dialogue\ncontext and learn the emotion cues from the context; 2) to achieve emotion\nrendering, we employ a contrastive learning-based emotion renderer module to\ninfer the accurate emotion style for the target utterance. To address the issue\nof data scarcity, we meticulously create emotional labels in terms of category\nand intensity, and annotate additional emotional information on the existing\nconversational dataset (DailyTalk). Both objective and subjective evaluations\nsuggest that our model outperforms the baseline models in understanding and\nrendering emotions. These evaluations also underscore the importance of\ncomprehensive emotional annotations. Code and audio samples can be found at:\nhttps://github.com/walker-hyf/ECSS.",
        "Task": "They propose an emotional conversational speech synthesis (generative) task, which aims to produce emotionally appropriate utterances conditioned on contextual dialogue history. To evaluate performance, they utilize the DailyTalk dataset, enriched with newly annotated emotion and emotion intensity labels.",
        "Data_type": "They employ multi-modal conversational data with five node types—text, audio, speaker, emotion, and emotion intensity—to construct a heterogeneous graph. Each node type captures distinct attributes (e.g., linguistic content or acoustic features), while edges represent various context-driven relationships among these nodes.",
        "Data_domain": "The paper’s dataset is drawn from DailyTalk, a publicly available collection of recorded multi-turn open-domain conversational speech. It primarily represents everyday dialogues between two speakers, reflecting real-life conversational scenarios.",
        "ML_phase": "They introduce a full training pipeline that begins with annotated multi-modal conversational data (text, audio, speaker, emotion category, and intensity), then encodes it via a heterogeneous graph-based context module, and finally integrates the learned representations into a FastSpeech2-style acoustic decoder. The approach involves contrastive learning losses for accurate emotion rendering, combining data preprocessing/labeling, graph-based model architecture design, and an end-to-end training framework."
    },
    "HeterogeneousGraph/1903.07293v2.json": {
        "title": "Heterogeneous Graph Attention Network",
        "abs": "Graph neural network, as a powerful graph representation technique based on\ndeep learning, has shown superior performance and attracted considerable\nresearch interest. However, it has not been fully considered in graph neural\nnetwork for heterogeneous graph which contains different types of nodes and\nlinks. The heterogeneity and rich semantic information bring great challenges\nfor designing a graph neural network for heterogeneous graph. Recently, one of\nthe most exciting advancements in deep learning is the attention mechanism,\nwhose great potential has been well demonstrated in various areas. In this\npaper, we first propose a novel heterogeneous graph neural network based on the\nhierarchical attention, including node-level and semantic-level attentions.\nSpecifically, the node-level attention aims to learn the importance between a\nnode and its metapath based neighbors, while the semantic-level attention is\nable to learn the importance of different meta-paths. With the learned\nimportance from both node-level and semantic-level attention, the importance of\nnode and meta-path can be fully considered. Then the proposed model can\ngenerate node embedding by aggregating features from meta-path based neighbors\nin a hierarchical manner. Extensive experimental results on three real-world\nheterogeneous graphs not only show the superior performance of our proposed\nmodel over the state-of-the-arts, but also demonstrate its potentially good\ninterpretability for graph analysis.",
        "Task": "They conduct a node-level classification task on heterogeneous graphs and evaluate performance using datasets such as DBLP, ACM, and IMDB. The learned embeddings are also assessed via a clustering task to further demonstrate their effectiveness.",
        "Data_type": "They work with heterogeneous graph-structured data in which nodes may have textual attributes (e.g., bag-of-words features) and belong to multiple categories. The paper’s framework is designed to handle various node types with associated textual features and different edge relations in a unified graph representation.",
        "Data_domain": "They utilize heterogeneous graph datasets drawn from academic domains (DBLP, ACM) and an entertainment domain (IMDB). Specifically, DBLP and ACM represent citation or publication networks, while IMDB pertains to movies.",
        "ML_phase": "They construct a heterogeneous graph from real-world datasets, generate meta-path-based neighbors for each node, and project node features via type-specific transformation. The model architecture is a hierarchical attention network (node-level then semantic-level) trained end-to-end with a cross-entropy loss on labeled nodes."
    },
    "HeterogeneousGraph/2005.13183v3.json": {
        "title": "Interpretable and Efficient Heterogeneous Graph Convolutional Network",
        "abs": "Graph Convolutional Network (GCN) has achieved extraordinary success in\nlearning effective task-specific representations of nodes in graphs. However,\nregarding Heterogeneous Information Network (HIN), existing HIN-oriented GCN\nmethods still suffer from two deficiencies: (1) they cannot flexibly explore\nall possible meta-paths and extract the most useful ones for a target object,\nwhich hinders both effectiveness and interpretability; (2) they often need to\ngenerate intermediate meta-path based dense graphs, which leads to high\ncomputational complexity. To address the above issues, we propose an\ninterpretable and efficient Heterogeneous Graph Convolutional Network (ie-HGCN)\nto learn the representations of objects in HINs. It is designed as a\nhierarchical aggregation architecture, i.e., object-level aggregation first,\nfollowed by type-level aggregation. The novel architecture can automatically\nextract useful meta-paths for each object from all possible meta-paths (within\na length limit), which brings good model interpretability. It can also reduce\nthe computational cost by avoiding intermediate HIN transformation and\nneighborhood attention. We provide theoretical analysis about the proposed\nie-HGCN in terms of evaluating the usefulness of all possible meta-paths, its\nconnection to the spectral graph convolution on HINs, and its quasi-linear time\ncomplexity. Extensive experiments on three real network datasets demonstrate\nthe superiority of ie-HGCN over the state-of-the-art methods.",
        "Task": "They aim to perform node-level classification on heterogeneous information networks, evaluating performance on benchmark datasets such as DBLP, ACM, and IMDB. Specifically, they focus on labeling nodes (e.g., authors or papers) in these networks according to different categories (e.g., research areas).",
        "Data_type": "They address heterogeneous graph-structured data in which each node type can possess numeric or textual features, and edges denote relations among these diverse node types. Specifically, their approach supports multiple object types (e.g., authors, papers, terms) with node-level features (real or randomly generated vectors) and utilizes adjacency information to capture rich structural relationships.",
        "Data_domain": "They use three heterogeneous networks drawn from academic (ACM, DBLP) and movie-rating (IMDB) domains. Specifically, the data spans bibliographic records with publication/authorship information and user–movie interactions.",
        "ML_phase": "They construct an end-to-end pipeline by first preparing row-normalized adjacency matrices and random or real-valued input features for each node type, then stacking multiple heterogeneous graph convolution layers composed of projection, object-level aggregation, and type-level attention. Finally, these layer outputs are trained via a supervised cross-entropy objective using standard gradient-based optimization."
    },
    "HeterogeneousGraph/2312.01878v8.json": {
        "title": "HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot  Prompt Learning",
        "abs": "Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.",
        "Task": "No task information can be extracted from the provided text. The paper does not offer any details on its target ML objective or corresponding datasets.",
        "Data_type": "No information regarding data type was provided in the available text. Consequently, there is no indication of whether the paper focuses on graph-structured data, multi-modal inputs, or any other specific representation.",
        "Data_domain": "No data domain is specified in the provided paper information. The paper does not indicate the origin or field of the datasets used.",
        "ML_phase": "No information is provided regarding data processing steps, model architecture design, or any training frameworks. Consequently, the precise location of the described technique within the ML pipeline cannot be determined from the given text."
    },
    "HeterogeneousGraph/2002.01680v2.json": {
        "title": "MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph  Embedding",
        "abs": "A large number of real-world graphs or networks are inherently heterogeneous,\ninvolving a diversity of node types and relation types. Heterogeneous graph\nembedding is to embed rich structural and semantic information of a\nheterogeneous graph into low-dimensional node representations. Existing models\nusually define multiple metapaths in a heterogeneous graph to capture the\ncomposite relations and guide neighbor selection. However, these models either\nomit node content features, discard intermediate nodes along the metapath, or\nonly consider one metapath. To address these three limitations, we propose a\nnew model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the\nfinal performance. Specifically, MAGNN employs three major components, i.e.,\nthe node content transformation to encapsulate input node attributes, the\nintra-metapath aggregation to incorporate intermediate semantic nodes, and the\ninter-metapath aggregation to combine messages from multiple metapaths.\nExtensive experiments on three real-world heterogeneous graph datasets for node\nclassification, node clustering, and link prediction show that MAGNN achieves\nmore accurate prediction results than state-of-the-art baselines.",
        "Task": "They focus on three downstream tasks: node classification (evaluated on IMDb and DBLP), node clustering (IMDb and DBLP), and link prediction (Last.fm). Each dataset is selected to benchmark performance for its respective task.",
        "Data_type": "They handle heterogeneous graphs with multiple node/edge types, where each node type A has an attribute matrix X_A ∈ ℝ^(|V_A|×d_A). These attributes can include textual features of dimension d_A that are projected into a shared latent space before embedding.",
        "Data_domain": "They use heterogeneous graph data from three real-world sources: an online film database (IMDb), an academic bibliography platform (DBLP), and a music platform (Last.fm). The datasets encompass movie, academic, and music-related entities, reflecting diverse node and edge types from each domain.",
        "ML_phase": "They first project heterogeneous node features into a shared latent space, then perform hierarchical aggregation along metapath instances, and finally fuse multiple metapaths via attention-based modules. The model is trained under either semi-supervised (cross-entropy) or unsupervised (negative sampling) objectives, producing node embeddings for classification, clustering, or link prediction."
    },
    "HeterogeneousGraph/2208.06129v1.json": {
        "title": "Multiplex Heterogeneous Graph Convolutional Network",
        "abs": "Heterogeneous graph convolutional networks have gained great popularity in\ntackling various network analytical tasks on heterogeneous network data,\nranging from link prediction to node classification. However, most existing\nworks ignore the relation heterogeneity with multiplex network between\nmulti-typed nodes and different importance of relations in meta-paths for node\nembedding, which can hardly capture the heterogeneous structure signals across\ndifferent relations. To tackle this challenge, this work proposes a Multiplex\nHeterogeneous Graph Convolutional Network (MHGCN) for heterogeneous network\nembedding. Our MHGCN can automatically learn the useful heterogeneous meta-path\ninteractions of different lengths in multiplex heterogeneous networks through\nmulti-layer convolution aggregation. Additionally, we effectively integrate\nboth multi-relation structural signals and attribute semantics into the learned\nnode embeddings with both unsupervised and semi-supervised learning paradigms.\nExtensive experiments on five real-world datasets with various network\nanalytical tasks demonstrate the significant superiority of MHGCN against\nstate-of-the-art embedding baselines in terms of all evaluation metrics.",
        "Task": "They address link prediction and node classification on multiplex heterogeneous networks. Both tasks are evaluated using multiple real-world datasets (e.g., Alibaba, Amazon, IMDB, AMiner, DBLP) to assess predictive performance.",
        "Data_type": "They focus on attributed multiplex heterogeneous networks encompassing multi-type nodes, multiple edge relations, and textual node attributes. In particular, the data is node-centric with each node carrying a feature vector (e.g., text-based) while edges capture diverse relational links.",
        "Data_domain": "The experimental evaluations involve five real-world graph datasets from both e-commerce (Alibaba, Amazon) and academic (AMiner, DBLP) domains, as well as the entertainment domain (IMDB). These datasets encompass diverse entity types such as users, products, authors, papers, and movies to represent multiplex heterogeneous networks.",
        "ML_phase": "They first decouple the attributed multiplex heterogeneous network into sub-graphs for each edge type and aggregate them (with learnable weights) into a unified adjacency matrix. Then, they apply a multi-layer (simplified) GCN framework—under either unsupervised or semi-supervised settings—to iteratively capture meta-path interactions, optimize learnable parameters (including relation weights), and produce the final node embeddings."
    },
    "HeterogeneousGraph/2207.02547v3.json": {
        "title": "Simple and Efficient Heterogeneous Graph Neural Network",
        "abs": "Heterogeneous graph neural networks (HGNNs) have powerful capability to embed\nrich structural and semantic information of a heterogeneous graph into node\nrepresentations. Existing HGNNs inherit many mechanisms from graph neural\nnetworks (GNNs) over homogeneous graphs, especially the attention mechanism and\nthe multi-layer structure. These mechanisms bring excessive complexity, but\nseldom work studies whether they are really effective on heterogeneous graphs.\nThis paper conducts an in-depth and detailed study of these mechanisms and\nproposes Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To\neasily capture structural information, SeHGNN pre-computes the neighbor\naggregation using a light-weight mean aggregator, which reduces complexity by\nremoving overused neighbor attention and avoiding repeated neighbor aggregation\nin every training epoch. To better utilize semantic information, SeHGNN adopts\nthe single-layer structure with long metapaths to extend the receptive field,\nas well as a transformer-based semantic fusion module to fuse features from\ndifferent metapaths. As a result, SeHGNN exhibits the characteristics of simple\nnetwork structure, high prediction accuracy, and fast training speed. Extensive\nexperiments on five real-world heterogeneous graphs demonstrate the superiority\nof SeHGNN over the state-of-the-arts on both accuracy and training speed.",
        "Task": "They address a node-level classification task on heterogeneous graphs. Their experiments evaluate classification performance on multiple benchmark datasets, including DBLP, ACM, IMDB, Freebase, and ogbn-mag.",
        "Data_type": "They operate on heterogeneous graph-structured data with multiple node and edge types, where each node can have textual or indexing-based features. The approach focuses on capturing complex relational structures rather than integrating multi-modal inputs such as images or molecular fingerprints.",
        "Data_domain": "These heterogeneous graph datasets primarily originate from academic networks (DBLP, ACM, and the Microsoft Academic Graph in ogbn-mag), a film dataset (IMDB), and a large-scale knowledge base (Freebase). Consequently, they reflect mixed data domains encompassing scholarly publications, cinematic entities, and general knowledge entities.",
        "ML_phase": "SeHGNN’s ML pipeline begins with a one-time neighbor aggregation in preprocessing, followed by multi-layer MLP blocks for feature projection, and concludes with a transformer-based semantic fusion step. This framework avoids repeated neighbor aggregation during training and accelerates end-to-end optimization."
    },
    "HeterogeneousGraph/2302.11329v2.json": {
        "title": "HINormer: Representation Learning On Heterogeneous Information Networks  with Graph Transformer",
        "abs": "Recent studies have highlighted the limitations of message-passing based\ngraph neural networks (GNNs), e.g., limited model expressiveness,\nover-smoothing, over-squashing, etc. To alleviate these issues, Graph\nTransformers (GTs) have been proposed which work in the paradigm that allows\nmessage passing to a larger coverage even across the whole graph. Hinging on\nthe global range attention mechanism, GTs have shown a superpower for\nrepresentation learning on homogeneous graphs. However, the investigation of\nGTs on heterogeneous information networks (HINs) is still under-exploited. In\nparticular, on account of the existence of heterogeneity, HINs show distinct\ndata characteristics and thus require different treatment. To bridge this gap,\nin this paper we investigate the representation learning on HINs with Graph\nTransformer, and propose a novel model named HINormer, which capitalizes on a\nlarger-range aggregation mechanism for node representation learning. In\nparticular, assisted by two major modules, i.e., a local structure encoder and\na heterogeneous relation encoder, HINormer can capture both the structural and\nheterogeneous information of nodes on HINs for comprehensive node\nrepresentations. We conduct extensive experiments on four HIN benchmark\ndatasets, which demonstrate that our proposed model can outperform the\nstate-of-the-art.",
        "Task": "They aim to solve node-level classification tasks on heterogeneous information networks. The authors evaluate performance using four benchmark HIN datasets—DBLP, IMDB, AMiner, and Freebase—by predicting node labels in multi-class or multi-label settings.",
        "Data_type": "They primarily handle heterogeneous graph-structured data with multiple node and edge types, where node attributes include textual features. The datasets comprise real-world networks (e.g., bibliographic or movie-related) modeled as HINs with multi-typed nodes and corresponding attribute information.",
        "Data_domain": "They employ four heterogeneous graph datasets from academic citations, a movie rating platform, and a knowledge graph. Specifically, DBLP and AMiner (academic), IMDB (movie rating), and Freebase (knowledge graph) illustrate real-world networks in these domains.",
        "ML_phase": "They sample D-hop node contexts to form input sequences, apply a local structure encoder and a heterogeneous relation encoder to embed structural and semantic information, and then feed these representations into a Transformer-based network. The model is trained end-to-end for node classification using a supervised cross-entropy loss."
    },
    "HeterogeneousGraph/2208.09957v2.json": {
        "title": "Heterogeneous Graph Masked Autoencoders",
        "abs": "Generative self-supervised learning (SSL), especially masked autoencoders,\nhas become one of the most exciting learning paradigms and has shown great\npotential in handling graph data. However, real-world graphs are always\nheterogeneous, which poses three critical challenges that existing methods\nignore: 1) how to capture complex graph structure? 2) how to incorporate\nvarious node attributes? and 3) how to encode different node positions? In\nlight of this, we study the problem of generative SSL on heterogeneous graphs\nand propose HGMAE, a novel heterogeneous graph masked autoencoder model to\naddress these challenges. HGMAE captures comprehensive graph information via\ntwo innovative masking techniques and three unique training strategies. In\nparticular, we first develop metapath masking and adaptive attribute masking\nwith dynamic mask rate to enable effective and stable learning on heterogeneous\ngraphs. We then design several training strategies including metapath-based\nedge reconstruction to adopt complex structural information, target attribute\nrestoration to incorporate various node attributes, and positional feature\nprediction to encode node positional information. Extensive experiments\ndemonstrate that HGMAE outperforms both contrastive and generative\nstate-of-the-art baselines on several tasks across multiple datasets. Codes are\navailable at https://github.com/meettyj/HGMAE.",
        "Task": "They focus on node-level classification and clustering tasks for heterogeneous graphs. Experiments are conducted on DBLP, Freebase, ACM, and AMiner to assess model performance.",
        "Data_type": "They handle heterogeneous graph-structured data with multiple node and edge types (including metapaths) and varied node attributes (e.g., textual). The paper’s approach reconstructs these adjacency structures and attributes in self-supervised fashion, capturing both structural and content information on real-world heterogeneous graphs.",
        "Data_domain": "The paper primarily deals with heterogeneous graph data drawn from academic, social, biomedical, and food domains. In experiments, the authors focus on four real datasets—DBLP, Freebase, ACM, and AMiner—covering academic and knowledge-base contexts.",
        "ML_phase": "They construct metapath-based adjacency matrices, perform adaptive attribute masking (with dynamic rates, leaving unchanged, and replacing), and extract positional features before feeding the masked inputs into an encoder-decoder pipeline. The model is then trained via three generative objectives (edge reconstruction, attribute restoration, and positional feature prediction) with a combined loss to learn comprehensive embeddings."
    },
    "HeterogeneousGraph/2205.09753v2.json": {
        "title": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory  Prediction via Scene Encoding",
        "abs": "Encoding a driving scene into vector representations has been an essential\ntask for autonomous driving that can benefit downstream tasks e.g. trajectory\nprediction. The driving scene often involves heterogeneous elements such as the\ndifferent types of objects (agents, lanes, traffic signs) and the semantic\nrelations between objects are rich and diverse. Meanwhile, there also exist\nrelativity across elements, which means that the spatial relation is a relative\nconcept and need be encoded in a ego-centric manner instead of in a global\ncoordinate system. Based on these observations, we propose Heterogeneous\nDriving Graph Transformer (HDGT), a backbone modelling the driving scene as a\nheterogeneous graph with different types of nodes and edges. For heterogeneous\ngraph construction, we connect different types of nodes according to diverse\nsemantic relations. For spatial relation encoding, the coordinates of the node\nas well as its in-edges are in the local node-centric coordinate system. For\nthe aggregation module in the graph neural network (GNN), we adopt the\ntransformer structure in a hierarchical way to fit the heterogeneous nature of\ninputs. Experimental results show that HDGT achieves state-of-the-art\nperformance for the task of trajectory prediction, on INTERACTION Prediction\nChallenge and Waymo Open Motion Challenge.",
        "Task": "They present a multi-agent trajectory forecasting task for autonomous driving, treated as a regression problem where future coordinates of each agent are predicted. The approach is validated on large-scale motion forecasting benchmarks INTERACTION and Waymo Open Motion.",
        "Data_type": "HDGT handles heterogeneous graph-structured data with numeric attributes, where each node represents either an agent (positions, velocities, headings) or a map element (e.g., polylines/polygons for lanes, traffic lights, etc.), and edges encode semantic relations (distance, topology, etc.). All raw spatial features are transformed into local ego-centric coordinates for each node, forming multi-typed node/edge information within a unified graph.",
        "Data_domain": "The paper’s datasets are large-scale real-world traffic scenarios, recorded for autonomous driving applications. They primarily encompass agent (vehicle, pedestrian, cyclist) states and HD-map elements (lanes, road lines, traffic signs) from urban road networks.",
        "ML_phase": "They first construct a heterogeneous graph from raw sensor and HD-map data, transform all node features into local reference systems (via CNN or PointNet), then stack multiple Transformer-based layers to aggregate/update node and edge representations. The final output heads are trained with a multi-modal trajectory prediction loss that combines regression (via smooth L1) and classification (via cross-entropy)."
    },
    "HeterogeneousGraph/2003.01332v1.json": {
        "title": "Heterogeneous Graph Transformer",
        "abs": "Recent years have witnessed the emerging success of graph neural networks\n(GNNs) for modeling structured data. However, most GNNs are designed for\nhomogeneous graphs, in which all nodes and edges belong to the same types,\nmaking them infeasible to represent heterogeneous structures. In this paper, we\npresent the Heterogeneous Graph Transformer (HGT) architecture for modeling\nWeb-scale heterogeneous graphs. To model heterogeneity, we design node- and\nedge-type dependent parameters to characterize the heterogeneous attention over\neach edge, empowering HGT to maintain dedicated representations for different\ntypes of nodes and edges. To handle dynamic heterogeneous graphs, we introduce\nthe relative temporal encoding technique into HGT, which is able to capture the\ndynamic structural dependency with arbitrary durations. To handle Web-scale\ngraph data, we design the heterogeneous mini-batch graph sampling\nalgorithm---HGSampling---for efficient and scalable training. Extensive\nexperiments on the Open Academic Graph of 179 million nodes and 2 billion edges\nshow that the proposed HGT model consistently outperforms all the\nstate-of-the-art GNN baselines by 9%--21% on various downstream tasks.",
        "Task": "They address a node-level classification task on heterogeneous graphs. Evaluation is conducted against datasets designed to measure performance on this node classification objective.",
        "Data_type": "They address heterogeneous graph-structured data, where multiple node types (e.g., papers, authors) and relation types (e.g., paper-author links) are involved. The node attributes often include textual information, enabling multi-relational modeling in a heterogeneous network.",
        "Data_domain": "It primarily utilizes academic data drawn from scholarly citation networks (e.g., DBLP).",
        "ML_phase": "The approach begins by converting heterogeneous graph data into a structured input format and then uses a hierarchical attention mechanism to learn node embeddings. It is trained in a supervised manner, optimizing classification objectives on labeled nodes within a mini-batch training framework."
    },
    "AnomalyDetection/2207.07361v1.json": {
        "title": "Registration based Few-Shot Anomaly Detection",
        "abs": "This paper considers few-shot anomaly detection (FSAD), a practical yet\nunder-studied setting for anomaly detection (AD), where only a limited number\nof normal images are provided for each category at training. So far, existing\nFSAD studies follow the one-model-per-category learning paradigm used for\nstandard AD, and the inter-category commonality has not been explored. Inspired\nby how humans detect anomalies, i.e., comparing an image in question to normal\nimages, we here leverage registration, an image alignment task that is\ninherently generalizable across categories, as the proxy task, to train a\ncategory-agnostic anomaly detection model. During testing, the anomalies are\nidentified by comparing the registered features of the test image and its\ncorresponding support (normal) images. As far as we know, this is the first\nFSAD method that trains a single generalizable model and requires no\nre-training or parameter fine-tuning for new categories. Experimental results\nhave shown that the proposed method outperforms the state-of-the-art FSAD\nmethods by 3%-8% in AUC on the MVTec and MPDD benchmarks.",
        "Task": "They address few-shot anomaly detection in images, requiring only a limited number of normal samples per category. Performance is evaluated on MVTec AD and MPDD datasets for anomaly detection and localization tasks.",
        "Data_type": "They exclusively work with image data drawn from real-world industrial scenarios (MVTec AD and MPDD) containing high-resolution, pixel-level information of objects. No textual, graph-structured, or other multi-modal inputs are employed, focusing solely on visual inspection tasks.",
        "Data_domain": "The data is collected from industrial manufacturing settings, focusing on defect detection in products such as metal parts and consumer goods. Specifically, it comes from two standard benchmark datasets (MVTec AD and MPDD) that capture diverse real-world industrial anomalies.",
        "ML_phase": "They aggregate normal images from multiple categories and feed pairs into a Siamese network with spatial transformer modules to learn feature registration in a category-agnostic manner. At inference, the model estimates a normal feature distribution from a few support samples using a statistical estimator, requiring no additional fine-tuning for anomaly detection."
    },
    "AnomalyDetection/2403.05897v1.json": {
        "title": "RealNet: A Feature Selection Network with Realistic Synthetic Anomaly  for Anomaly Detection",
        "abs": "Self-supervised feature reconstruction methods have shown promising advances\nin industrial image anomaly detection and localization. Despite this progress,\nthese methods still face challenges in synthesizing realistic and diverse\nanomaly samples, as well as addressing the feature redundancy and pre-training\nbias of pre-trained feature. In this work, we introduce RealNet, a feature\nreconstruction network with realistic synthetic anomaly and adaptive feature\nselection. It is incorporated with three key innovations: First, we propose\nStrength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion\nprocess-based synthesis strategy capable of generating samples with varying\nanomaly strengths that mimic the distribution of real anomalous samples.\nSecond, we develop Anomaly-aware Features Selection (AFS), a method for\nselecting representative and discriminative pre-trained feature subsets to\nimprove anomaly detection performance while controlling computational costs.\nThird, we introduce Reconstruction Residuals Selection (RRS), a strategy that\nadaptively selects discriminative residuals for comprehensive identification of\nanomalous regions across multiple levels of granularity. We assess RealNet on\nfour benchmark datasets, and our results demonstrate significant improvements\nin both Image AUROC and Pixel AUROC compared to the current state-o-the-art\nmethods. The code, data, and models are available at\nhttps://github.com/cnulab/RealNet.",
        "Task": "They address image-level and pixel-level anomaly detection (classification and segmentation) in industrial images. The approach is evaluated on multiple benchmarks, including MVTec-AD, MPDD, BTAD, and VisA.",
        "Data_type": "They exclusively handle single-modal 2D image data, specifically industrial product images at resolutions such as 256×256. The approach uses both real normal images and synthetic anomalies for anomaly detection, with no mention of additional modalities or graph/text data.",
        "Data_domain": "The paper’s datasets come from industrial manufacturing settings, encompassing categories such as different metal products, objects, and textures used in quality inspection tasks. They include well-known industrial anomaly detection benchmarks (MVTec-AD, MPDD, BTAD, and VisA), capturing real-world production images for defect identification.",
        "ML_phase": "They introduce a Strength-controllable Diffusion Anomaly Synthesis (SDAS) stage to generate anomalous samples from normal images, which then serve as training inputs for a self-supervised anomaly detection setup. In parallel, they employ multi-scale pre-trained CNN features filtered by Anomaly-aware Features Selection and then reconstructed with Reconstruction Residuals Selection, creating the RealNet architecture that is trained end-to-end for robust anomaly detection."
    },
    "AnomalyDetection/2303.00601v2.json": {
        "title": "Multimodal Industrial Anomaly Detection via Hybrid Fusion",
        "abs": "2D-based Industrial Anomaly Detection has been widely discussed, however,\nmultimodal industrial anomaly detection based on 3D point clouds and RGB images\nstill has many untouched fields. Existing multimodal industrial anomaly\ndetection methods directly concatenate the multimodal features, which leads to\na strong disturbance between features and harms the detection performance. In\nthis paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly\ndetection method with hybrid fusion scheme: firstly, we design an unsupervised\nfeature fusion with patch-wise contrastive learning to encourage the\ninteraction of different modal features; secondly, we use a decision layer\nfusion with multiple memory banks to avoid loss of information and additional\nnovelty classifiers to make the final decision. We further propose a point\nfeature alignment operation to better align the point cloud and RGB features.\nExtensive experiments show that our multimodal industrial anomaly detection\nmodel outperforms the state-of-the-art (SOTA) methods on both detection and\nsegmentation precision on MVTec-3D AD dataset. Code is available at\nhttps://github.com/nomewang/M3DM.",
        "Task": "They address unsupervised industrial anomaly detection (image-level classification) and defect segmentation. They evaluate their approach on the MVTec-3D AD dataset to measure performance.",
        "Data_type": "This work handles multi-modal data consisting of 2D RGB image inputs and 3D point cloud data (x, y, z coordinates). They focus on combining color features from images with shape information from point clouds to detect industrial anomalies.",
        "Data_domain": "The dataset is sourced from industrial inspection scenarios, containing 2D images and 3D point clouds of real-world manufactured products (e.g., cookies, potatoes). The primary focus is on detecting anomalies in these industrial items.",
        "ML_phase": "They first remove background points from the 3D input via RANSAC and resize both 3D and RGB data, then extract features using pretrained Transformers before fusing them via a contrastive-learning module and storing them in separate memory banks. Finally, they train an OCSVM-based Decision Layer Fusion on these memory banks for final anomaly detection and segmentation."
    },
    "AnomalyDetection/2201.10703v2.json": {
        "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
        "abs": "Knowledge distillation (KD) achieves promising results on the challenging\nproblem of unsupervised anomaly detection (AD).The representation discrepancy\nof anomalies in the teacher-student (T-S) model provides essential evidence for\nAD. However, using similar or identical architectures to build the teacher and\nstudent models in previous studies hinders the diversity of anomalous\nrepresentations. To tackle this problem, we propose a novel T-S model\nconsisting of a teacher encoder and a student decoder and introduce a simple\nyet effective \"reverse distillation\" paradigm accordingly. Instead of receiving\nraw images directly, the student network takes teacher model's one-class\nembedding as input and targets to restore the teacher's multiscale\nrepresentations. Inherently, knowledge distillation in this study starts from\nabstract, high-level presentations to low-level features. In addition, we\nintroduce a trainable one-class bottleneck embedding (OCBE) module in our T-S\nmodel. The obtained compact embedding effectively preserves essential\ninformation on normal patterns, but abandons anomaly perturbations. Extensive\nexperimentation on AD and one-class novelty detection benchmarks shows that our\nmethod surpasses SOTA performance, demonstrating our proposed approach's\neffectiveness and generalizability.",
        "Task": "They address unsupervised anomaly detection as an image-level classification task (normal vs. abnormal) and anomaly localization as a pixel-wise segmentation task (identifying regions of deviation). These tasks are quantitatively evaluated on public benchmarks, such as MVTec and other one-class novelty detection datasets.",
        "Data_type": "They train and evaluate on single-modal 2D image data (e.g., MVTec, MNIST, FashionMNIST, and CIFAR10). No graph-structured, textual, or multi-modal inputs are considered.",
        "Data_domain": "They utilize image data primarily from industrial inspection (e.g., MVTec), alongside standard vision benchmarks (e.g., MNIST, FashionMNIST, CIFAR10, Caltech-256) and references to medical out-of-distribution detection and video surveillance scenarios. Thus, the paper’s data domain is broadly image-based, covering both real-world industrial/medical applications and common academic datasets for anomaly and novelty detection.",
        "ML_phase": "They freeze a pre-trained teacher encoder, feed normal images to derive multi-scale features, and then apply a trainable one-class bottleneck embedding before passing these compact representations to a student decoder. The system is trained end-to-end via knowledge distillation, ensuring the student learns only normal patterns and exhibits large reconstruction discrepancy for anomalies."
    },
    "AnomalyDetection/2210.09693v2.json": {
        "title": "TFAD: A Decomposition Time Series Anomaly Detection Architecture with  Time-Frequency Analysis",
        "abs": "Time series anomaly detection is a challenging problem due to the complex\ntemporal dependencies and the limited label data. Although some algorithms\nincluding both traditional and deep models have been proposed, most of them\nmainly focus on time-domain modeling, and do not fully utilize the information\nin the frequency domain of the time series data. In this paper, we propose a\nTime-Frequency analysis based time series Anomaly Detection model, or TFAD for\nshort, to exploit both time and frequency domains for performance improvement.\nBesides, we incorporate time series decomposition and data augmentation\nmechanisms in the designed time-frequency architecture to further boost the\nabilities of performance and interpretability. Empirical studies on widely used\nbenchmark datasets show that our approach obtains state-of-the-art performance\nin univariate and multivariate time series anomaly detection tasks. Code is\nprovided at https://github.com/DAMO-DI-ML/CIKM22-TFAD.",
        "Task": "They address time-series anomaly detection, which is essentially a classification task labeling each point or subsequence as anomalous or normal. Its performance is evaluated using several univariate and multivariate time series benchmarks, including KPI, Yahoo, SMAP, and MSL.",
        "Data_type": "They operate on numeric time series data, covering both univariate and multivariate scenarios (e.g., KPI curves, telemetry channels). This work does not extend to graph-structured inputs, text/image modalities, or other representation formats beyond time series.",
        "Data_domain": "The paper focuses on time-series data primarily sourced from internet service KPIs (e.g., Yahoo traffic data, eBay, Sogou, Tencent) and NASA’s satellite telemetry, covering both univariate and multivariate real-world monitoring scenarios.",
        "ML_phase": "They implement a pipeline that first applies data augmentation (including both normal and anomaly-specific methods) and time-series decomposition, then uses window-splitting to feed both the time-domain and Fourier-transformed signals into a TCN-based module. Finally, the context and full windows are compared via a similarity measure to generate anomaly scores."
    },
    "AnomalyDetection/2303.15140v2.json": {
        "title": "SimpleNet: A Simple Network for Image Anomaly Detection and Localization",
        "abs": "We propose a simple and application-friendly network (called SimpleNet) for\ndetecting and localizing anomalies. SimpleNet consists of four components: (1)\na pre-trained Feature Extractor that generates local features, (2) a shallow\nFeature Adapter that transfers local features towards target domain, (3) a\nsimple Anomaly Feature Generator that counterfeits anomaly features by adding\nGaussian noise to normal features, and (4) a binary Anomaly Discriminator that\ndistinguishes anomaly features from normal features. During inference, the\nAnomaly Feature Generator would be discarded. Our approach is based on three\nintuitions. First, transforming pre-trained features to target-oriented\nfeatures helps avoid domain bias. Second, generating synthetic anomalies in\nfeature space is more effective, as defects may not have much commonality in\nthe image space. Third, a simple discriminator is much efficient and practical.\nIn spite of simplicity, SimpleNet outperforms previous methods quantitatively\nand qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly\ndetection AUROC of 99.6%, reducing the error by 55.5% compared to the next best\nperforming model. Furthermore, SimpleNet is faster than existing methods, with\na high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet\ndemonstrates significant improvements in performance on the One-Class Novelty\nDetection task. Code: https://github.com/DonaldRR/SimpleNet.",
        "Task": "They address unsupervised image anomaly detection and localization, which involves classifying and segmenting defective regions in industrial images. The authors also extend their approach to one-class novelty detection on CIFAR-10.",
        "Data_type": "They use 2D color images (e.g., 224×224 for MVTec AD and 32×32 for CIFAR-10) for anomaly detection tasks. No graph-structured, multi-modal, or textual data are involved.",
        "Data_domain": "The paper primarily uses industrial manufacturing images from the MVTec AD benchmark, which contain various textured and object categories relevant to production defects. Additionally, it incorporates the CIFAR-10 dataset for one-class novelty detection.",
        "ML_phase": "SimpleNet operates in the training and inference stages of an anomaly detection pipeline by extracting features from a pre-trained backbone, adapting them to the target domain, and synthesizing anomalous features via Gaussian noise addition. It then trains a simple MLP-based discriminator on normal and synthetic samples to perform anomaly detection."
    },
    "AnomalyDetection/2111.09099v6.json": {
        "title": "Self-Supervised Predictive Convolutional Attentive Block for Anomaly  Detection",
        "abs": "Anomaly detection is commonly pursued as a one-class classification problem,\nwhere models can only learn from normal training samples, while being evaluated\non both normal and abnormal test samples. Among the successful approaches for\nanomaly detection, a distinguished category of methods relies on predicting\nmasked information (e.g. patches, future frames, etc.) and leveraging the\nreconstruction error with respect to the masked information as an abnormality\nscore. Different from related methods, we propose to integrate the\nreconstruction-based functionality into a novel self-supervised predictive\narchitectural building block. The proposed self-supervised block is generic and\ncan easily be incorporated into various state-of-the-art anomaly detection\nmethods. Our block starts with a convolutional layer with dilated filters,\nwhere the center area of the receptive field is masked. The resulting\nactivation maps are passed through a channel attention module. Our block is\nequipped with a loss that minimizes the reconstruction error with respect to\nthe masked area in the receptive field. We demonstrate the generality of our\nblock by integrating it into several state-of-the-art frameworks for anomaly\ndetection on image and video, providing empirical evidence that shows\nconsiderable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We\nrelease our code as open source at https://github.com/ristea/sspcab.",
        "Task": "They address anomaly detection framed as a one-class classification task for both images and videos, where anomalies are recognized as outliers relative to normal training samples. Performance is evaluated through image-level, pixel-level (segmentation), and frame-level detection metrics on the MVTec AD dataset (industrial inspection images) and Avenue/ShanghaiTech datasets (video surveillance scenarios).",
        "Data_type": "They handle two-dimensional image data (MVTec AD) and video data (Avenue, ShanghaiTech) for anomaly detection tasks. No textual, graphical, or multi-modal inputs (e.g., SMILES/fingerprint) are addressed.",
        "Data_domain": "The data originate from industrial inspection images (MVTec AD) and surveillance videos captured for public security across Avenue and ShanghaiTech. These domains encompass manufacturing line defect detection and event monitoring (e.g., fights, irregular motion) in real-world scenes.",
        "ML_phase": "They incorporate a novel self-supervised predictive convolutional attentive block into existing neural architectures by replacing the penultimate convolutional layer, introducing a masked convolution and a channel attention module. During training, the block’s reconstruction objective is added to the underlying model’s loss, enabling it to learn to predict masked features in a self-supervised manner."
    },
    "AnomalyDetection/2303.14535v3.json": {
        "title": "EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level  Latencies",
        "abs": "Detecting anomalies in images is an important task, especially in real-time\ncomputer vision applications. In this work, we focus on computational\nefficiency and propose a lightweight feature extractor that processes an image\nin less than a millisecond on a modern GPU. We then use a student-teacher\napproach to detect anomalous features. We train a student network to predict\nthe extracted features of normal, i.e., anomaly-free training images. The\ndetection of anomalies at test time is enabled by the student failing to\npredict their features. We propose a training loss that hinders the student\nfrom imitating the teacher feature extractor beyond the normal images. It\nallows us to drastically reduce the computational cost of the student-teacher\nmodel, while improving the detection of anomalous features. We furthermore\naddress the detection of challenging logical anomalies that involve invalid\ncombinations of normal local features, for example, a wrong ordering of\nobjects. We detect these anomalies by efficiently incorporating an autoencoder\nthat analyzes images globally. We evaluate our method, called EfficientAD, on\n32 datasets from three industrial anomaly detection dataset collections.\nEfficientAD sets new standards for both the detection and the localization of\nanomalies. At a latency of two milliseconds and a throughput of six hundred\nimages per second, it enables a fast handling of anomalies. Together with its\nlow error rate, this makes it an economical solution for real-world\napplications and a fruitful basis for future research.",
        "Task": "They address image-level anomaly detection (i.e., classifying whether an image contains a defect) and pixel-level anomaly localization (i.e., segmenting the defective regions). They evaluate their method on industrial inspection datasets (MVTec AD, VisA, and MVTec LOCO) that include both structural and logical anomalies.",
        "Data_type": "They handle single-image inputs, specifically 2D RGB or grayscale industrial images of various sizes (commonly 256×256 or 512×512) for anomaly detection. Pixel-precise masks accompany these images to enable both image-level and pixel-level anomaly evaluation.",
        "Data_domain": "The data originates from industrial inspection scenarios, specifically using image-based datasets (MVTec AD, VisA, and MVTec LOCO) that capture various defects in manufactured products. These datasets contain only normal images during training and include both structural and logical anomalies in their testing.",
        "ML_phase": "They resize and normalize images to generate patch-level features from a distilled teacher network, then train a student model on normal data with a novel loss that selectively backpropagates high-error patches and penalizes generalization to out-of-distribution samples. Concurrently, an autoencoder is trained to reconstruct teacher features in order to detect logical anomalies, with final anomaly maps (local and global) combined and calibrated via quantile-based normalization."
    },
    "AnomalyDetection/2305.16713v3.json": {
        "title": "ReConPatch : Contrastive Patch Representation Learning for Industrial  Anomaly Detection",
        "abs": "Anomaly detection is crucial to the advanced identification of product\ndefects such as incorrect parts, misaligned components, and damages in\nindustrial manufacturing. Due to the rare observations and unknown types of\ndefects, anomaly detection is considered to be challenging in machine learning.\nTo overcome this difficulty, recent approaches utilize the common visual\nrepresentations pre-trained from natural image datasets and distill the\nrelevant features. However, existing approaches still have the discrepancy\nbetween the pre-trained feature and the target data, or require the input\naugmentation which should be carefully designed, particularly for the\nindustrial dataset. In this paper, we introduce ReConPatch, which constructs\ndiscriminative features for anomaly detection by training a linear modulation\nof patch features extracted from the pre-trained model. ReConPatch employs\ncontrastive representation learning to collect and distribute features in a way\nthat produces a target-oriented and easily separable representation. To address\nthe absence of labeled pairs for the contrastive learning, we utilize two\nsimilarity measures between data representations, pairwise and contextual\nsimilarities, as pseudo-labels. Our method achieves the state-of-the-art\nanomaly detection performance (99.72%) for the widely used and challenging\nMVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly\ndetection performance (95.8%) for the BTAD dataset.",
        "Task": "They address an unsupervised anomaly detection task on industrial images, evaluating both image-level (normal/abnormal) classification and pixel-level segmentation. They use the MVTec AD and BTAD datasets to measure performance.",
        "Data_type": "They handle 2D industrial RGB images for anomaly detection. Their experiments focus on datasets like MVTec AD and BTAD, which contain normal and defective product images.",
        "Data_domain": "They use industrial manufacturing image data, specifically from MVTec AD and BTAD, to capture normal and defective product scenarios. This setting centers on visual inspection of manufactured items under real-world industrial conditions.",
        "ML_phase": "They extract patch-level features from a pretrained backbone, apply a lightweight linear transformation (f layer) plus an optional projection layer (g), and then store representative features in a memory bank via coreset subsampling for distance-based anomaly scoring. Training uses a relaxed contrastive objective with pairwise and contextual similarities, and inference computes anomaly scores by comparing new patch embeddings against the memory bank."
    },
    "AnomalyDetection/2203.03962v1.json": {
        "title": "Generative Cooperative Learning for Unsupervised Video Anomaly Detection",
        "abs": "Video anomaly detection is well investigated in weakly-supervised and\none-class classification (OCC) settings. However, unsupervised video anomaly\ndetection methods are quite sparse, likely because anomalies are less frequent\nin occurrence and usually not well-defined, which when coupled with the absence\nof ground truth supervision, could adversely affect the performance of the\nlearning algorithms. This problem is challenging yet rewarding as it can\ncompletely eradicate the costs of obtaining laborious annotations and enable\nsuch systems to be deployed without human intervention. To this end, we propose\na novel unsupervised Generative Cooperative Learning (GCL) approach for video\nanomaly detection that exploits the low frequency of anomalies towards building\na cross-supervision between a generator and a discriminator. In essence, both\nnetworks get trained in a cooperative fashion, thereby allowing unsupervised\nlearning. We conduct extensive experiments on two large-scale video anomaly\ndetection datasets, UCF crime, and ShanghaiTech. Consistent improvement over\nthe existing state-of-the-art unsupervised and OCC methods corroborate the\neffectiveness of our approach.",
        "Task": "They tackle unsupervised anomaly detection in surveillance videos, treating it as a frame-level classification task (normal vs. anomalous). Performance is evaluated on the UCF-Crime and ShanghaiTech datasets.",
        "Data_type": "They work with large-scale surveillance video data, treating each video as a sequence of segments to capture spatiotemporal information. No other modalities (e.g., text, SMILES) are involved, as the focus remains on unlabeled videos containing both normal and anomalous events.",
        "Data_domain": "The paper’s data domain consists of real-world video surveillance footage (e.g., UCF-Crime, ShanghaiTech) recorded by CCTV cameras. These datasets thus focus on visual monitoring scenarios for anomaly detection.",
        "ML_phase": "They introduce an unsupervised training pipeline where an autoencoder (generator) and a binary classifier (discriminator) iteratively provide pseudo-labels for each other. This design includes a self-supervised pretraining step, pseudo-label thresholding, and negative learning to refine the reconstructed anomalies, culminating in final anomaly scores from the discriminator."
    },
    "AnomalyDetection/2206.06602v4.json": {
        "title": "Deep Isolation Forest for Anomaly Detection",
        "abs": "Isolation forest (iForest) has been emerging as arguably the most popular\nanomaly detector in recent years due to its general effectiveness across\ndifferent benchmarks and strong scalability. Nevertheless, its linear\naxis-parallel isolation method often leads to (i) failure in detecting hard\nanomalies that are difficult to isolate in\nhigh-dimensional/non-linear-separable data space, and (ii) notorious\nalgorithmic bias that assigns unexpectedly lower anomaly scores to artefact\nregions. These issues contribute to high false negative errors. Several iForest\nextensions are introduced, but they essentially still employ shallow, linear\ndata partition, restricting their power in isolating true anomalies. Therefore,\nthis paper proposes deep isolation forest. We introduce a new representation\nscheme that utilises casually initialised neural networks to map original data\ninto random representation ensembles, where random axis-parallel cuts are\nsubsequently applied to perform the data partition. This representation scheme\nfacilitates high freedom of the partition in the original data space\n(equivalent to non-linear partition on subspaces of varying sizes), encouraging\na unique synergy between random representations and random partition-based\nisolation. Extensive experiments show that our model achieves significant\nimprovement over state-of-the-art isolation-based methods and deep detectors on\ntabular, graph and time series datasets; our model also inherits desired\nscalability from iForest.",
        "Task": "They address an unsupervised anomaly detection task, employing multiple real-world datasets (tabular, graph, and time series) to evaluate detection performance. The goal is to identify unusual instances (outliers) in these diverse data domains.",
        "Data_type": "DIF can process tabular data, graph-structured data (with node-level attributes), and time-series data by simply substituting an appropriate randomly initialized network backbone for each domain. It thereby supports diverse data types without imposing specialized feature engineering or restrictive data-specific assumptions.",
        "Data_domain": "They employ tabular data from intrusion detection, finance, text domains, and ecology, time-series data from NASA spacecraft and ECG signals, and graph data from Tox21 on chemical compounds. These cover multiple real-world domains such as cybersecurity, finance, biomedical, and molecular toxicity analysis.",
        "ML_phase": "DIF produces random embeddings via casually initialized neural networks, then grows isolation trees on these projected subspaces to score anomalies based on path lengths and deviation degrees. The approach leverages a subsampling-based ensemble mechanism and a mini-batch parallelization (CERE) for efficient representation generation without requiring explicit network training."
    },
    "AnomalyDetection/2308.15300v1.json": {
        "title": "MSFlow: Multi-Scale Flow-based Framework for Unsupervised Anomaly  Detection",
        "abs": "Unsupervised anomaly detection (UAD) attracts a lot of research interest and\ndrives widespread applications, where only anomaly-free samples are available\nfor training. Some UAD applications intend to further locate the anomalous\nregions without any anomaly information.\n  Although the absence of anomalous samples and annotations deteriorates the\nUAD performance, an inconspicuous yet powerful statistics model, the\nnormalizing flows, is appropriate for anomaly detection and localization in an\nunsupervised fashion. The flow-based probabilistic models, only trained on\nanomaly-free data, can efficiently distinguish unpredictable anomalies by\nassigning them much lower likelihoods than normal data.\n  Nevertheless, the size variation of unpredictable anomalies introduces\nanother inconvenience to the flow-based methods for high-precision anomaly\ndetection and localization. To generalize the anomaly size variation, we\npropose a novel Multi-Scale Flow-based framework dubbed MSFlow composed of\nasymmetrical parallel flows followed by a fusion flow to exchange multi-scale\nperceptions. Moreover, different multi-scale aggregation strategies are adopted\nfor image-wise anomaly detection and pixel-wise anomaly localization according\nto the discrepancy between them. The proposed MSFlow is evaluated on three\nanomaly detection datasets, significantly outperforming existing methods.\nNotably, on the challenging MVTec AD benchmark, our MSFlow achieves a new\nstate-of-the-art with a detection AUORC score of up to 99.7%, localization\nAUCROC score of 98.8%, and PRO score of 97.1%. The reproducible code is\navailable at https://github.com/cool-xuan/msflow.",
        "Task": "They aim to perform unsupervised anomaly detection on industrial and surveillance images, addressing both image-level classification (defect vs. normal) and pixel-wise segmentation (localizing anomalies). The evaluation is conducted on MVTec AD, Magnetic Tile Defects, and mSTC datasets.",
        "Data_type": "They employ 2D image data in both static (industrial product images) and video frame (pedestrian violations) formats. No textual, graph-based, or other multimodal inputs are described, indicating the paper exclusively handles standard pixel-level image data.",
        "Data_domain": "The paper focuses on industrial product inspection and video anomaly scenarios, relying primarily on the MVTec AD (industrial product images), Magnetic Tile Defects (tile defect images), and mini Shanghai Tech Campus (video-based violence detection) datasets. This includes diverse industrial defect types (e.g., scratches or misplaced parts) and human-activity anomalies.",
        "ML_phase": "They freeze a pre-trained backbone to extract multi-scale feature maps from the first three stages and downsample them as inputs to an asymmetrical multi-scale normalizing flow with a fusion flow for cross-scale information exchange. The model is then optimized in an unsupervised manner by maximizing the likelihood of normal data distributions, followed by multi-level likelihood aggregation for image-wise detection and pixel-wise localization."
    },
    "3D-Object-Detection/2103.15297v1.json": {
        "title": "LiDAR R-CNN: An Efficient and Universal 3D Object Detector",
        "abs": "LiDAR-based 3D detection in point cloud is essential in the perception system\nof autonomous driving. In this paper, we present LiDAR R-CNN, a second stage\ndetector that can generally improve any existing 3D detector. To fulfill the\nreal-time and high precision requirement in practice, we resort to point-based\napproach other than the popular voxel-based approach. However, we find an\noverlooked issue in previous work: Naively applying point-based methods like\nPointNet could make the learned features ignore the size of proposals. To this\nend, we analyze this problem in detail and propose several methods to remedy\nit, which bring significant performance improvement. Comprehensive experimental\nresults on real-world datasets like Waymo Open Dataset (WOD) and KITTI dataset\nwith various popular detectors demonstrate the universality and superiority of\nour LiDAR R-CNN. In particular, based on one variant of PointPillars, our\nmethod could achieve new state-of-the-art results with minor cost. Codes will\nbe released at https://github.com/tusimple/LiDAR_RCNN .",
        "Task": "They tackle a 3D object detection task that involves classifying and regressing 7DoF bounding boxes (location, dimension, and orientation) from LiDAR point clouds. They evaluate performance on standard autonomous driving datasets such as the Waymo Open Dataset and KITTI.",
        "Data_type": "They handle irregular, sparse 3D point cloud data captured by LiDAR sensors. The approach focuses on processing raw point-level representations to estimate object location, dimension, and orientation in real-world driving scenarios.",
        "Data_domain": "The data is sourced from LiDAR point clouds in real-world autonomous driving scenarios that capture vehicles, pedestrians, and complex urban environments. The authors specifically employ large-scale mobility datasets, including the Waymo Open Dataset and KITTI, to validate 3D detection tasks in this domain.",
        "ML_phase": "This paper introduces a second-stage 3D detection module that refines bounding boxes generated by various first-stage detectors. The method normalizes LiDAR points within each proposal region and then applies a lightweight PointNet-based network for classification and regression, trained with a softmax cross-entropy and SmoothL1 loss scheme."
    },
    "3D-Object-Detection/2206.15398v6.json": {
        "title": "PolarFormer: Multi-camera 3D Object Detection with Polar Transformer",
        "abs": "3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives.",
        "Task": "They address a 3D object detection task from multi-camera images. The method is evaluated on the nuScenes dataset to assess detection performance.",
        "Data_type": "They process multi-camera 2D images at a resolution of 1600×900 from the nuScenes dataset. The approach focuses on leveraging these inputs for 3D object detection.",
        "Data_domain": "The data is drawn from urban autonomous driving scenarios, specifically multi-camera imagery in the nuScenes dataset. It includes sequential frames from six surrounding cameras capturing diverse real-world street scenes.",
        "ML_phase": "A cross-plane encoder first transforms multi-scale image features into polar-ray embeddings, which are then unified by a polar alignment module, enhanced via a multi-scale Polar BEV encoder, and finally decoded by a Polar detection head for 3D bounding box prediction. The model is trained end-to-end with an AdamW optimizer, leveraging a pre-trained backbone and multi-camera data to learn polar representations."
    },
    "3D-Object-Detection/2104.00678v2.json": {
        "title": "Group-Free 3D Object Detection via Transformers",
        "abs": "Recently, directly detecting 3D objects from 3D point clouds has received\nincreasing attention. To extract object representation from an irregular point\ncloud, existing methods usually take a point grouping step to assign the points\nto an object candidate so that a PointNet-like network could be used to derive\nobject features from the grouped points. However, the inaccurate point\nassignments caused by the hand-crafted grouping scheme decrease the performance\nof 3D object detection.\n  In this paper, we present a simple yet effective method for directly\ndetecting 3D objects from the 3D point cloud. Instead of grouping local points\nto each object candidate, our method computes the feature of an object from all\nthe points in the point cloud with the help of an attention mechanism in the\nTransformers \\cite{vaswani2017attention}, where the contribution of each point\nis automatically learned in the network training. With an improved attention\nstacking scheme, our method fuses object features in different stages and\ngenerates more accurate object detection results. With few bells and whistles,\nthe proposed method achieves state-of-the-art 3D object detection performance\non two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models\nare publicly available at \\url{https://github.com/zeliu98/Group-Free-3D}",
        "Task": "This paper tackles a 3D object detection task on point clouds, where the goal is to localize and classify objects in 3D space with bounding boxes. They evaluate performance using common metrics (e.g., mAP) on ScanNet V2 and SUN RGB-D datasets.",
        "Data_type": "They handle irregular and sparse 3D point clouds, where each sample consists of a set of (x, y, z) coordinates possibly with additional attributes like color or normals. The datasets include indoor scenes (e.g., from ScanNet V2 or SUN RGB-D) with up to tens of thousands of points per instance.",
        "Data_domain": "These experiments use 3D point cloud data from indoor scenes, derived from large-scale scans (ScanNet V2) and single-view depth captures (SUN RGB-D). The domain is real-world indoor environments, including various household objects annotated with 3D bounding boxes.",
        "ML_phase": "They preprocess point clouds (e.g., random flip/rotation/scale), extract per-point features with a PointNet++ backbone, and then sample initial object candidates before stacked attention modules iteratively refine bounding boxes. The system is trained end-to-end with AdamW under a multi-stage loss, and final predictions can be ensembled across decoder stages at inference."
    },
    "3D-Object-Detection/2110.06923v1.json": {
        "title": "Object DGCNN: 3D Object Detection using Dynamic Graphs",
        "abs": "3D object detection often involves complicated training and testing\npipelines, which require substantial domain knowledge about individual\ndatasets. Inspired by recent non-maximum suppression-free 2D object detection\nmodels, we propose a 3D object detection architecture on point clouds. Our\nmethod models 3D object detection as message passing on a dynamic graph,\ngeneralizing the DGCNN framework to predict a set of objects. In our\nconstruction, we remove the necessity of post-processing via object confidence\naggregation or non-maximum suppression. To facilitate object detection from\nsparse point clouds, we also propose a set-to-set distillation approach\ncustomized to 3D detection. This approach aligns the outputs of the teacher\nmodel and the student model in a permutation-invariant fashion, significantly\nsimplifying knowledge distillation for the 3D detection task. Our method\nachieves state-of-the-art performance on autonomous driving benchmarks. We also\nprovide abundant analysis of the detection model and distillation framework.",
        "Task": "This paper addresses a 3D object detection task on point cloud data, specifically predicting bounding boxes for objects in large-scale scenes. They train and evaluate their method on the nuScenes dataset for performance benchmarking.",
        "Data_type": "They process 3D point clouds in ℝ³ with per-point features ℝ^K and convert them into a 2D BEV feature map of size H^d×W^d×C^d. Each object is finally represented by a 9D bounding box (position, size, heading angle, velocity) and a categorical label.",
        "Data_domain": "The dataset derives from real-world autonomous driving scenarios, specifically large-scale LiDAR point clouds from the nuScenes dataset captured in urban environments (Boston and Singapore). It involves aggregated sensor data representing diverse traffic conditions for 3D object detection tasks.",
        "ML_phase": "They voxelize LiDAR point clouds into BEV grids for efficient feature extraction, then apply a set-based object detection head (Object DGCNN) trained with Hungarian matching instead of dense anchors or NMS. The pipeline is further extended with a set-to-set knowledge distillation stage, transferring knowledge from a teacher model to a student model during training."
    },
    "3D-Object-Detection/2305.04925v1.json": {
        "title": "PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR  Point Clouds",
        "abs": "In order to deal with the sparse and unstructured raw point clouds, LiDAR\nbased 3D object detection research mostly focuses on designing dedicated local\npoint aggregators for fine-grained geometrical modeling. In this paper, we\nrevisit the local point aggregators from the perspective of allocating\ncomputational resources. We find that the simplest pillar based models perform\nsurprisingly well considering both accuracy and latency. Additionally, we show\nthat minimal adaptions from the success of 2D object detection, such as\nenlarging receptive field, significantly boost the performance. Extensive\nexperiments reveal that our pillar based networks with modernized designs in\nterms of architecture and training render the state-of-the-art performance on\nthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our results\nchallenge the common intuition that the detailed geometry modeling is essential\nto achieve high performance for 3D object detection.",
        "Task": "They address the 3D object detection task in LiDAR point clouds for autonomous driving. The performance is evaluated on the Waymo Open Dataset and nuScenes benchmarks.",
        "Data_type": "They handle large-scale LiDAR point cloud data composed of 3D coordinates derived from autonomous driving scenarios. The method processes these irregular, sparse point clouds to perform 3D object detection.",
        "Data_domain": "The datasets come from LiDAR point clouds recorded in real-world autonomous driving scenarios. This primarily involves road scenes captured by vehicle-mounted sensors.",
        "ML_phase": "They discretize raw LiDAR point clouds into structured grids (pillars or voxels) and feed them through a ResNet-style backbone with neck modules (e.g., ASPP) to extract multi-level features, then apply a center-based multi-group head for final bounding-box predictions. The model is trained end-to-end with AdamW under a one-cycle schedule, using data augmentations (flip, rotation, scaling, translation, and faded copy-and-paste) to boost robustness."
    },
    "3D-Object-Detection/2205.07403v5.json": {
        "title": "PillarNet: Real-Time and High-Performance Pillar-based 3D Object  Detection",
        "abs": "Real-time and high-performance 3D object detection is of critical importance\nfor autonomous driving. Recent top-performing 3D object detectors mainly rely\non point-based or 3D voxel-based convolutions, which are both computationally\ninefficient for onboard deployment. In contrast, pillar-based methods use\nsolely 2D convolutions, which consume less computation resources, but they lag\nfar behind their voxel-based counterparts in detection accuracy. In this paper,\nby examining the primary performance gap between pillar- and voxel-based\ndetectors, we develop a real-time and high-performance pillar-based detector,\ndubbed PillarNet.The proposed PillarNet consists of a powerful encoder network\nfor effective pillar feature learning, a neck network for spatial-semantic\nfeature fusion and the commonly used detect head. Using only 2D convolutions,\nPillarNet is flexible to an optional pillar size and compatible with classical\n2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits\nfrom our designed orientation-decoupled IoU regression loss along with the\nIoU-aware prediction branch. Extensive experimental results on the large-scale\nnuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet\nperforms well over state-of-the-art 3D detectors in terms of effectiveness and\nefficiency. Code is available at \\url{https://github.com/agent-sgs/PillarNet}.",
        "Task": "They address the 3D object detection task from LiDAR point clouds, predicting bounding boxes for objects in autonomous driving scenes. The performance is evaluated on the nuScenes and Waymo Open Dataset benchmarks.",
        "Data_type": "They handle LiDAR-based 3D point cloud data for autonomous driving tasks. The paper specifically converts raw 3D point clouds into a 2D pillar representation for efficient 3D object detection.",
        "Data_domain": "They use large-scale LiDAR data from real-world autonomous driving scenarios. Specifically, the paper employs datasets collected from urban traffic environments for 3D object detection tasks.",
        "ML_phase": "They propose a pillar-based “encoder–neck–head” architecture using sparse 2D convolutions, where the encoder hierarchically downsamples pillar features, the neck fuses multi-scale spatial–semantic features, and a center-based head performs the final detection. The training pipeline employs an Adam optimizer with a one-cycle scheduler, standard LiDAR augmentations, and IoU-aware losses for real-time performance."
    },
    "3D-Object-Detection/2303.03595v2.json": {
        "title": "LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global  Cross-Modal Fusion",
        "abs": "LiDAR-camera fusion methods have shown impressive performance in 3D object\ndetection. Recent advanced multi-modal methods mainly perform global fusion,\nwhere image features and point cloud features are fused across the whole scene.\nSuch practice lacks fine-grained region-level information, yielding suboptimal\nfusion performance. In this paper, we present the novel Local-to-Global fusion\nnetwork (LoGoNet), which performs LiDAR-camera fusion at both local and global\nlevels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous\nliterature, while we exclusively use point centroids to more precisely\nrepresent the position of voxel features, thus achieving better cross-modal\nalignment. As to the Local Fusion (LoF), we first divide each proposal into\nuniform grids and then project these grid centers to the images. The image\nfeatures around the projected grid points are sampled to be fused with\nposition-decorated point cloud features, maximally utilizing the rich\ncontextual information around the proposals. The Feature Dynamic Aggregation\n(FDA) module is further proposed to achieve information interaction between\nthese locally and globally fused features, thus producing more informative\nmulti-modal features. Extensive experiments on both Waymo Open Dataset (WOD)\nand KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D\ndetection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection\nleaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy\nthat, for the first time, the detection performance on three classes surpasses\n80 APH (L2) simultaneously. Code will be available at\n\\url{https://github.com/sankin97/LoGoNet}.",
        "Task": "They tackle 3D object detection on LiDAR and camera data. The method’s performance is evaluated on the Waymo Open Dataset and KITTI benchmarks.",
        "Data_type": "They process 3D LiDAR point clouds and synchronize them with multi-camera 2D RGB images. The resulting multi-modal data (3D+2D) is used to perform 3D object detection in autonomous driving scenarios.",
        "Data_domain": "All data in this paper comes from large-scale autonomous driving datasets: the Waymo Open Dataset and the KITTI benchmark. These contain LiDAR point clouds and multi-camera images collected from real-world driving scenarios.",
        "ML_phase": "They voxelize LiDAR points and extract features through a 3D backbone and RPN to generate region proposals, while a pretrained 2D detector processes multi-camera images. In a second-stage refinement, they fuse these multi-level voxel and image features with the proposed local-to-global modules, then train the model end-to-end (using a two-stage training strategy) to refine bounding boxes."
    },
    "3D-Object-Detection/2006.11275v2.json": {
        "title": "Center-based 3D Object Detection and Tracking",
        "abs": "Three-dimensional objects are commonly represented as 3D boxes in a\npoint-cloud. This representation mimics the well-studied image-based 2D\nbounding-box detection but comes with additional challenges. Objects in a 3D\nworld do not follow any particular orientation, and box-based detectors have\ndifficulties enumerating all orientations or fitting an axis-aligned bounding\nbox to rotated objects. In this paper, we instead propose to represent, detect,\nand track 3D objects as points. Our framework, CenterPoint, first detects\ncenters of objects using a keypoint detector and regresses to other attributes,\nincluding 3D size, 3D orientation, and velocity. In a second stage, it refines\nthese estimates using additional point features on the object. In CenterPoint,\n3D object tracking simplifies to greedy closest-point matching. The resulting\ndetection and tracking algorithm is simple, efficient, and effective.\nCenterPoint achieved state-of-the-art performance on the nuScenes benchmark for\nboth 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single\nmodel. On the Waymo Open Dataset, CenterPoint outperforms all previous single\nmodel method by a large margin and ranks first among all Lidar-only\nsubmissions. The code and pretrained models are available at\nhttps://github.com/tianweiy/CenterPoint.",
        "Task": "They address 3D object detection and 3D object tracking tasks. The paper reports performance on the Waymo Open Dataset and nuScenes datasets to evaluate these tasks.",
        "Data_type": "They process sparse 3D LiDAR point-clouds from autonomous driving scenarios, where each point encodes spatial (x, y, z) and reflectance measurements. No additional node, textual, or multi-modal data are involved.",
        "Data_domain": "The data domain is real-world autonomous driving, collected via LiDAR sensors in diverse traffic environments. Specifically, the paper uses large-scale point-cloud datasets from the Waymo Open Dataset and nuScenes.",
        "ML_phase": "Data is first voxelized or converted into pillars, augmented with flips, scaling, rotation, and ground-truth sampling, then processed by a 3D backbone to yield a map-view feature representation. The model uses a single- or two-stage center-based head—trained end-to-end with focal and L1 losses—to regress 3D bounding boxes and optionally a velocity estimate for tracking."
    },
    "3D-Object-Detection/1711.06396v1.json": {
        "title": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection",
        "abs": "Accurate detection of objects in 3D point clouds is a central problem in many\napplications, such as autonomous navigation, housekeeping robots, and\naugmented/virtual reality. To interface a highly sparse LiDAR point cloud with\na region proposal network (RPN), most existing efforts have focused on\nhand-crafted feature representations, for example, a bird's eye view\nprojection. In this work, we remove the need of manual feature engineering for\n3D point clouds and propose VoxelNet, a generic 3D detection network that\nunifies feature extraction and bounding box prediction into a single stage,\nend-to-end trainable deep network. Specifically, VoxelNet divides a point cloud\ninto equally spaced 3D voxels and transforms a group of points within each\nvoxel into a unified feature representation through the newly introduced voxel\nfeature encoding (VFE) layer. In this way, the point cloud is encoded as a\ndescriptive volumetric representation, which is then connected to a RPN to\ngenerate detections. Experiments on the KITTI car detection benchmark show that\nVoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a\nlarge margin. Furthermore, our network learns an effective discriminative\nrepresentation of objects with various geometries, leading to encouraging\nresults in 3D detection of pedestrians and cyclists, based on only LiDAR.",
        "Task": "They address a 3D object detection task on sparse LiDAR point clouds, where the goal is to predict bounding boxes for objects along with their classes. The authors evaluate their method on the KITTI dataset, focusing on cars, pedestrians, and cyclists.",
        "Data_type": "They handle sparse LiDAR 3D point clouds (on the order of 100k points) with reflectance values. These points are grouped into equally spaced 3D voxel grids for direct feature encoding and detection.",
        "Data_domain": "The paper’s data domain is LiDAR point clouds captured in real-world street environments for autonomous driving. Specifically, it relies on the KITTI benchmark dataset, which includes scenes with cars, pedestrians, and cyclists.",
        "ML_phase": "Point clouds are converted into voxel representations, and a specialized Voxel Feature Encoding module learns local shape features within each voxel. These features are then aggregated through 3D convolutions and fed into a Region Proposal Network, enabling end-to-end training for 3D object detection."
    },
    "3D-Object-Detection/1904.01649v1.json": {
        "title": "MVX-Net: Multimodal VoxelNet for 3D Object Detection",
        "abs": "Many recent works on 3D object detection have focused on designing neural\nnetwork architectures that can consume point cloud data. While these approaches\ndemonstrate encouraging performance, they are typically based on a single\nmodality and are unable to leverage information from other modalities, such as\na camera. Although a few approaches fuse data from different modalities, these\nmethods either use a complicated pipeline to process the modalities\nsequentially, or perform late-fusion and are unable to learn interaction\nbetween different modalities at early stages. In this work, we present\nPointFusion and VoxelFusion: two simple yet effective early-fusion approaches\nto combine the RGB and point cloud modalities, by leveraging the recently\nintroduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates\nsignificant improvements in performance over approaches which only use point\ncloud data. Furthermore, the proposed method provides results competitive with\nthe state-of-the-art multimodal algorithms, achieving top-2 ranking in five of\nthe six bird's eye view and 3D detection categories on the KITTI benchmark, by\nusing a simple single stage network.",
        "Task": "They tackle a 3D object detection task, predicting 3D bounding boxes around objects in point cloud data. The KITTI dataset is used for quantitative performance evaluation.",
        "Data_type": "This paper handles LiDAR-based 3D point cloud data (XYZ coordinates plus reflectance) fused with 2D RGB image features. The final multimodal inputs consist of voxelized 3D representations and high-level image embedding vectors for improved detection performance.",
        "Data_domain": "They use data from the autonomous driving domain, specifically the KITTI dataset which includes LiDAR point clouds and RGB images of real-world road scenes. This domain focuses on vehicular and traffic environments for 3D object detection tasks.",
        "ML_phase": "They extract high-level 2D features from a pre-trained detector, project 3D inputs for early (point-level) or later (voxel-level) fusion via Voxel Feature Encoding, and pass the fused representations into a 3D region proposal network. The training involves a trimmed RPN with standard anchor-based bounding-box regression/classification, uses batch normalization and ReLU layers, and is optimized end-to-end with stochastic gradient descent and partial data augmentations."
    },
    "3D-Object-Detection/1812.05784v2.json": {
        "title": "PointPillars: Fast Encoders for Object Detection from Point Clouds",
        "abs": "Object detection in point clouds is an important aspect of many robotics\napplications such as autonomous driving. In this paper we consider the problem\nof encoding a point cloud into a format appropriate for a downstream detection\npipeline. Recent literature suggests two types of encoders; fixed encoders tend\nto be fast but sacrifice accuracy, while encoders that are learned from data\nare more accurate, but slower. In this work we propose PointPillars, a novel\nencoder which utilizes PointNets to learn a representation of point clouds\norganized in vertical columns (pillars). While the encoded features can be used\nwith any standard 2D convolutional detection architecture, we further propose a\nlean downstream network. Extensive experimentation shows that PointPillars\noutperforms previous encoders with respect to both speed and accuracy by a\nlarge margin. Despite only using lidar, our full detection pipeline\nsignificantly outperforms the state of the art, even among fusion methods, with\nrespect to both the 3D and bird's eye view KITTI benchmarks. This detection\nperformance is achieved while running at 62 Hz: a 2 - 4 fold runtime\nimprovement. A faster version of our method matches the state of the art at 105\nHz. These benchmarks suggest that PointPillars is an appropriate encoding for\nobject detection in point clouds.",
        "Task": "They address a 3D object detection task from LiDAR point clouds. Performance is evaluated on benchmarking datasets, specifically the KITTI dataset.",
        "Data_type": "They process 3D point cloud data from LiDAR sensors by converting raw coordinates into “pillars,” where each pillar represents a vertical column of points. The approach is tailored for sparse 3D LiDAR data, but can also generalize to other point-based modalities (e.g., radar).",
        "Data_domain": "The data in this paper is sourced from real-world urban driving scenes captured by lidar sensors in the KITTI dataset for autonomous vehicle applications.",
        "ML_phase": "PointPillars converts LiDAR point clouds into “pillars,” applies a PointNet-like feature encoding, and uses a 2D CNN backbone plus a single-stage detection head for 3D box prediction. They augment data (e.g., ground-truth sampling, flipping, rotations, scaling) and train end-to-end with an Adam optimizer and focal-based classification plus Smooth L1 localization losses."
    },
    "3D-Object-Detection/2004.05679v1.json": {
        "title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection",
        "abs": "In this paper, we address the 3D object detection task by capturing\nmulti-level contextual information with the self-attention mechanism and\nmulti-scale feature fusion. Most existing 3D object detection methods recognize\nobjects individually, without giving any consideration on contextual\ninformation between these objects. Comparatively, we propose Multi-Level\nContext VoteNet (MLCVNet) to recognize 3D objects correlatively, building on\nthe state-of-the-art VoteNet. We introduce three context modules into the\nvoting and classifying stages of VoteNet to encode contextual information at\ndifferent levels. Specifically, a Patch-to-Patch Context (PPC) module is\nemployed to capture contextual information between the point patches, before\nvoting for their corresponding object centroid points. Subsequently, an\nObject-to-Object Context (OOC) module is incorporated before the proposal and\nclassification stage, to capture the contextual information between object\ncandidates. Finally, a Global Scene Context (GSC) module is designed to learn\nthe global scene context. We demonstrate these by capturing contextual\ninformation at patch, object and scene levels. Our method is an effective way\nto promote detection accuracy, achieving new state-of-the-art detection\nperformance on challenging 3D object detection datasets, i.e., SUN RGBD and\nScanNet. We also release our code at https://github.com/NUAAXQ/MLCVNet.",
        "Task": "They focus on 3D object detection from point clouds, predicting bounding boxes for indoor objects. The approach is evaluated on SUN RGB-D and ScanNet datasets to measure detection performance.",
        "Data_type": "They operate on unstructured 3D point clouds, where each sample contains spatial coordinates capturing geometry information in indoor scenes. These point clouds often exhibit occlusions and incomplete data but serve as the sole input modality for their detection framework.",
        "Data_domain": "The data domain comprises real-world indoor 3D point clouds, primarily drawn from SUN RGB-D and ScanNet datasets. Scenes typically capture household environments with complex spatial arrangements and occlusions.",
        "ML_phase": "They adopt an end-to-end 3D detection pipeline based on VoteNet, where point clouds are first processed by PointNet++ to extract seed features, then a Hough voting module produces object proposals. Subsequently, three context modules (patch-patch, object-object, and global scene) enrich features before final bounding box regression and classification, all trained with an Adam optimizer."
    },
    "3D-Object-Detection/2208.11112v4.json": {
        "title": "DeepInteraction: 3D Object Detection via Modality Interaction",
        "abs": "Existing top-performance 3D object detectors typically rely on the\nmulti-modal fusion strategy. This design is however fundamentally restricted\ndue to overlooking the modality-specific useful information and finally\nhampering the model performance. To address this limitation, in this work we\nintroduce a novel modality interaction strategy where individual per-modality\nrepresentations are learned and maintained throughout for enabling their unique\ncharacteristics to be exploited during object detection. To realize this\nproposed strategy, we design a DeepInteraction architecture characterized by a\nmulti-modal representational interaction encoder and a multi-modal predictive\ninteraction decoder. Experiments on the large-scale nuScenes dataset show that\nour proposed method surpasses all prior arts often by a large margin.\nCrucially, our method is ranked at the first position at the highly competitive\nnuScenes object detection leaderboard.",
        "Task": "They tackle a 3D object detection task by predicting bounding boxes and categories in 3D space. The approach is evaluated on the nuScenes dataset to gauge detection performance.",
        "Data_type": "They handle 3D LiDAR point clouds (voxelized or pillar-based within ranges such as [-54m, 54m]×[-5m, 3m]) and 2D multi-camera RGB images (e.g., six views at 1600×900 resolution). Both modalities provide multi-modal data for autonomous driving scenarios.",
        "Data_domain": "The data originates from an autonomous driving domain, specifically the nuScenes dataset recording LiDAR point clouds and multi-camera images in real-world traffic environments. It captures diverse driving scenarios to facilitate 3D object detection research.",
        "ML_phase": "They preprocess LiDAR and camera data via voxelization, image resizing, depth completion, and random augmentations (e.g., rotation, scaling, flipping) before aligning them through calibration. The dual-stream network architecture employs separate LiDAR and camera backbones combined via a multi-modal interaction encoder-decoder, and is trained in two stages (LiDAR-only, then multi-modal) with a set-based detection objective."
    },
    "3D-Object-Detection/2303.02314v1.json": {
        "title": "Virtual Sparse Convolution for Multimodal 3D Object Detection",
        "abs": "Recently, virtual/pseudo-point-based 3D object detection that seamlessly\nfuses RGB images and LiDAR data by depth completion has gained great attention.\nHowever, virtual points generated from an image are very dense, introducing a\nhuge amount of redundant computation during detection. Meanwhile, noises\nbrought by inaccurate depth completion significantly degrade detection\nprecision. This paper proposes a fast yet effective backbone, termed\nVirConvNet, based on a new operator VirConv (Virtual Sparse Convolution), for\nvirtual-point-based 3D object detection. VirConv consists of two key designs:\n(1) StVD (Stochastic Voxel Discard) and (2) NRConv (Noise-Resistant Submanifold\nConvolution). StVD alleviates the computation problem by discarding large\namounts of nearby redundant voxels. NRConv tackles the noise problem by\nencoding voxel features in both 2D image and 3D LiDAR space. By integrating\nVirConv, we first develop an efficient pipeline VirConv-L based on an early\nfusion design. Then, we build a high-precision pipeline VirConv-T based on a\ntransformed refinement scheme. Finally, we develop a semi-supervised pipeline\nVirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test\nleaderboard, our VirConv-L achieves 85% AP with a fast running speed of 56ms.\nOur VirConv-T and VirConv-S attains a high-precision of 86.3% and 87.2% AP, and\ncurrently rank 2nd and 1st, respectively. The code is available at\nhttps://github.com/hailanyi/VirConv.",
        "Task": "They tackle a 3D object detection task using LiDAR and camera inputs, evaluated primarily on the KITTI and nuScenes datasets. They measure detection performance by standard 3D metrics (e.g., 3D AP) to validate improvements in accuracy, especially for distant objects.",
        "Data_type": "They utilize LiDAR-based 3D point cloud data and RGB images to perform multi-modal 3D object detection. They additionally incorporate dense virtual points generated from depth completion to unify 2D and 3D scene information.",
        "Data_domain": "The data come from autonomous driving scenarios, encompassing LiDAR scans and color images collected in real-world driving environments. Specifically, it includes datasets such as KITTI and nuScenes for 3D object detection tasks.",
        "ML_phase": "They introduce a multimodal pipeline that first fuses LiDAR points with image-based virtual points via bin-based sampling to mitigate density issues while preserving long-range geometry. Then, a dedicated 3D backbone with extended 2D receptive fields (NRConv) is trained in a multi-stage (and optionally semi-supervised) framework to encode noise-resistant voxel features for 3D detection."
    },
    "3D-Object-Detection/2110.06922v1.json": {
        "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
        "abs": "We introduce a framework for multi-camera 3D object detection. In contrast to\nexisting works, which estimate 3D bounding boxes directly from monocular images\nor use depth prediction networks to generate input for 3D object detection from\n2D information, our method manipulates predictions directly in 3D space. Our\narchitecture extracts 2D features from multiple camera images and then uses a\nsparse set of 3D object queries to index into these 2D features, linking 3D\npositions to multi-view images using camera transformation matrices. Finally,\nour model makes a bounding box prediction per object query, using a set-to-set\nloss to measure the discrepancy between the ground-truth and the prediction.\nThis top-down approach outperforms its bottom-up counterpart in which object\nbounding box prediction follows per-pixel depth estimation, since it does not\nsuffer from the compounding error introduced by a depth prediction model.\nMoreover, our method does not require post-processing such as non-maximum\nsuppression, dramatically improving inference speed. We achieve\nstate-of-the-art performance on the nuScenes autonomous driving benchmark.",
        "Task": "They address a 3D object detection task, specifically predicting 3D bounding boxes of objects from multi-camera RGB images. Performance is evaluated on the nuScenes autonomous driving dataset.",
        "Data_type": "They work with multi-camera 2D RGB images (i.e., several synchronized views) and known camera parameters (intrinsics/extrinsics) to infer 3D bounding boxes. No LiDAR/radar data is used, and each object is described by its 3D location, dimensions, pose, and category labels solely from the image streams.",
        "Data_domain": "This paper leverages multi-camera image data captured from real-world autonomous driving scenarios on urban roads. The domain is automotive self-driving environments.",
        "ML_phase": "They extract multi-scale features from multi-camera RGB images with a shared ResNet-FPN backbone, then iteratively refine a sparse set of object queries in 3D by back-projecting reference points into each camera view for feature sampling. The entire pipeline is trained end-to-end using a set-to-set (Hungarian-based) loss function with AdamW optimization and requires no NMS post-processing."
    },
    "3D-Object-Detection/2012.11704v1.json": {
        "title": "HDNET: Exploiting HD Maps for 3D Object Detection",
        "abs": "In this paper we show that High-Definition (HD) maps provide strong priors\nthat can boost the performance and robustness of modern 3D object detectors.\nTowards this goal, we design a single stage detector that extracts geometric\nand semantic features from the HD maps. As maps might not be available\neverywhere, we also propose a map prediction module that estimates the map on\nthe fly from raw LiDAR data. We conduct extensive experiments on KITTI as well\nas a large-scale 3D detection benchmark containing 1 million frames, and show\nthat the proposed map-aware detector consistently outperforms the\nstate-of-the-art in both mapped and un-mapped scenarios. Importantly the whole\nframework runs at 20 frames per second.",
        "Task": "They tackle a 3D object detection task from LiDAR data, aiming to accurately localize and classify objects in three-dimensional space. Performance is evaluated on the KITTI BEV detection benchmark and the large-scale TOR4D dataset.",
        "Data_type": "They handle 3D LiDAR point-cloud data, which is projected into a 2D Bird’s Eye View grid, and fuse it with semantic/geometric HD map information similarly represented in 2D. The framework can also incorporate online-estimated HD map priors for scenarios without offline HD map availability.",
        "Data_domain": "This work leverages LiDAR point clouds and HD maps collected in real-world autonomous driving scenarios. The data covers road geometry and traffic participants for 3D object detection in a self-driving domain.",
        "ML_phase": "They introduce a single-stage detection pipeline that first rasterizes LiDAR into bird's-eye view, optionally fuses HD map priors (either from offline maps or an online prediction network), then applies a fully convolutional backbone and header for bounding box regression and classification. Training uses a multi-task loss (classification + regression) and employs data dropout on the map channels for robustness to missing HD map inputs."
    },
    "3D-Object-Detection/1911.10150v2.json": {
        "title": "PointPainting: Sequential Fusion for 3D Object Detection",
        "abs": "Camera and lidar are important sensor modalities for robotics in general and\nself-driving cars in particular. The sensors provide complementary information\noffering an opportunity for tight sensor-fusion. Surprisingly, lidar-only\nmethods outperform fusion methods on the main benchmark datasets, suggesting a\ngap in the literature. In this work, we propose PointPainting: a sequential\nfusion method to fill this gap. PointPainting works by projecting lidar points\ninto the output of an image-only semantic segmentation network and appending\nthe class scores to each point. The appended (painted) point cloud can then be\nfed to any lidar-only method. Experiments show large improvements on three\ndifferent state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on\nthe KITTI and nuScenes datasets. The painted version of PointRCNN represents a\nnew state of the art on the KITTI leaderboard for the bird's-eye view detection\ntask. In ablation, we study how the effects of Painting depends on the quality\nand format of the semantic segmentation output, and demonstrate how latency can\nbe minimized through pipelining.",
        "Task": "They address a 3D object detection task, leveraging LiDAR and camera data. Performance is evaluated on the KITTI and nuScenes datasets.",
        "Data_type": "They use multi-modal sensor inputs, specifically 3D LiDAR point clouds with dimensions (N, D≥3) (e.g., x, y, z, reflectance) and standard 2D image data. Through projection, each point is augmented with per-pixel segmentation scores, resulting in an enriched point cloud of dimension (N, D + C).",
        "Data_domain": "The data in this paper is drawn from autonomous driving scenarios, where multimodal (LiDAR plus camera) sensor data is collected from real-world driving environments. Specifically, the experiments use the KITTI and nuScenes datasets, both standard benchmarks for 3D object detection in self-driving vehicle applications.",
        "ML_phase": "Images are first processed by a semantic segmentation network, and each LiDAR point is then “painted” with the resulting scores before being fed into a LiDAR-based 3D detection network. Training occurs in two stages—one for the segmentation model and one for the 3D detector—allowing a flexible, sequential fusion pipeline."
    },
    "3D-Object-Detection/1910.06528v2.json": {
        "title": "End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point  Clouds",
        "abs": "Recent work on 3D object detection advocates point cloud voxelization in\nbirds-eye view, where objects preserve their physical dimensions and are\nnaturally separable. When represented in this view, however, point clouds are\nsparse and have highly variable point density, which may cause detectors\ndifficulties in detecting distant or small objects (pedestrians, traffic signs,\netc.). On the other hand, perspective view provides dense observations, which\ncould allow more favorable feature encoding for such cases. In this paper, we\naim to synergize the birds-eye view and the perspective view and propose a\nnovel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn\nto utilize the complementary information from both. Specifically, we introduce\ndynamic voxelization, which has four merits compared to existing voxelization\nmethods, i) removing the need of pre-allocating a tensor with fixed size; ii)\novercoming the information loss due to stochastic point/voxel dropout; iii)\nyielding deterministic voxel embeddings and more stable detection outcomes; iv)\nestablishing the bi-directional relationship between points and voxels, which\npotentially lays a natural foundation for cross-view feature fusion. By\nemploying dynamic voxelization, the proposed feature fusion architecture\nenables each point to learn to fuse context information from different views.\nMVF operates on points and can be naturally extended to other approaches using\nLiDAR point clouds. We evaluate our MVF model extensively on the newly released\nWaymo Open Dataset and on the KITTI dataset and demonstrate that it\nsignificantly improves detection accuracy over the comparable single-view\nPointPillars baseline.",
        "Task": "They address a 3D object detection task on LiDAR point clouds, aiming to predict accurate bounding boxes for objects in real-world driving scenarios. Performance is evaluated on the Waymo Open Dataset and the KITTI dataset, measuring detection accuracy across various conditions and ranges.",
        "Data_type": "They handle raw 3D LiDAR point cloud data, preserving each point’s 3D coordinates (and optionally intensity) without discarding information. The paper’s methods focus on dense and sparse point distributions for autonomous driving scenarios.",
        "Data_domain": "The data comes from LiDAR sensors mounted on autonomous vehicles, capturing real-world driving environments. It includes large-scale 3D point clouds collected under varied conditions for automotive object detection tasks.",
        "ML_phase": "They perform a novel dynamic voxelization of the LiDAR data to preserve all points before encoding features in both birds-eye and perspective views. These voxel-level and perspective-level embeddings are fused at the point level, then trained end-to-end with a standard anchor-based loss for 3D object detection."
    },
    "3D-Object-Detection/1904.09664v2.json": {
        "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
        "abs": "Current 3D object detection methods are heavily influenced by 2D detectors.\nIn order to leverage architectures in 2D detectors, they often convert 3D point\nclouds to regular grids (i.e., to voxel grids or to bird's eye view images), or\nrely on detection in 2D images to propose 3D boxes. Few works have attempted to\ndirectly detect objects in point clouds. In this work, we return to first\nprinciples to construct a 3D detection pipeline for point cloud data and as\ngeneric as possible. However, due to the sparse nature of the data -- samples\nfrom 2D manifolds in 3D space -- we face a major challenge when directly\npredicting bounding box parameters from scene points: a 3D object centroid can\nbe far from any surface point thus hard to regress accurately in one step. To\naddress the challenge, we propose VoteNet, an end-to-end 3D object detection\nnetwork based on a synergy of deep point set networks and Hough voting. Our\nmodel achieves state-of-the-art 3D detection on two large datasets of real 3D\nscans, ScanNet and SUN RGB-D with a simple design, compact model size and high\nefficiency. Remarkably, VoteNet outperforms previous methods by using purely\ngeometric information without relying on color images.",
        "Task": "They tackle a 3D object detection task, aiming to predict oriented 3D bounding boxes for objects directly from point clouds. Their approach is quantitatively evaluated on the SUN RGB-D and ScanNet datasets.",
        "Data_type": "The method processes 3D point clouds (XYZ and optional height) as input, derived either from single-frame depth images or larger 3D scans. It does not incorporate textual, multi-modal (image-text), or graph-based data, focusing instead on purely geometric information for 3D detection.",
        "Data_domain": "They use 3D point clouds collected from indoor scenes via depth cameras and range sensors, focusing on datasets like SUN RGB-D and ScanNet. Consequently, the data domain is real-world indoor environments.",
        "ML_phase": "They process raw point clouds through a PointNet++ backbone to extract seed point features, then apply a voting module to shift seeds toward object centers and aggregate votes into proposals via a learned clustering step. The entire pipeline is trained end-to-end with multi-task losses (covering center regression, heading/size estimation, and semantic classification) to output final 3D bounding boxes and classes."
    },
    "3D-Object-Detection/2109.02497v2.json": {
        "title": "Voxel Transformer for 3D Object Detection",
        "abs": "We present Voxel Transformer (VoTr), a novel and effective voxel-based\nTransformer backbone for 3D object detection from point clouds. Conventional 3D\nconvolutional backbones in voxel-based 3D detectors cannot efficiently capture\nlarge context information, which is crucial for object recognition and\nlocalization, owing to the limited receptive fields. In this paper, we resolve\nthe problem by introducing a Transformer-based architecture that enables\nlong-range relationships between voxels by self-attention. Given the fact that\nnon-empty voxels are naturally sparse but numerous, directly applying standard\nTransformer on voxels is non-trivial. To this end, we propose the sparse voxel\nmodule and the submanifold voxel module, which can operate on the empty and\nnon-empty voxel positions effectively. To further enlarge the attention range\nwhile maintaining comparable computational overhead to the convolutional\ncounterparts, we propose two attention mechanisms for multi-head attention in\nthose two modules: Local Attention and Dilated Attention, and we further\npropose Fast Voxel Query to accelerate the querying process in multi-head\nattention. VoTr contains a series of sparse and submanifold voxel modules and\ncan be applied in most voxel-based detectors. Our proposed VoTr shows\nconsistent improvement over the convolutional baselines while maintaining\ncomputational efficiency on the KITTI dataset and the Waymo Open dataset.",
        "Task": "They tackle the 3D object detection task, predicting bounding boxes from point clouds. Their experiments assess model performance on the Waymo Open and KITTI datasets.",
        "Data_type": "They operate on 3D point cloud data, converting it into sparse voxel grids. No other modalities (e.g., text or images) are used; only these voxelized point clouds are processed for 3D object detection.",
        "Data_domain": "The data used in this work come from LiDAR point clouds captured in real-world autonomous driving scenarios. Specifically, the authors utilize large-scale datasets like Waymo Open and KITTI, both central to autonomous vehicle research.",
        "ML_phase": "They replace the conventional 3D convolutional backbone in voxel-based detectors with a Transformer-based architecture composed of sparse and submanifold voxel modules that operate on non-empty voxel indices. The entire network is trained end-to-end (e.g., via ADAM) within standard single-stage or two-stage frameworks, leveraging multi-head self-attention (local and dilated) for robust feature aggregation before final bounding-box prediction."
    },
    "3D-Object-Detection/2109.08141v1.json": {
        "title": "An End-to-End Transformer Model for 3D Object Detection",
        "abs": "We propose 3DETR, an end-to-end Transformer based object detection model for\n3D point clouds. Compared to existing detection methods that employ a number of\n3D-specific inductive biases, 3DETR requires minimal modifications to the\nvanilla Transformer block. Specifically, we find that a standard Transformer\nwith non-parametric queries and Fourier positional embeddings is competitive\nwith specialized architectures that employ libraries of 3D-specific operators\nwith hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and\neasy to implement, enabling further improvements by incorporating 3D domain\nknowledge. Through extensive experiments, we show 3DETR outperforms the\nwell-established and highly optimized VoteNet baselines on the challenging\nScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks\nbeyond detection, and can serve as a building block for future research.",
        "Task": "They address 3D object detection, predicting bounding boxes from point clouds. The paper evaluates performance on ScanNet and SUN RGB-D benchmarks.",
        "Data_type": "They operate on 3D point cloud data represented as an unordered set of points in R³, optionally extended to R⁶ by including normals. Input sizes typically reach up to tens of thousands of points per scene.",
        "Data_domain": "The paper’s data comes from real-world indoor 3D point clouds captured by depth sensors. Specifically, it is evaluated on well-known 3D detection benchmarks like ScanNet and SUNRGBD.",
        "ML_phase": "They minimally preprocess point clouds into subsampled features, feed them into a standard Transformer-based encoder-decoder, and obtain bounding boxes via a parallel decoding strategy. Training is end-to-end with bipartite set matching to pair predictions with ground truth, removing the need for separate post-processing like NMS."
    },
    "3D-Object-Detection/2204.12463v1.json": {
        "title": "Focal Sparse Convolutional Networks for 3D Object Detection",
        "abs": "Non-uniformed 3D sparse data, e.g., point clouds or voxels in different\nspatial positions, make contribution to the task of 3D object detection in\ndifferent ways. Existing basic components in sparse convolutional networks\n(Sparse CNNs) process all sparse data, regardless of regular or submanifold\nsparse convolution. In this paper, we introduce two new modules to enhance the\ncapability of Sparse CNNs, both are based on making feature sparsity learnable\nwith position-wise importance prediction. They are focal sparse convolution\n(Focals Conv) and its multi-modal variant of focal sparse convolution with\nfusion, or Focals Conv-F for short. The new modules can readily substitute\ntheir plain counterparts in existing Sparse CNNs and be jointly trained in an\nend-to-end fashion. For the first time, we show that spatially learnable\nsparsity in sparse convolution is essential for sophisticated 3D object\ndetection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks\nvalidate the effectiveness of our approach. Without bells and whistles, our\nresults outperform all existing single-model entries on the nuScenes test\nbenchmark at the paper submission time. Code and models are at\nhttps://github.com/dvlab-research/FocalsConv.",
        "Task": "They address the task of 3D object detection on sparse point cloud data, evaluated on benchmarks such as KITTI, nuScenes, and Waymo. The goal is to predict bounding boxes and classes of objects in 3D space.",
        "Data_type": "They handle unstructured 3D point cloud data from LiDAR sensors and can fuse RGB camera images for multi-modal input. The point-clouds are commonly voxelized, enabling sparse 3D feature representations alongside optional camera-based features for improved detection.",
        "Data_domain": "The data originates from 3D scenes captured in autonomous driving contexts using LIDAR sensors. In some cases, it is complemented by camera images (multi-modal) from datasets such as KITTI, nuScenes, and Waymo.",
        "ML_phase": "They voxelize and augment point clouds, then apply focal sparse convolution modules within a standard sparse CNN backbone to dynamically adjust output sparsity. Training uses regular and submanifold convolutions plus an optional multi-modal fusion branch, all supervised by focal losses for both object classification and importance maps."
    },
    "3D-Object-Detection/2203.05625v3.json": {
        "title": "PETR: Position Embedding Transformation for Multi-View 3D Object  Detection",
        "abs": "In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research. Code is\navailable at \\url{https://github.com/megvii-research/PETR}.",
        "Task": "They aim to perform 3D object detection from multi-view images. Performance is demonstrated on the nuScenes dataset.",
        "Data_type": "They utilize multi-view 2D RGB images as inputs, encoding 3D position information to output 3D bounding boxes. No text, graph, or molecular representations are involved, focusing solely on multi-view image data for 3D detection.",
        "Data_domain": "The training and evaluation data come from an autonomous driving domain, specifically large-scale multi-modal sensor data (camera, LiDAR, radar) captured in urban traffic environments. The paper primarily uses the nuScenes dataset, which provides synchronized 3D bounding box annotations across multiple camera views for real-world driving scenarios.",
        "ML_phase": "PETR processes multi-view images through a backbone to produce 2D features, encodes them with 3D coordinates as position embeddings, and feeds these 3D-aware features into a transformer decoder for 3D bounding-box predictions. The system employs anchor-based object queries, trains end-to-end using AdamW with Hungarian matching for loss assignment, and applies multi-scale data augmentation."
    },
    "3D-Object-Detection/2104.10956v3.json": {
        "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection",
        "abs": "Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging than\nconventional 2D cases due to its inherent ill-posed property, which is mainly\nreflected in the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this paper, we\nstudy this problem with a practice built on a fully convolutional single-stage\ndetector and propose a general framework FCOS3D. Specifically, we first\ntransform the commonly defined 7-DoF 3D targets to the image domain and\ndecouple them as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with consideration of their 2D scales and assigned\nonly according to the projected 3D-center for the training procedure.\nFurthermore, the center-ness is redefined with a 2D Gaussian distribution based\non the 3D-center to fit the 3D target formulation. All of these make this\nframework simple yet effective, getting rid of any 2D detection or 2D-3D\ncorrespondence priors. Our solution achieves 1st place out of all the\nvision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.\nCode and models are released at https://github.com/open-mmlab/mmdetection3d.",
        "Task": "They tackle a 3D object detection task from monocular images. The method is evaluated on the large-scale nuScenes dataset for performance benchmarking.",
        "Data_type": "They process monocular RGB images using ground-truth 3D bounding box annotations, focusing on single camera frames (no LiDAR or other modalities). The dataset consists solely of 2D images mapped to 3D target information for object detection.",
        "Data_domain": "All data is taken from a large-scale autonomous driving domain, specifically collected to capture real-world road scenarios for self-driving vehicles.",
        "ML_phase": "The method uses a single-stage anchor-free architecture with a ResNet-FPN backbone to extract multi-scale features, then regresses 3D boxes via a center-based head. It is trained end-to-end with distance-based target assignment, a Gaussian-defined center-ness, and multi-task losses for classification, depth, orientation, and size."
    },
    "3D-Object-Detection/1910.13302v3.json": {
        "title": "Weighted boxes fusion: Ensembling boxes from different object detection  models",
        "abs": "In this work, we present a novel method for combining predictions of object\ndetection models: weighted boxes fusion. Our algorithm utilizes confidence\nscores of all proposed bounding boxes to constructs the averaged boxes. We\ntested method on several datasets and evaluated it in the context of the Open\nImages and COCO Object Detection tracks, achieving top results in these\nchallenges. The source code is publicly available at\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion",
        "Task": "They address the object detection task of localizing and classifying objects in images. Performance is evaluated on the Open Images and Microsoft COCO datasets.",
        "Data_type": "They focus on object detection in 2D image data (bounding boxes) and also extend the method to 3D boxes for volumetric tasks. No graph-structured or multi-modal text/image inputs are addressed, as the work centers on combining detection outputs in image-based and extended 3D settings.",
        "Data_domain": "The data is sourced from large-scale academic image datasets (Open Images and Microsoft COCO), both of which are prominent in computer vision research for benchmarking object detection tasks.",
        "ML_phase": "Weighted Boxes Fusion operates as a post-processing ensemble method, applied after individual object detection models (or TTA predictions) generate their bounding boxes. It fuses all predicted boxes by iteratively aggregating their coordinates and confidence scores into a single, refined set of outputs."
    },
    "3D-Object-Detection/1903.01864v2.json": {
        "title": "Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features  for Amodal 3D Object Detection",
        "abs": "In this work, we propose a novel method termed \\emph{Frustum ConvNet\n(F-ConvNet)} for amodal 3D object detection from point clouds. Given 2D region\nproposals in an RGB image, our method first generates a sequence of frustums\nfor each region proposal, and uses the obtained frustums to group local points.\nF-ConvNet aggregates point-wise features as frustum-level feature vectors, and\narrays these feature vectors as a feature map for use of its subsequent\ncomponent of fully convolutional network (FCN), which spatially fuses\nfrustum-level features and supports an end-to-end and continuous estimation of\noriented boxes in the 3D space. We also propose component variants of\nF-ConvNet, including an FCN variant that extracts multi-resolution frustum\nfeatures, and a refined use of F-ConvNet over a reduced 3D space. Careful\nablation studies verify the efficacy of these component variants. F-ConvNet\nassumes no prior knowledge of the working 3D environment and is thus\ndataset-agnostic. We present experiments on both the indoor SUN-RGBD and\noutdoor KITTI datasets. F-ConvNet outperforms all existing methods on SUN-RGBD,\nand at the time of submission it outperforms all published works on the KITTI\nbenchmark. Code has been made available at:\n{\\url{https://github.com/zhixinwang/frustum-convnet}.}",
        "Task": "They aim to perform amodal 3D object detection (i.e., oriented 3D bounding box estimation) from partial point clouds. The task is evaluated on the SUN-RGBD and KITTI datasets.",
        "Data_type": "They process 3D point clouds from LiDAR or depth sensors alongside corresponding RGB images, focusing on partial object surfaces captured from a single viewpoint. The approach enables amodal 3D object detection by leveraging both point cloud data and 2D region proposals from images.",
        "Data_domain": "The data used in this work comes from 3D sensory inputs (LiDAR point clouds and RGB-D images) captured in real-world scenarios. They include outdoor driving datasets (e.g., KITTI) and indoor environments (e.g., SUN-RGBD).",
        "ML_phase": "They introduce a pipeline that takes 2D region proposals from an external detector, groups point clouds into sequential “sliding frustums,” and extracts aggregated local features using a shared-weight PointNet module. These features are reshaped into 2D feature maps and processed by fully convolutional layers for end-to-end classification and bounding-box regression, with an optional refinement step."
    },
    "3D-Object-Detection/2206.10092v2.json": {
        "title": "BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object  Detection",
        "abs": "In this research, we propose a new 3D object detector with a trustworthy\ndepth estimation, dubbed BEVDepth, for camera-based Bird's-Eye-View (BEV) 3D\nobject detection. Our work is based on a key observation -- depth estimation in\nrecent approaches is surprisingly inadequate given the fact that depth is\nessential to camera 3D detection. Our BEVDepth resolves this by leveraging\nexplicit depth supervision. A camera-awareness depth estimation module is also\nintroduced to facilitate the depth predicting capability. Besides, we design a\nnovel Depth Refinement Module to counter the side effects carried by imprecise\nfeature unprojection. Aided by customized Efficient Voxel Pooling and\nmulti-frame mechanism, BEVDepth achieves the new state-of-the-art 60.9% NDS on\nthe challenging nuScenes test set while maintaining high efficiency. For the\nfirst time, the NDS score of a camera model reaches 60%.",
        "Task": "They address the task of 3D object detection using multi-view camera inputs. Performance is benchmarked on the nuScenes dataset via detection metrics.",
        "Data_type": "They process multi-view camera images for 3D object detection tasks. Additionally, LiDAR point clouds provide depth supervision but the primary input data type is multi-view RGB images.",
        "Data_domain": "This paper uses multi-view camera and LiDAR data from real-world autonomous driving scenarios. Specifically, it employs the nuScenes dataset, which provides diverse sensor streams in an urban traffic context.",
        "ML_phase": "They propose a multi-view 3D detection model that incorporates LiDAR-derived ground-truth depth supervision, camera intrinsics/extrinsics encoding, and a Depth Refinement Module into the overall architecture to generate precise BEV representations. The framework is trained end-to-end with efficient voxel pooling, multi-frame fusion, and explicit depth losses to enhance the detector’s performance."
    },
    "3D-Object-Detection/2103.05073v1.json": {
        "title": "Offboard 3D Object Detection from Point Cloud Sequences",
        "abs": "While current 3D object recognition research mostly focuses on the real-time,\nonboard scenario, there are many offboard use cases of perception that are\nlargely under-explored, such as using machines to automatically generate\nhigh-quality 3D labels. Existing 3D object detectors fail to satisfy the\nhigh-quality requirement for offboard uses due to the limited input and speed\nconstraints. In this paper, we propose a novel offboard 3D object detection\npipeline using point cloud sequence data. Observing that different frames\ncapture complementary views of objects, we design the offboard detector to make\nuse of the temporal points through both multi-frame object detection and novel\nobject-centric refinement models. Evaluated on the Waymo Open Dataset, our\npipeline named 3D Auto Labeling shows significant gains compared to the\nstate-of-the-art onboard detectors and our offboard baselines. Its performance\nis even on par with human labels verified through a human label study. Further\nexperiments demonstrate the application of auto labels for semi-supervised\nlearning and provide extensive analysis to validate various design choices.",
        "Task": "They address offboard 3D object detection using the entire LiDAR sequence to localize and classify objects in autonomous driving scenarios. Performance is quantitatively evaluated on the large-scale Waymo Open Dataset with 3D bounding box metrics.",
        "Data_type": "They handle sequential 3D point cloud data collected by LiDAR sensors, where each frame contains raw XYZ coordinates (plus optional channels like intensity) and associated bounding box annotations. The entire dataset comprises long temporal sequences of these point clouds used for offboard 3D object detection.",
        "Data_domain": "The data originates from real-world autonomous driving scenarios, specifically LiDAR point cloud sequences in the Waymo Open Dataset. It encompasses diverse urban environments, weather conditions, and traffic situations for 3D object recognition tasks.",
        "ML_phase": "They first feed multi-frame LiDAR scans into an enhanced “MVF++” network (with anchor-free design, auxiliary segmentation loss, and test-time augmentation), then track detected boxes across frames via a Kalman filter–based multi-object tracker. Next, they extract each object’s entire point-cloud sequence (“track data”) and refine bounding boxes with separate static or dynamic object-centric models (PointNet-like architectures), using iterative regression and segmentation, yielding final high-quality 3D auto labels."
    },
    "3D-Object-Detection/2112.00322v2.json": {
        "title": "FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection",
        "abs": "Recently, promising applications in robotics and augmented reality have\nattracted considerable attention to 3D object detection from point clouds. In\nthis paper, we present FCAF3D - a first-in-class fully convolutional\nanchor-free indoor 3D object detection method. It is a simple yet effective\nmethod that uses a voxel representation of a point cloud and processes voxels\nwith sparse convolutions. FCAF3D can handle large-scale scenes with minimal\nruntime through a single fully convolutional feed-forward pass. Existing 3D\nobject detection methods make prior assumptions on the geometry of objects, and\nwe argue that it limits their generalization ability. To get rid of any prior\nassumptions, we propose a novel parametrization of oriented bounding boxes that\nallows obtaining better results in a purely data-driven way. The proposed\nmethod achieves state-of-the-art 3D object detection results in terms of\nmAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The\ncode and models are available at https://github.com/samsunglabs/fcaf3d.",
        "Task": "They tackle 3D object detection from point clouds, aiming to localize and classify objects in complex indoor scenes. Performance is evaluated on multiple indoor benchmarks (ScanNet, SUN RGB-D, and S3DIS) using mean average precision measures.",
        "Data_type": "They process unstructured 3D point clouds (optionally with RGB channels). Their approach outputs oriented 3D bounding boxes for objects in indoor scenes.",
        "Data_domain": "The data comes from reconstructed or captured 3D indoor scenes used in academic benchmarks (ScanNet, SUN RGB-D, S3DIS). These datasets contain real-world point clouds with indoor objects annotated for 3D detection tasks.",
        "ML_phase": "They voxelize up to 100k point samples at a chosen resolution and feed them into a sparse ResNet-like backbone, followed by a simplified neck that prunes features based on the model’s classification outputs. They then employ a fully convolutional anchor-free head with multi-level location assignment and train the entire network end-to-end over 12 epochs using focal classification, IoU-based regression, and a centerness loss optimized via Adam."
    },
    "3D-Object-Detection/1902.09738v2.json": {
        "title": "Stereo R-CNN based 3D Object Detection for Autonomous Driving",
        "abs": "We propose a 3D object detection method for autonomous driving by fully\nexploiting the sparse and dense, semantic and geometry information in stereo\nimagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo\ninputs to simultaneously detect and associate object in left and right images.\nWe add extra branches after stereo Region Proposal Network (RPN) to predict\nsparse keypoints, viewpoints, and object dimensions, which are combined with 2D\nleft-right boxes to calculate a coarse 3D object bounding box. We then recover\nthe accurate 3D bounding box by a region-based photometric alignment using left\nand right RoIs. Our method does not require depth input and 3D position\nsupervision, however, outperforms all existing fully supervised image-based\nmethods. Experiments on the challenging KITTI dataset show that our method\noutperforms the state-of-the-art stereo-based method by around 30% AP on both\n3D detection and 3D localization tasks. Code has been released at\nhttps://github.com/HKUST-Aerial-Robotics/Stereo-RCNN.",
        "Task": "They address a 3D object detection task involving both classification and bounding box regression. The approach is evaluated on the KITTI dataset.",
        "Data_type": "They operate on stereo image pairs (left-right camera frames) in the 2D domain, using disparity between the two views to recover 3D object information. Their approach leverages bounding box annotations in each stereo frame and infers object depth for autonomous driving scenarios.",
        "Data_domain": "The data originates from real-world autonomous driving scenarios, specifically relying on stereo imagery collected in urban environments. It focuses on the KITTI dataset, a benchmark of street scenes used for evaluating 3D object detection.",
        "ML_phase": "They present a multi-stage stereo-based detection pipeline comprising a shared-weight ResNet-101 FPN backbone for parallel feature extraction, a Stereo RPN to generate paired left-right proposals, a keypoint prediction branch, and a 3D box estimator with a subsequent dense alignment module for final 3D box refinement. The entire framework is trained using a multi-task loss strategy with uncertainty-based weighting and flipping augmentation for robust joint detection and 3D localization."
    },
    "3D-Object-Detection/2209.05588v1.json": {
        "title": "CenterFormer: Center-based Transformer for 3D Object Detection",
        "abs": "Query-based transformer has shown great potential in constructing long-range\nattention in many image-domain tasks, but has rarely been considered in\nLiDAR-based 3D object detection due to the overwhelming size of the point cloud\ndata. In this paper, we propose CenterFormer, a center-based transformer\nnetwork for 3D object detection. CenterFormer first uses a center heatmap to\nselect center candidates on top of a standard voxel-based point cloud encoder.\nIt then uses the feature of the center candidate as the query embedding in the\ntransformer. To further aggregate features from multiple frames, we design an\napproach to fuse features through cross-attention. Lastly, regression heads are\nadded to predict the bounding box on the output center feature representation.\nOur design reduces the convergence difficulty and computational complexity of\nthe transformer structure. The results show significant improvements over the\nstrong baseline of anchor-free object detection networks. CenterFormer achieves\nstate-of-the-art performance for a single model on the Waymo Open Dataset, with\n73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly\noutperforming all previously published CNN and transformer-based methods. Our\ncode is publicly available at https://github.com/TuSimple/centerformer",
        "Task": "They tackle LiDAR-based 3D object detection, which is a bounding box–level classification and regression task. They evaluate performance on large-scale benchmarks including the Waymo Open Dataset and nuScenes.",
        "Data_type": "They use 3D LiDAR point clouds, often aggregated over multiple frames for temporal information. The data is converted into voxel or bird’s-eye-view (BEV) representations for object detection.",
        "Data_domain": "The data come from large-scale LiDAR scans captured in real-world autonomous driving environments, specifically the Waymo Open Dataset and the nuScenes dataset.",
        "ML_phase": "They voxelize the LiDAR point clouds into a BEV representation using a standard backbone, then generate multi-scale center proposals to initialize transformer queries. The network is trained end-to-end with center-based classification and box regression losses, optionally incorporating multi-frame features through cross-attention to fuse temporal information."
    },
    "3D-Object-Detection/2203.10981v2.json": {
        "title": "MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer",
        "abs": "Monocular 3D object detection is an important yet challenging task in\nautonomous driving. Some existing methods leverage depth information from an\noff-the-shelf depth estimator to assist 3D detection, but suffer from the\nadditional computational burden and achieve limited performance caused by\ninaccurate depth priors. To alleviate this, we propose MonoDTR, a novel\nend-to-end depth-aware transformer network for monocular 3D object detection.\nIt mainly consists of two components: (1) the Depth-Aware Feature Enhancement\n(DFE) module that implicitly learns depth-aware features with auxiliary\nsupervision without requiring extra computation, and (2) the Depth-Aware\nTransformer (DTR) module that globally integrates context- and depth-aware\nfeatures. Moreover, different from conventional pixel-wise positional\nencodings, we introduce a novel depth positional encoding (DPE) to inject depth\npositional hints into transformers. Our proposed depth-aware modules can be\neasily plugged into existing image-only monocular 3D object detectors to\nimprove the performance. Extensive experiments on the KITTI dataset demonstrate\nthat our approach outperforms previous state-of-the-art monocular-based methods\nand achieves real-time detection. Code is available at\nhttps://github.com/kuanchihhuang/MonoDTR",
        "Task": "They tackle a 3D object detection task from monocular images, aiming to predict object bounding boxes in 3D space. The approach is evaluated primarily on the KITTI 3D object detection dataset (and also on nuScenes) to measure detection performance.",
        "Data_type": "They operate on single-view (monocular) RGB images from real-world driving scenes, using corresponding depth annotations derived from LiDAR or generated via auxiliary depth supervision. The paper thus focuses on image-based 3D object detection data, where each 2D image is paired with ground-truth 3D bounding boxes.",
        "Data_domain": "The dataset is sourced from real-world autonomous driving environments, primarily using the KITTI benchmark and also evaluated on nuScenes. These are standard academic datasets containing camera-captured scenes of vehicles, pedestrians, and other road objects.",
        "ML_phase": "They construct an image feature backbone, introduce a lightweight depth-aware feature module supervised by discretized LiDAR-based depths, then fuse context-depth features via a transformer into a unified representation. Training uses an anchor-based 2D–3D detection head (classification, regression, and auxiliary depth losses) optimized with Adam in an end-to-end pipeline."
    }
}